{"2024-03-26T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.17631v1","updated":"2024-03-26T12:08:04Z","published":"2024-03-26T12:08:04Z","title":"AniArtAvatar: Animatable 3D Art Avatar from a Single Image","summary":"  We present a novel approach for generating animatable 3D-aware art avatars\nfrom a single image, with controllable facial expressions, head poses, and\nshoulder movements. Unlike previous reenactment methods, our approach utilizes\na view-conditioned 2D diffusion model to synthesize multi-view images from a\nsingle art portrait with a neutral expression. With the generated colors and\nnormals, we synthesize a static avatar using an SDF-based neural surface. For\navatar animation, we extract control points, transfer the motion with these\npoints, and deform the implicit canonical space. Firstly, we render the front\nimage of the avatar, extract the 2D landmarks, and project them to the 3D space\nusing a trained SDF network. We extract 3D driving landmarks using 3DMM and\ntransfer the motion to the avatar landmarks. To animate the avatar pose, we\nmanually set the body height and bound the head and torso of an avatar with two\ncages. The head and torso can be animated by transforming the two cages. Our\napproach is a one-shot pipeline that can be applied to various styles.\nExperiments demonstrate that our method can generate high-quality 3D art\navatars with desired control over different motions.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.17631v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2403.17934v1","updated":"2024-03-26T17:59:23Z","published":"2024-03-26T17:59:23Z","title":"AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation","summary":"  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.\n","authors":["Qingping Sun","Yanjun Wang","Ailing Zeng","Wanqi Yin","Chen Wei","Wenjia Wang","Haiyi Mei","Chi Sing Leung","Ziwei Liu","Lei Yang","Zhongang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.17934v1.pdf","comment":"Homepage: https://ttxskk.github.io/AiOS/"},{"id":"http://arxiv.org/abs/2403.17822v1","updated":"2024-03-26T16:00:31Z","published":"2024-03-26T16:00:31Z","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","summary":"  3D Gaussian splatting, a novel differentiable rendering technique, has\nachieved state-of-the-art novel view synthesis results with high rendering\nspeeds and relatively low training times. However, its performance on scenes\ncommonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. We extend 3D Gaussian splatting with depth and\nnormal cues to tackle challenging indoor datasets and showcase techniques for\nefficient mesh extraction, an important downstream application. Specifically,\nwe regularize the optimization procedure with depth information, enforce local\nsmoothness of nearby Gaussians, and use the geometry of the 3D Gaussians\nsupervised by normal cues to achieve better alignment with the true scene\ngeometry. We improve depth estimation and novel view synthesis results over\nbaselines and show how this simple yet effective regularization technique can\nbe used to directly extract meshes from the Gaussian representation yielding\nmore physically accurate reconstructions on indoor scenes. Our code will be\nreleased in https://github.com/maturk/dn-splatter.\n","authors":["Matias Turkulainen","Xuqian Ren","Iaroslav Melekhov","Otto Seiskari","Esa Rahtu","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2403.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17782v1","updated":"2024-03-26T15:15:15Z","published":"2024-03-26T15:15:15Z","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","summary":"  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n","authors":["Chenjian Gao","Boyan Jiang","Xinghui Li","Yingpeng Zhang","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17782v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.12225v2","updated":"2024-03-26T15:06:00Z","published":"2024-02-19T15:33:09Z","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and\n  Scalability","summary":"  Auto-regressive models have achieved impressive results in 2D image\ngeneration by modeling joint distributions in grid space. In this paper, we\nextend auto-regressive models to 3D domains, and seek a stronger ability of 3D\nshape generation by improving auto-regressive models at capacity and\nscalability simultaneously. Firstly, we leverage an ensemble of publicly\navailable 3D datasets to facilitate the training of large-scale models. It\nconsists of a comprehensive collection of approximately 900,000 objects, with\nmultiple properties of meshes, points, voxels, rendered images, and text\ncaptions. This diverse labeled dataset, termed Objaverse-Mix, empowers our\nmodel to learn from a wide range of object variations. However, directly\napplying 3D auto-regression encounters critical challenges of high\ncomputational demands on volumetric grids and ambiguous auto-regressive order\nalong grid dimensions, resulting in inferior quality of 3D shapes. To this end,\nwe then present a novel framework Argus3D in terms of capacity. Concretely, our\napproach introduces discrete representation learning based on a latent vector\ninstead of volumetric grids, which not only reduces computational costs but\nalso preserves essential geometric details by learning the joint distributions\nin a more tractable order. The capacity of conditional generation can thus be\nrealized by simply concatenating various conditioning inputs to the latent\nvector, such as point clouds, categories, images, and texts. In addition,\nthanks to the simplicity of our model architecture, we naturally scale up our\napproach to a larger model with an impressive 3.6 billion parameters, further\nenhancing the quality of versatile 3D generation. Extensive experiments on four\ngeneration tasks demonstrate that Argus3D can synthesize diverse and faithful\nshapes across multiple categories, achieving remarkable performance.\n","authors":["Xuelin Qian","Yu Wang","Simian Luo","Yinda Zhang","Ying Tai","Zhenyu Zhang","Chengjie Wang","Xiangyang Xue","Bo Zhao","Tiejun Huang","Yunsheng Wu","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2402.12225v2.pdf","comment":"Project page: https://argus-3d.github.io/ . Datasets:\n  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:\n  substantial text overlap with arXiv:2303.14700"},{"id":"http://arxiv.org/abs/2403.17541v1","updated":"2024-03-26T09:44:34Z","published":"2024-03-26T09:44:34Z","title":"WordRobe: Text-Guided Generation of Textured 3D Garments","summary":"  In this paper, we tackle a new and challenging problem of text-driven\ngeneration of 3D garments with high-quality textures. We propose \"WordRobe\", a\nnovel framework for the generation of unposed & textured 3D garment meshes from\nuser-friendly text prompts. We achieve this by first learning a latent\nrepresentation of 3D garments using a novel coarse-to-fine training strategy\nand a loss for latent disentanglement, promoting better latent interpolation.\nSubsequently, we align the garment latent space to the CLIP embedding space in\na weakly supervised manner, enabling text-driven 3D garment generation and\nediting. For appearance modeling, we leverage the zero-shot generation\ncapability of ControlNet to synthesize view-consistent texture maps in a single\nfeed-forward inference step, thereby drastically decreasing the generation time\nas compared to existing methods. We demonstrate superior performance over\ncurrent SOTAs for learning 3D garment latent space, garment interpolation, and\ntext-driven texture synthesis, supported by quantitative evaluation and\nqualitative user study. The unposed 3D garment meshes generated using WordRobe\ncan be directly fed to standard cloth simulation & animation pipelines without\nany post-processing.\n","authors":["Astitva Srivastava","Pranav Manu","Amit Raj","Varun Jampani","Avinash Sharma"],"pdf_url":"https://arxiv.org/pdf/2403.17541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07632v3","updated":"2024-03-26T07:00:27Z","published":"2023-06-13T09:02:57Z","title":"NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated\n  Rendering","summary":"  This paper presents a method, namely NeuS-PIR, for recovering relightable\nneural surfaces using pre-integrated rendering from multi-view images or video.\nUnlike methods based on NeRF and discrete meshes, our method utilizes implicit\nneural surface representation to reconstruct high-quality geometry, which\nfacilitates the factorization of the radiance field into two components: a\nspatially varying material field and an all-frequency lighting representation.\nThis factorization, jointly optimized using an adapted differentiable\npre-integrated rendering framework with material encoding regularization, in\nturn addresses the ambiguity of geometry reconstruction and leads to better\ndisentanglement and refinement of each scene property. Additionally, we\nintroduced a method to distil indirect illumination fields from the learned\nrepresentations, further recovering the complex illumination effect like\ninter-reflection. Consequently, our method enables advanced applications such\nas relighting, which can be seamlessly integrated with modern graphics engines.\nQualitative and quantitative experiments have shown that NeuS-PIR outperforms\nexisting methods across various tasks on both synthetic and real datasets.\nSource code is available at https://github.com/Sheldonmao/NeuSPIR\n","authors":["Shi Mao","Chenming Wu","Zhelun Shen","Yifan Wang","Dayan Wu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.07632v3.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2403.17898v1","updated":"2024-03-26T17:39:36Z","published":"2024-03-26T17:39:36Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n","authors":["Kerui Ren","Lihan Jiang","Tao Lu","Mulin Yu","Linning Xu","Zhangkai Ni","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.17898v1.pdf","comment":"Project page: https://city-super.github.io/octree-gs/"},{"id":"http://arxiv.org/abs/2305.15094v2","updated":"2024-03-26T13:57:26Z","published":"2023-05-24T12:22:23Z","title":"InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree\n  Neural Radiance Fields","summary":"  We propose InNeRF360, an automatic system that accurately removes\ntext-specified objects from 360-degree Neural Radiance Fields (NeRF). The\nchallenge is to effectively remove objects while inpainting perceptually\nconsistent content for the missing regions, which is particularly demanding for\nexisting NeRF models due to their implicit volumetric representation. Moreover,\nunbounded scenes are more prone to floater artifacts in the inpainted region\nthan frontal-facing scenes, as the change of object appearance and background\nacross views is more sensitive to inaccurate segmentations and inconsistent\ninpainting. With a trained NeRF and a text description, our method efficiently\nremoves specified objects and inpaints visually consistent content without\nartifacts. We apply depth-space warping to enforce consistency across multiview\ntext-encoded segmentations, and then refine the inpainted NeRF model using\nperceptual priors and 3D diffusion-based geometric priors to ensure visual\nplausibility. Through extensive experiments in segmentation and inpainting on\n360-degree and frontal-facing NeRFs, we show that our approach is effective and\nenhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.\n","authors":["Dongqing Wang","Tong Zhang","Alaa Abboud","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2305.15094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17537v1","updated":"2024-03-26T09:42:28Z","published":"2024-03-26T09:42:28Z","title":"NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using\n  Heuristics-Guided Segmentation","summary":"  Neural Radiance Field (NeRF) has been widely recognized for its excellence in\nnovel view synthesis and 3D scene reconstruction. However, their effectiveness\nis inherently tied to the assumption of static scenes, rendering them\nsusceptible to undesirable artifacts when confronted with transient distractors\nsuch as moving objects or shadows. In this work, we propose a novel paradigm,\nnamely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances\nthe separation of static scenes from transient distractors by harmoniously\ncombining the strengths of hand-crafted heuristics and state-of-the-art\nsegmentation models, thus significantly transcending the limitations of\nprevious solutions. Furthermore, we delve into the meticulous design of\nheuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based\nheuristics and color residual heuristics, catering to a diverse range of\ntexture profiles. Extensive experiments demonstrate the superiority and\nrobustness of our method in mitigating transient distractors for NeRFs trained\nin non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.\n","authors":["Jiahao Chen","Yipeng Qin","Lingjie Liu","Jiangbo Lu","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17537v1.pdf","comment":"To appear in CVPR2024"},{"id":"http://arxiv.org/abs/2306.07632v3","updated":"2024-03-26T07:00:27Z","published":"2023-06-13T09:02:57Z","title":"NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated\n  Rendering","summary":"  This paper presents a method, namely NeuS-PIR, for recovering relightable\nneural surfaces using pre-integrated rendering from multi-view images or video.\nUnlike methods based on NeRF and discrete meshes, our method utilizes implicit\nneural surface representation to reconstruct high-quality geometry, which\nfacilitates the factorization of the radiance field into two components: a\nspatially varying material field and an all-frequency lighting representation.\nThis factorization, jointly optimized using an adapted differentiable\npre-integrated rendering framework with material encoding regularization, in\nturn addresses the ambiguity of geometry reconstruction and leads to better\ndisentanglement and refinement of each scene property. Additionally, we\nintroduced a method to distil indirect illumination fields from the learned\nrepresentations, further recovering the complex illumination effect like\ninter-reflection. Consequently, our method enables advanced applications such\nas relighting, which can be seamlessly integrated with modern graphics engines.\nQualitative and quantitative experiments have shown that NeuS-PIR outperforms\nexisting methods across various tasks on both synthetic and real datasets.\nSource code is available at https://github.com/Sheldonmao/NeuSPIR\n","authors":["Shi Mao","Chenming Wu","Zhelun Shen","Yifan Wang","Dayan Wu","Liangjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16080v2","updated":"2024-03-26T02:25:58Z","published":"2024-03-24T10:06:40Z","title":"PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic\n  Human Modeling","summary":"  High-quality human reconstruction and photo-realistic rendering of a dynamic\nscene is a long-standing problem in computer vision and graphics. Despite\nconsiderable efforts invested in developing various capture systems and\nreconstruction algorithms, recent advancements still struggle with loose or\noversized clothing and overly complex poses. In part, this is due to the\nchallenges of acquiring high-quality human datasets. To facilitate the\ndevelopment of these fields, in this paper, we present PKU-DyMVHumans, a\nversatile human-centric dataset for high-fidelity reconstruction and rendering\nof dynamic human scenarios from dense multi-view videos. It comprises 8.2\nmillion frames captured by more than 56 synchronized cameras across diverse\nscenarios. These sequences comprise 32 human subjects across 45 different\nscenarios, each with a high-detailed appearance and realistic human motion.\nInspired by recent advancements in neural radiance field (NeRF)-based scene\nrepresentations, we carefully set up an off-the-shelf framework that is easy to\nprovide those state-of-the-art NeRF-based implementations and benchmark on\nPKU-DyMVHumans dataset. It is paving the way for various applications like\nfine-grained foreground/background decomposition, high-quality human\nreconstruction and photo-realistic novel view synthesis of a dynamic scene.\nExtensive studies are performed on the benchmark, demonstrating new\nobservations and challenges that emerge from using such high-fidelity dynamic\ndata. The dataset is available at: https://pku-dymvhumans.github.io.\n","authors":["Xiaoyun Zheng","Liwei Liao","Xufeng Li","Jianbo Jiao","Rongjie Wang","Feng Gao","Shiqi Wang","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16080v2.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"}],"Deblur":[{"id":"http://arxiv.org/abs/2403.17377v1","updated":"2024-03-26T04:49:11Z","published":"2024-03-26T04:49:11Z","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","summary":"  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n","authors":["Donghoon Ahn","Hyoungwon Cho","Jaewon Min","Wooseok Jang","Jungwoo Kim","SeonHwa Kim","Hyun Hee Park","Kyong Hwan Jin","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17377v1.pdf","comment":"Project page is available at\n  https://ku-cvlab.github.io/Perturbed-Attention-Guidance"}]},"2024-03-25T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.16964v1","updated":"2024-03-25T17:22:11Z","published":"2024-03-25T17:22:11Z","title":"GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction","summary":"  Presenting a 3D scene from multiview images remains a core and long-standing\nchallenge in computer vision and computer graphics. Two main requirements lie\nin rendering and reconstruction. Notably, SOTA rendering quality is usually\nachieved with neural volumetric rendering techniques, which rely on aggregated\npoint/primitive-wise color and neglect the underlying scene geometry. Learning\nof neural implicit surfaces is sparked from the success of neural rendering.\nCurrent works either constrain the distribution of density fields or the shape\nof primitives, resulting in degraded rendering quality and flaws on the learned\nscene surfaces. The efficacy of such methods is limited by the inherent\nconstraints of the chosen neural representation, which struggles to capture\nfine surface details, especially for larger, more intricate scenes. To address\nthese issues, we introduce GSDF, a novel dual-branch architecture that combines\nthe benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)\nrepresentation with neural Signed Distance Fields (SDF). The core idea is to\nleverage and enhance the strengths of each branch while alleviating their\nlimitation through mutual guidance and joint supervision. We show on diverse\nscenes that our design unlocks the potential for more accurate and detailed\nsurface reconstructions, and at the meantime benefits 3DGS rendering with\nstructures that are more aligned with the underlying geometry.\n","authors":["Mulin Yu","Tao Lu","Linning Xu","Lihan Jiang","Yuanbo Xiangli","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.16964v1.pdf","comment":"Project page: https://city-super.github.io/GSDF"},{"id":"http://arxiv.org/abs/2312.02480v2","updated":"2024-03-25T06:22:09Z","published":"2023-12-05T04:13:31Z","title":"Differentiable Point-based Inverse Rendering","summary":"  We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting.\n","authors":["Hoon-Gyu Chung","Seokjun Choi","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2312.02480v2.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/1905.10711v5","updated":"2024-03-25T22:10:45Z","published":"2019-05-26T01:58:28Z","title":"DISN: Deep Implicit Surface Network for High-quality Single-view 3D\n  Reconstruction","summary":"  Reconstructing 3D shapes from single-view images has been a long-standing\nresearch problem. In this paper, we present DISN, a Deep Implicit Surface\nNetwork which can generate a high-quality detail-rich 3D mesh from an 2D image\nby predicting the underlying signed distance fields. In addition to utilizing\nglobal image features, DISN predicts the projected location for each 3D point\non the 2D image, and extracts local features from the image feature maps.\nCombining global and local features significantly improves the accuracy of the\nsigned distance field prediction, especially for the detail-rich areas. To the\nbest of our knowledge, DISN is the first method that constantly captures\ndetails such as holes and thin structures present in 3D shapes from single-view\nimages. DISN achieves the state-of-the-art single-view reconstruction\nperformance on a variety of shape categories reconstructed from both synthetic\nand real images. Code is available at https://github.com/xharlie/DISN The\nsupplementary can be found at\nhttps://xharlie.github.io/images/neurips_2019_supp.pdf\n","authors":["Qiangeng Xu","Weiyue Wang","Duygu Ceylan","Radomir Mech","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/1905.10711v5.pdf","comment":"This project was in part supported by the gift funding to the\n  University of Southern California from Adobe Research"},{"id":"http://arxiv.org/abs/2403.17213v1","updated":"2024-03-25T21:40:44Z","published":"2024-03-25T21:40:44Z","title":"AnimateMe: 4D Facial Expressions via Diffusion Models","summary":"  The field of photorealistic 3D avatar reconstruction and generation has\ngarnered significant attention in recent years; however, animating such avatars\nremains challenging. Recent advances in diffusion models have notably enhanced\nthe capabilities of generative models in 2D animation. In this work, we\ndirectly utilize these models within the 3D domain to achieve controllable and\nhigh-fidelity 4D facial animation. By integrating the strengths of diffusion\nprocesses and geometric deep learning, we employ Graph Neural Networks (GNNs)\nas denoising diffusion models in a novel approach, formulating the diffusion\nprocess directly on the mesh space and enabling the generation of 3D facial\nexpressions. This facilitates the generation of facial deformations through a\nmesh-diffusion-based model. Additionally, to ensure temporal coherence in our\nanimations, we propose a consistent noise sampling method. Under a series of\nboth quantitative and qualitative experiments, we showcase that the proposed\nmethod outperforms prior work in 4D expression synthesis by generating\nhigh-fidelity extreme expressions. Furthermore, we applied our method to\ntextured 4D facial expression generation, implementing a straightforward\nextension that involves training on a large-scale textured 4D facial expression\ndatabase.\n","authors":["Dimitrios Gerogiannis","Foivos Paraperas Papantoniou","Rolandos Alexandros Potamias","Alexandros Lattas","Stylianos Moschoglou","Stylianos Ploumpis","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2403.17213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17103v1","updated":"2024-03-25T18:41:43Z","published":"2024-03-25T18:41:43Z","title":"Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos","summary":"  We present a method to build animatable dog avatars from monocular videos.\nThis is challenging as animals display a range of (unpredictable) non-rigid\nmovements and have a variety of appearance details (e.g., fur, spots, tails).\nWe develop an approach that links the video frames via a 4D solution that\njointly solves for animal's pose variation, and its appearance (in a canonical\npose). To this end, we significantly improve the quality of template-based\nshape fitting by endowing the SMAL parametric model with Continuous Surface\nEmbeddings, which brings image-to-mesh reprojection constaints that are denser,\nand thus stronger, than the previously used sparse semantic keypoint\ncorrespondences. To model appearance, we propose an implicit duplex-mesh\ntexture that is defined in the canonical pose, but can be deformed using SMAL\npose coefficients and later rendered to enforce a photometric compatibility\nwith the input video frames. On the challenging CoP3D and APTv2 datasets, we\ndemonstrate superior results (both in terms of pose estimates and predicted\nappearance) to existing template-free (RAC) and template-based approaches\n(BARC, BITE).\n","authors":["Remy Sabathier","Niloy J. Mitra","David Novotny"],"pdf_url":"https://arxiv.org/pdf/2403.17103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08399v2","updated":"2024-03-25T16:50:43Z","published":"2024-01-16T14:41:42Z","title":"TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object\n  Understanding","summary":"  Humans commonly work with multiple objects in daily life and can intuitively\ntransfer manipulation skills to novel objects by understanding object\nfunctional regularities. However, existing technical approaches for analyzing\nand synthesizing hand-object manipulation are mostly limited to handling a\nsingle hand and object due to the lack of data support. To address this, we\nconstruct TACO, an extensive bimanual hand-object-interaction dataset spanning\na large variety of tool-action-object compositions for daily human activities.\nTACO contains 2.5K motion sequences paired with third-person and egocentric\nviews, precise hand-object 3D meshes, and action labels. To rapidly expand the\ndata scale, we present a fully automatic data acquisition pipeline combining\nmulti-view sensing with an optical motion capture system. With the vast\nresearch fields provided by TACO, we benchmark three generalizable\nhand-object-interaction tasks: compositional action recognition, generalizable\nhand-object motion forecasting, and cooperative grasp synthesis. Extensive\nexperiments reveal new insights, challenges, and opportunities for advancing\nthe studies of generalizable hand-object motion analysis and synthesis. Our\ndata and code are available at https://taco2024.github.io.\n","authors":["Yun Liu","Haolin Yang","Xu Si","Ling Liu","Zipeng Li","Yuxiang Zhang","Yebin Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2401.08399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16510v1","updated":"2024-03-25T07:54:18Z","published":"2024-03-25T07:54:18Z","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","summary":"  Despite the remarkable process of talking-head-based avatar-creating\nsolutions, directly generating anchor-style videos with full-body motions\nremains challenging. In this study, we propose Make-Your-Anchor, a novel system\nnecessitating only a one-minute video clip of an individual for training,\nsubsequently enabling the automatic generation of anchor-style videos with\nprecise torso and hand movements. Specifically, we finetune a proposed\nstructure-guided diffusion model on input video to render 3D mesh conditions\ninto human appearances. We adopt a two-stage training strategy for the\ndiffusion model, effectively binding movements with specific appearances. To\nproduce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise\ndiffusion model to a 3D style without additional training cost, and a simple\nyet effective batch-overlapped temporal denoising module is proposed to bypass\nthe constraints on video length during inference. Finally, a novel\nidentity-specific face enhancement module is introduced to improve the visual\nquality of facial regions in the output videos. Comparative experiments\ndemonstrate the effectiveness and superiority of the system in terms of visual\nquality, temporal coherence, and identity preservation, outperforming SOTA\ndiffusion/non-diffusion methods. Project page:\n\\url{https://github.com/ICTMCG/Make-Your-Anchor}.\n","authors":["Ziyao Huang","Fan Tang","Yong Zhang","Xiaodong Cun","Juan Cao","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16510v1.pdf","comment":"accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2403.16481v1","updated":"2024-03-25T07:07:50Z","published":"2024-03-25T07:07:50Z","title":"REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices","summary":"  This work tackles the challenging task of achieving real-time novel view\nsynthesis on various scenes, including highly reflective objects and unbounded\noutdoor scenes. Existing real-time rendering methods, especially those based on\nmeshes, often have subpar performance in modeling surfaces with rich\nview-dependent appearances. Our key idea lies in leveraging meshes for\nrendering acceleration while incorporating a novel approach to parameterize\nview-dependent information. We decompose the color into diffuse and specular,\nand model the specular color in the reflected direction based on a neural\nenvironment map. Our experiments demonstrate that our method achieves\ncomparable reconstruction quality for highly reflective surfaces compared to\nstate-of-the-art offline methods, while also efficiently enabling real-time\nrendering on edge devices such as smartphones.\n","authors":["Chaojie Ji","Yufeng Li","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.16481v1.pdf","comment":"Project Page:https://xdimlab.github.io/REFRAME/"},{"id":"http://arxiv.org/abs/2403.16431v1","updated":"2024-03-25T05:22:34Z","published":"2024-03-25T05:22:34Z","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene\n  Understanding","summary":"  Point scene understanding is a challenging task to process real-world scene\npoint cloud, which aims at segmenting each object, estimating its pose, and\nreconstructing its mesh simultaneously. Recent state-of-the-art method first\nsegments each object and then processes them independently with multiple stages\nfor the different sub-tasks. This leads to a complex pipeline to optimize and\nmakes it hard to leverage the relationship constraints between multiple\nobjects. In this work, we propose a novel Disentangled Object-Centric\nTRansformer (DOCTR) that explores object-centric representation to facilitate\nlearning with multiple objects for the multiple sub-tasks in a unified manner.\nEach object is represented as a query, and a Transformer decoder is adapted to\niteratively optimize all the queries involving their relationship. In\nparticular, we introduce a semantic-geometry disentangled query (SGDQ) design\nthat enables the query features to attend separately to semantic information\nand geometric information relevant to the corresponding sub-tasks. A hybrid\nbipartite matching module is employed to well use the supervisions from all the\nsub-tasks during training. Qualitative and quantitative experimental results\ndemonstrate that our method achieves state-of-the-art performance on the\nchallenging ScanNet dataset. Code is available at\nhttps://github.com/SAITPublic/DOCTR.\n","authors":["Xiaoxuan Yu","Hao Wang","Weiming Li","Qiang Wang","Soonyong Cho","Younghun Sung"],"pdf_url":"https://arxiv.org/pdf/2403.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17460v3","updated":"2024-03-25T03:21:39Z","published":"2023-11-29T09:02:07Z","title":"W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera\n  Calibration and Orientation Correction","summary":"  For a long time, in reconstructing 3D human bodies from monocular images,\nmost methods opted to simplify the task by minimizing the influence of the\ncamera. Using a coarse focal length setting results in the reconstructed bodies\nnot aligning well with distorted images. Ignoring camera rotation leads to an\nunrealistic reconstructed body pose in world space. Consequently, the\napplication scenarios of existing methods are confined to controlled\nenvironments. When confronted with complex and diverse in-the-wild images, they\nstruggle to achieve accurate and reasonable reconstruction in world space. To\naddress the above issues, we propose W-HMR, which decouples global body\nrecovery into camera calibration, local body recovery, and global body\norientation correction. We design the first weak-supervised camera calibration\nmethod for body distortion, eliminating dependence on focal length labels and\nachieving finer mesh-image alignment. We propose a novel orientation correction\nmodule to allow the reconstructed human body to remain normal in world space.\nDecoupling body orientation and body pose enables our model to consider the\naccuracy in camera coordinate and the reasonableness in world coordinate\nsimultaneously, expanding the range of applications. As a result, W-HMR\nachieves high-quality reconstruction in dual coordinate systems, particularly\nin challenging scenes. Codes and demos have been released on the project page\nhttps://yw0208.github.io/w-hmr/.\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2311.17460v3.pdf","comment":"Project Page: https://yw0208.github.io/w-hmr/"}],"NeRF":[{"id":"http://arxiv.org/abs/2403.17001v1","updated":"2024-03-25T17:59:31Z","published":"2024-03-25T17:59:31Z","title":"VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation","summary":"  Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io.\n","authors":["Yang Chen","Yingwei Pan","Haibo Yang","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17001v1.pdf","comment":"CVPR 2024; Project page: https://vp3d-cvpr24.github.io"},{"id":"http://arxiv.org/abs/2403.16885v1","updated":"2024-03-25T15:56:17Z","published":"2024-03-25T15:56:17Z","title":"CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance\n  Fields from Sparse Inputs","summary":"  Neural Radiance Fields (NeRF) have shown impressive capabilities for\nphotorealistic novel view synthesis when trained on dense inputs. However, when\ntrained on sparse inputs, NeRF typically encounters issues of incorrect density\nor color predictions, mainly due to insufficient coverage of the scene causing\npartial and sparse supervision, thus leading to significant performance\ndegradation. While existing works mainly consider ray-level consistency to\nconstruct 2D learning regularization based on rendered color, depth, or\nsemantics on image planes, in this paper we propose a novel approach that\nmodels 3D spatial field consistency to improve NeRF's performance with sparse\ninputs. Specifically, we first adopt a voxel-based ray sampling strategy to\nensure that the sampled rays intersect with a certain voxel in 3D space. We\nthen randomly sample additional points within the voxel and apply a Transformer\nto infer the properties of other points on each ray, which are then\nincorporated into the volume rendering. By backpropagating through the\nrendering loss, we enhance the consistency among neighboring points.\nAdditionally, we propose to use a contrastive loss on the encoder output of the\nTransformer to further improve consistency within each voxel. Experiments\ndemonstrate that our method yields significant improvement over different\nradiance fields in the sparse inputs setting, and achieves comparable\nperformance with current works.\n","authors":["Yingji Zhong","Lanqing Hong","Zhenguo Li","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16885v1.pdf","comment":"The paper is accepted by CVPR 2024. Project page is available at\n  https://zhongyingji.github.io/CVT-xRF"},{"id":"http://arxiv.org/abs/2312.09913v2","updated":"2024-03-25T14:09:09Z","published":"2023-12-15T16:23:42Z","title":"LAENeRF: Local Appearance Editing for Neural Radiance Fields","summary":"  Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest\ntowards editable implicit 3D representations has surged over the last years.\nHowever, editing implicit or hybrid representations as used for NeRFs is\ndifficult due to the entanglement of appearance and geometry encoded in the\nmodel parameters. Despite these challenges, recent research has shown first\npromising steps towards photorealistic and non-photorealistic appearance edits.\nThe main open issues of related work include limited interactivity, a lack of\nsupport for local edits and large memory requirements, rendering them less\nuseful in practice. We address these limitations with LAENeRF, a unified\nframework for photorealistic and non-photorealistic appearance editing of\nNeRFs. To tackle local editing, we leverage a voxel grid as starting point for\nregion selection. We learn a mapping from expected ray terminations to final\noutput color, which can optionally be supervised by a style loss, resulting in\na framework which can perform photorealistic and non-photorealistic appearance\nediting of selected regions. Relying on a single point per ray for our mapping,\nwe limit memory requirements and enable fast optimization. To guarantee\ninteractivity, we compose the output color using a set of learned, modifiable\nbase colors, composed with additive layer mixing. Compared to concurrent work,\nLAENeRF enables recoloring and stylization while keeping processing time low.\nFurthermore, we demonstrate that our approach surpasses baseline methods both\nquantitatively and qualitatively.\n","authors":["Lukas Radl","Michael Steiner","Andreas Kurz","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2312.09913v2.pdf","comment":"Accepted to CVPR 2024! Project website:\n  https://r4dl.github.io/LAENeRF/"},{"id":"http://arxiv.org/abs/2402.07310v2","updated":"2024-03-25T12:58:45Z","published":"2024-02-11T21:16:42Z","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis","summary":"  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n","authors":["Leandro A. Passos","Douglas Rodrigues","Danilo Jodas","Kelton A. P. Costa","Ahsan Adeel","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2402.07310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16410v1","updated":"2024-03-25T04:05:23Z","published":"2024-03-25T04:05:23Z","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","summary":"  As a neuromorphic sensor with high temporal resolution, spike cameras offer\nnotable advantages over traditional cameras in high-speed vision applications\nsuch as high-speed optical estimation, depth estimation, and object tracking.\nInspired by the success of the spike camera, we proposed Spike-NeRF, the first\nNeural Radiance Field derived from spike data, to achieve 3D reconstruction and\nnovel viewpoint synthesis of high-speed scenes. Instead of the multi-view\nimages at the same time of NeRF, the inputs of Spike-NeRF are continuous spike\nstreams captured by a moving spike camera in a very short time. To reconstruct\na correct and stable 3D scene from high-frequency but unstable spike data, we\ndevised spike masks along with a distinctive loss function. We evaluate our\nmethod qualitatively and numerically on several challenging synthetic scenes\ngenerated by blender with the spike camera simulator. Our results demonstrate\nthat Spike-NeRF produces more visually appealing results than the existing\nmethods and the baseline we proposed in high-speed scenes. Our code and data\nwill be released soon.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Mianzhi Liu","Ziyi Guo","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16410v1.pdf","comment":"This paper is accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.10119v2","updated":"2024-03-25T01:08:14Z","published":"2024-03-15T09:08:27Z","title":"URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural\n  Radiance Fields","summary":"  We propose a novel rolling shutter bundle adjustment method for neural\nradiance fields (NeRF), which utilizes the unordered rolling shutter (RS)\nimages to obtain the implicit 3D representation. Existing NeRF methods suffer\nfrom low-quality images and inaccurate initial camera poses due to the RS\neffect in the image, whereas, the previous method that incorporates the RS into\nNeRF requires strict sequential data input, limiting its widespread\napplicability. In constant, our method recovers the physical formation of RS\nimages by estimating camera poses and velocities, thereby removing the input\nconstraints on sequential data. Moreover, we adopt a coarse-to-fine training\nstrategy, in which the RS epipolar constraints of the pairwise frames in the\nscene graph are used to detect the camera poses that fall into local minima.\nThe poses detected as outliers are corrected by the interpolation method with\nneighboring poses. The experimental results validate the effectiveness of our\nmethod over state-of-the-art works and demonstrate that the reconstruction of\n3D representations is not constrained by the requirement of video sequence\ninput.\n","authors":["Bo Xu","Ziao Liu","Mengqi Guo","Jiancheng Li","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10119v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.16368v1","updated":"2024-03-25T02:17:20Z","published":"2024-03-25T02:17:20Z","title":"Distilling Semantic Priors from SAM to Efficient Image Restoration\n  Models","summary":"  In image restoration (IR), leveraging semantic priors from segmentation\nmodels has been a common approach to improve performance. The recent segment\nanything model (SAM) has emerged as a powerful tool for extracting advanced\nsemantic priors to enhance IR tasks. However, the computational cost of SAM is\nprohibitive for IR, compared to existing smaller IR models. The incorporation\nof SAM for extracting semantic priors considerably hampers the model inference\nefficiency. To address this issue, we propose a general framework to distill\nSAM's semantic knowledge to boost exiting IR models without interfering with\ntheir inference process. Specifically, our proposed framework consists of the\nsemantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)\nscheme. SPF fuses two kinds of information between the restored image predicted\nby the original IR model and the semantic mask predicted by SAM for the refined\nrestored image. SPD leverages a self-distillation manner to distill the fused\nsemantic priors to boost the performance of original IR models. Additionally,\nwe design a semantic-guided relation (SGR) module for SPD, which ensures\nsemantic feature representation space consistency to fully distill the priors.\nWe demonstrate the effectiveness of our framework across multiple IR models and\ntasks, including deraining, deblurring, and denoising.\n","authors":["Quan Zhang","Xiaoyu Liu","Wei Li","Hanting Chen","Junchao Liu","Jie Hu","Zhiwei Xiong","Chun Yuan","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16368v1.pdf","comment":null}]},"2024-03-24T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.16210v1","updated":"2024-03-24T16:09:21Z","published":"2024-03-24T16:09:21Z","title":"Frankenstein: Generating Semantic-Compositional 3D Scenes in One\n  Tri-Plane","summary":"  We present Frankenstein, a diffusion-based framework that can generate\nsemantic-compositional 3D scenes in a single pass. Unlike existing methods that\noutput a single, unified 3D shape, Frankenstein simultaneously generates\nmultiple separated shapes, each corresponding to a semantically meaningful\npart. The 3D scene information is encoded in one single tri-plane tensor, from\nwhich multiple Singed Distance Function (SDF) fields can be decoded to\nrepresent the compositional shapes. During training, an auto-encoder compresses\ntri-planes into a latent space, and then the denoising diffusion process is\nemployed to approximate the distribution of the compositional scenes.\nFrankenstein demonstrates promising results in generating room interiors as\nwell as human avatars with automatically separated parts. The generated scenes\nfacilitate many downstream applications, such as part-wise re-texturing, object\nrearrangement in the room or avatar cloth re-targeting.\n","authors":["Han Yan","Yang Li","Zhennan Wu","Shenzhou Chen","Weixuan Sun","Taizhang Shang","Weizhe Liu","Tian Chen","Xiaqiang Dai","Chao Ma","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.16210v1.pdf","comment":"Video: https://youtu.be/lRn-HqyCrLI"}],"Mesh":[{"id":"http://arxiv.org/abs/2310.15168v3","updated":"2024-03-24T23:32:50Z","published":"2023-10-23T17:59:52Z","title":"Ghost on the Shell: An Expressive Representation of General 3D Shapes","summary":"  The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.\n","authors":["Zhen Liu","Yao Feng","Yuliang Xiu","Weiyang Liu","Liam Paull","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2310.15168v3.pdf","comment":"ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:\n  https://gshell3d.github.io/)"}],"NeRF":[{"id":"http://arxiv.org/abs/2403.16224v1","updated":"2024-03-24T16:34:47Z","published":"2024-03-24T16:34:47Z","title":"Inverse Rendering of Glossy Objects via the Neural Plenoptic Function\n  and Radiance Fields","summary":"  Inverse rendering aims at recovering both geometry and materials of objects.\nIt provides a more compatible reconstruction for conventional rendering\nengines, compared with the neural radiance fields (NeRFs). On the other hand,\nexisting NeRF-based inverse rendering methods cannot handle glossy objects with\nlocal light interactions well, as they typically oversimplify the illumination\nas a 2D environmental map, which assumes infinite lights only. Observing the\nsuperiority of NeRFs in recovering radiance fields, we propose a novel 5D\nNeural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more\naccurate lighting-object interactions can be formulated via the rendering\nequation. We also design a material-aware cone sampling strategy to efficiently\nintegrate lights inside the BRDF lobes with the help of pre-filtered radiance\nfields. Our method has two stages: the geometry of the target object and the\npre-filtered environmental radiance fields are reconstructed in the first\nstage, and materials of the target object are estimated in the second stage\nwith the proposed NeP and material-aware cone sampling strategy. Extensive\nexperiments on the proposed real-world and synthetic datasets demonstrate that\nour method can reconstruct high-fidelity geometry/materials of challenging\nglossy objects with complex lighting interactions from nearby objects. Project\nwebpage: https://whyy.site/paper/nep\n","authors":["Haoyuan Wang","Wenbo Hu","Lei Zhu","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2403.16224v1.pdf","comment":"CVPR 2024 paper. Project webpage https://whyy.site/paper/nep"},{"id":"http://arxiv.org/abs/2312.04784v2","updated":"2024-03-24T13:56:31Z","published":"2023-12-08T01:53:06Z","title":"Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular\n  Video","summary":"  Recent advancements in 3D avatar generation excel with multi-view supervision\nfor photorealistic models. However, monocular counterparts lag in quality\ndespite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is\na fully-differentiable pipeline that learns high-fidelity 3D human avatars from\njust a single RGB video. A pose-conditioned deformable NeRF is optimized to\nvolumetrically represent a human subject in canonical T-pose. The canonical\nrepresentation is then leveraged to efficiently associate neural textures using\n2D-3D correspondences. This enables the separation of diffused color generation\nand lighting correction branches that jointly compose an RGB prediction. The\ndesign allows to control intermediate results for human pose, body shape,\ntexture, and lighting with text prompts. An image-conditioned diffusion model\nthereby helps to animate appearance and pose of the 3D avatar to create video\nsequences with previously unseen human motion. Extensive experiments show that\nReCaLaB outperforms previous monocular approaches in terms of image quality for\nimage synthesis tasks. Moreover, natural language offers an intuitive user\ninterface for creative manipulation of 3D human avatars.\n","authors":["Yuchen Rao","Eduardo Perez Pellitero","Benjamin Busam","Yiren Zhou","Jifei Song"],"pdf_url":"https://arxiv.org/pdf/2312.04784v2.pdf","comment":"Video link: https://youtu.be/Oz83z1es2J4"},{"id":"http://arxiv.org/abs/2403.16141v1","updated":"2024-03-24T13:27:49Z","published":"2024-03-24T13:27:49Z","title":"Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes","summary":"  Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic\nscenes often involve explicit modeling of scene dynamics. However, this\napproach faces challenges in modeling scene dynamics in urban environments,\nwhere moving objects of various categories and scales are present. In such\nsettings, it becomes crucial to effectively eliminate moving objects to\naccurately reconstruct static backgrounds. Our research introduces an\ninnovative method, termed here as Entity-NeRF, which combines the strengths of\nknowledge-based and statistical strategies. This approach utilizes entity-wise\nstatistics, leveraging entity segmentation and stationary entity classification\nthrough thing/stuff segmentation. To assess our methodology, we created an\nurban scene dataset masked with moving objects. Our comprehensive experiments\ndemonstrate that Entity-NeRF notably outperforms existing techniques in\nremoving moving objects and reconstructing static urban backgrounds, both\nquantitatively and qualitatively.\n","authors":["Takashi Otonari","Satoshi Ikehata","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2403.16141v1.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2024), Project website:\n  https://otonari726.github.io/entitynerf/"},{"id":"http://arxiv.org/abs/2403.16095v1","updated":"2024-03-24T11:19:59Z","published":"2024-03-24T11:19:59Z","title":"CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D\n  Gaussian Field","summary":"  Recently neural radiance fields (NeRF) have been widely exploited as 3D\nrepresentations for dense simultaneous localization and mapping (SLAM). Despite\ntheir notable successes in surface modeling and novel view synthesis, existing\nNeRF-based methods are hindered by their computationally intensive and\ntime-consuming volume rendering pipeline. This paper presents an efficient\ndense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D\nGaussian field with high consistency and geometric stability. Through an\nin-depth analysis of Gaussian Splatting, we propose several techniques to\nconstruct a consistent and stable 3D Gaussian field suitable for tracking and\nmapping. Additionally, a novel depth uncertainty model is proposed to ensure\nthe selection of valuable Gaussian primitives during optimization, thereby\nimproving tracking efficiency and accuracy. Experiments on various datasets\ndemonstrate that CG-SLAM achieves superior tracking and mapping performance\nwith a notable tracking speed of up to 15 Hz. We will make our source code\npublicly available. Project page: https://zju3dv.github.io/cg-slam.\n","authors":["Jiarui Hu","Xianhao Chen","Boyin Feng","Guanglin Li","Liangjing Yang","Hujun Bao","Guofeng Zhang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16095v1.pdf","comment":"Project Page: https://zju3dv.github.io/cg-slam"},{"id":"http://arxiv.org/abs/2403.16092v1","updated":"2024-03-24T11:09:41Z","published":"2024-03-24T11:09:41Z","title":"Are NeRFs ready for autonomous driving? Towards closing the\n  real-to-simulation gap","summary":"  Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing\nautonomous driving (AD) research, offering scalable closed-loop simulation and\ndata augmentation capabilities. However, to trust the results achieved in\nsimulation, one needs to ensure that AD systems perceive real and rendered data\nin the same way. Although the performance of rendering methods is increasing,\nmany scenarios will remain inherently challenging to reconstruct faithfully. To\nthis end, we propose a novel perspective for addressing the real-to-simulated\ndata gap. Rather than solely focusing on improving rendering fidelity, we\nexplore simple yet effective methods to enhance perception model robustness to\nNeRF artifacts without compromising performance on real data. Moreover, we\nconduct the first large-scale investigation into the real-to-simulated data gap\nin an AD setting using a state-of-the-art neural rendering technique.\nSpecifically, we evaluate object detectors and an online mapping model on real\nand simulated data, and study the effects of different pre-training strategies.\nOur results show notable improvements in model robustness to simulated data,\neven improving real-world performance in some cases. Last, we delve into the\ncorrelation between the real-to-simulated gap and image reconstruction metrics,\nidentifying FID and LPIPS as strong indicators.\n","authors":["Carl Lindström","Georg Hess","Adam Lilja","Maryam Fatemi","Lars Hammarstrand","Christoffer Petersson","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2403.16092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16043v1","updated":"2024-03-24T07:04:08Z","published":"2024-03-24T07:04:08Z","title":"Semantic Is Enough: Only Semantic Information For NeRF Reconstruction","summary":"  Recent research that combines implicit 3D representation with semantic\ninformation, like Semantic-NeRF, has proven that NeRF model could perform\nexcellently in rendering 3D structures with semantic labels. This research aims\nto extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing\nsolely on semantic output and removing the RGB output component. We reformulate\nthe model and its training procedure to leverage only the cross-entropy loss\nbetween the model semantic output and the ground truth semantic images,\nremoving the colour data traditionally used in the original Semantic-NeRF\napproach. We then conduct a series of identical experiments using the original\nand the modified Semantic-NeRF model. Our primary objective is to obverse the\nimpact of this modification on the model performance by Semantic-NeRF, focusing\non tasks such as scene understanding, object detection, and segmentation. The\nresults offer valuable insights into the new way of rendering the scenes and\nprovide an avenue for further research and development in semantic-focused 3D\nscene understanding.\n","authors":["Ruibo Wang","Song Zhang","Ping Huang","Donghai Zhang","Wei Yan"],"pdf_url":"https://arxiv.org/pdf/2403.16043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13897v2","updated":"2024-03-24T05:20:15Z","published":"2023-08-26T14:50:24Z","title":"InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules","summary":"  Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant\nchallenge that existing approaches struggle to address without extensive\nmodifications to vanilla NeRF framework. We introduce InsertNeRF, a method for\nINStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play\nHyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific\nreference scenes, transforming multi-scale sampling-aware features into\nscene-specific representations. This novel design allows for more accurate and\nefficient representations of complex appearances and geometries. Experiments\nshow that this method not only achieves superior generalization performance but\nalso provides a flexible pathway for integration with other NeRF-like systems,\neven in sparse input settings. Code will be available\nhttps://github.com/bbbbby-99/InsertNeRF.\n","authors":["Yanqi Bao","Tianyu Ding","Jing Huo","Wenbin Li","Yuxin Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2308.13897v2.pdf","comment":"This work was accepted at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.15981v1","updated":"2024-03-24T02:15:14Z","published":"2024-03-24T02:15:14Z","title":"Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance\n  Fields","summary":"  Accurate collection of plant phenotyping is critical to optimising\nsustainable farming practices in precision agriculture. Traditional phenotyping\nin controlled laboratory environments, while valuable, falls short in\nunderstanding plant growth under real-world conditions. Emerging sensor and\ndigital technologies offer a promising approach for direct phenotyping of\nplants in farm environments. This study investigates a learning-based\nphenotyping method using the Neural Radiance Field to achieve accurate in-situ\nphenotyping of pepper plants in greenhouse environments. To quantitatively\nevaluate the performance of this method, traditional point cloud registration\non 3D scanning data is implemented for comparison. Experimental result shows\nthat NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the\n3D scanning methods. The mean distance error between the scanner-based method\nand the NeRF-based method is 0.865mm. This study shows that the learning-based\nNeRF method achieves similar accuracy to 3D scanning-based methods but with\nimproved scalability and robustness.\n","authors":["unhong Zhao","Wei Ying","Yaoqiang Pan","Zhenfeng Yi","Chao Chen","Kewei Hu","Hanwen Kang"],"pdf_url":"https://arxiv.org/pdf/2403.15981v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.16205v1","updated":"2024-03-24T15:58:48Z","published":"2024-03-24T15:58:48Z","title":"Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown\n  Domains","summary":"  This paper presents an innovative framework designed to train an image\ndeblurring algorithm tailored to a specific camera device. This algorithm works\nby transforming a blurry input image, which is challenging to deblur, into\nanother blurry image that is more amenable to deblurring. The transformation\nprocess, from one blurry state to another, leverages unpaired data consisting\nof sharp and blurry images captured by the target camera device. Learning this\nblur-to-blur transformation is inherently simpler than direct blur-to-sharp\nconversion, as it primarily involves modifying blur patterns rather than the\nintricate task of reconstructing fine image details. The efficacy of the\nproposed approach has been demonstrated through comprehensive experiments on\nvarious benchmarks, where it significantly outperforms state-of-the-art methods\nboth quantitatively and qualitatively. Our code and data are available at\nhttps://zero1778.github.io/blur2blur/\n","authors":["Bang-Dang Pham","Phong Tran","Anh Tran","Cuong Pham","Rang Nguyen","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2403.16205v1.pdf","comment":"Accepted to CVPR 2024"}]},"2024-03-21T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.14619v1","updated":"2024-03-21T17:59:16Z","published":"2024-03-21T17:59:16Z","title":"ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition","summary":"  3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.\n","authors":["Tianhao Wu","Chuanxia Zheng","Tat-Jen Cham","Qianyi Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14619v1.pdf","comment":"Project Page: https://sm0kywu.github.io/ClusteringSDF/"},{"id":"http://arxiv.org/abs/2403.14366v1","updated":"2024-03-21T12:49:32Z","published":"2024-03-21T12:49:32Z","title":"SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance\n  Field","summary":"  Vision-centric 3D environment understanding is both vital and challenging for\nautonomous driving systems. Recently, object-free methods have attracted\nconsiderable attention. Such methods perceive the world by predicting the\nsemantics of discrete voxel grids but fail to construct continuous and accurate\nobstacle surfaces. To this end, in this paper, we propose SurroundSDF to\nimplicitly predict the signed distance field (SDF) and semantic field for the\ncontinuous perception from surround images. Specifically, we introduce a\nquery-based approach and utilize SDF constrained by the Eikonal formulation to\naccurately describe the surfaces of obstacles. Furthermore, considering the\nabsence of precise SDF ground truth, we propose a novel weakly supervised\nparadigm for SDF, referred to as the Sandwich Eikonal formulation, which\nemphasizes applying correct and dense constraints on both sides of the surface,\nthereby enhancing the perceptual accuracy of the surface. Experiments suggest\nthat our method achieves SOTA for both occupancy prediction and 3D scene\nreconstruction tasks on the nuScenes dataset.\n","authors":["Lizhe Liu","Bohua Wang","Hongwei Xie","Daqi Liu","Li Liu","Zhiqiang Tian","Kuiyuan Yang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14085v1","updated":"2024-03-21T02:31:17Z","published":"2024-03-21T02:31:17Z","title":"Surface Reconstruction from Point Clouds via Grid-based Intersection\n  Prediction","summary":"  Surface reconstruction from point clouds is a crucial task in the fields of\ncomputer vision and computer graphics. SDF-based methods excel at\nreconstructing smooth meshes with minimal error and artifacts but struggle with\nrepresenting open surfaces. On the other hand, UDF-based methods can\neffectively represent open surfaces but often introduce noise near the surface,\nleading to artifacts in the mesh. In this work, we propose a novel approach\nthat directly predicts the intersection points between sampled line segments of\npoint pairs and implicit surfaces. This method not only preserves the ability\nto represent open surfaces but also eliminates artifacts in the mesh. Our\napproach demonstrates state-of-the-art performance on three datasets: ShapeNet,\nMGN, and ScanNet. The code will be made available upon acceptance.\n","authors":["Hui Tian","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14085v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2403.14554v1","updated":"2024-03-21T16:53:03Z","published":"2024-03-21T16:53:03Z","title":"Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering","summary":"  We propose Gaussian Frosting, a novel mesh-based representation for\nhigh-quality rendering and editing of complex 3D effects in real-time. Our\napproach builds on the recent 3D Gaussian Splatting framework, which optimizes\na set of 3D Gaussians to approximate a radiance field from images. We propose\nfirst extracting a base mesh from Gaussians during optimization, then building\nand refining an adaptive layer of Gaussians with a variable thickness around\nthe mesh to better capture the fine details and volumetric effects near the\nsurface, such as hair or grass. We call this layer Gaussian Frosting, as it\nresembles a coating of frosting on a cake. The fuzzier the material, the\nthicker the frosting. We also introduce a parameterization of the Gaussians to\nenforce them to stay inside the frosting layer and automatically adjust their\nparameters when deforming, rescaling, editing or animating the mesh. Our\nrepresentation allows for efficient rendering using Gaussian splatting, as well\nas editing and animation by modifying the base mesh. We demonstrate the\neffectiveness of our method on various synthetic and real scenes, and show that\nit outperforms existing surface-based approaches. We will release our code and\na web-based viewer as additional contributions. Our project page is the\nfollowing: https://anttwo.github.io/frosting/\n","authors":["Antoine Guédon","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2403.14554v1.pdf","comment":"Project Webpage: https://anttwo.github.io/frosting/"},{"id":"http://arxiv.org/abs/2312.09641v2","updated":"2024-03-21T15:57:28Z","published":"2023-12-15T09:30:47Z","title":"Ins-HOI: Instance Aware Human-Object Interactions Recovery","summary":"  Accurately modeling detailed interactions between human/hand and object is an\nappealing yet challenging task. Current multi-view capture systems are only\ncapable of reconstructing multiple subjects into a single, unified mesh, which\nfails to model the states of each instance individually during interactions. To\naddress this, previous methods use template-based representations to track\nhuman/hand and object. However, the quality of the reconstructions is limited\nby the descriptive capabilities of the templates so that these methods are\ninherently struggle with geometry details, pressing deformations and invisible\ncontact surfaces. In this work, we propose an end-to-end Instance-aware\nHuman-Object Interactions recovery (Ins-HOI) framework by introducing an\ninstance-level occupancy field representation. However, the real-captured data\nis presented as a holistic mesh, unable to provide instance-level supervision.\nTo address this, we further propose a complementary training strategy that\nleverages synthetic data to introduce instance-level shape priors, enabling the\ndisentanglement of occupancy fields for different instances. Specifically,\nsynthetic data, created by randomly combining individual scans of humans/hands\nand objects, guides the network to learn a coarse prior of instances.\nMeanwhile, real-captured data helps in learning the overall geometry and\nrestricting interpenetration in contact areas. As demonstrated in experiments,\nour method Ins-HOI supports instance-level reconstruction and provides\nreasonable and realistic invisible contact surfaces even in cases of extremely\nclose interaction. To facilitate the research of this task, we collect a\nlarge-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans\nwith real-world human-chair and hand-object interactions. The code and data\nwill be public for research purposes.\n","authors":["Jiajun Zhang","Yuxiang Zhang","Hongwen Zhang","Xiao Zhou","Boyao Zhou","Ruizhi Shao","Zonghai Hu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09641v2.pdf","comment":"Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and\n  Dataset Page: https://github.com/jiajunzhang16/ins-hoi"},{"id":"http://arxiv.org/abs/2403.14376v1","updated":"2024-03-21T13:06:57Z","published":"2024-03-21T13:06:57Z","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space\n  Complexity","summary":"  The conventional mesh-based Level of Detail (LoD) technique, exemplified by\napplications such as Google Earth and many game engines, exhibits the\ncapability to holistically represent a large scene even the Earth, and achieves\nrendering with a space complexity of O(log n). This constrained data\nrequirement not only enhances rendering efficiency but also facilitates dynamic\ndata fetching, thereby enabling a seamless 3D navigation experience for users.\nIn this work, we extend this proven LoD technique to Neural Radiance Fields\n(NeRF) by introducing an octree structure to represent the scenes in different\nscales. This innovative approach provides a mathematically simple and elegant\nrepresentation with a rendering space complexity of O(log n), aligned with the\nefficiency of mesh-based LoD techniques. We also present a novel training\nstrategy that maintains a complexity of O(n). This strategy allows for parallel\ntraining with minimal overhead, ensuring the scalability and efficiency of our\nproposed method. Our contribution is not only in extending the capabilities of\nexisting techniques but also in establishing a foundation for scalable and\nefficient large-scale scene representation using NeRF and octree structures.\n","authors":["Jiabin Liang","Lanqing Zhang","Zhuoran Zhao","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14085v1","updated":"2024-03-21T02:31:17Z","published":"2024-03-21T02:31:17Z","title":"Surface Reconstruction from Point Clouds via Grid-based Intersection\n  Prediction","summary":"  Surface reconstruction from point clouds is a crucial task in the fields of\ncomputer vision and computer graphics. SDF-based methods excel at\nreconstructing smooth meshes with minimal error and artifacts but struggle with\nrepresenting open surfaces. On the other hand, UDF-based methods can\neffectively represent open surfaces but often introduce noise near the surface,\nleading to artifacts in the mesh. In this work, we propose a novel approach\nthat directly predicts the intersection points between sampled line segments of\npoint pairs and implicit surfaces. This method not only preserves the ability\nto represent open surfaces but also eliminates artifacts in the mesh. Our\napproach demonstrates state-of-the-art performance on three datasets: ShapeNet,\nMGN, and ScanNet. The code will be made available upon acceptance.\n","authors":["Hui Tian","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14085v1.pdf","comment":null}],"NeRF":[{"id":"http://arxiv.org/abs/2312.02362v2","updated":"2024-03-21T21:28:37Z","published":"2023-12-04T21:43:00Z","title":"PointNeRF++: A multi-scale, point-based Neural Radiance Field","summary":"  Point clouds offer an attractive source of information to complement images\nin neural scene representations, especially when few images are available.\nNeural rendering methods based on point clouds do exist, but they do not\nperform well when the point cloud quality is low -- e.g., sparse or incomplete,\nwhich is often the case with real-world data. We overcome these problems with a\nsimple representation that aggregates point clouds at multiple scale levels\nwith sparse voxel grids at different resolutions. To deal with point cloud\nsparsity, we average across multiple scale levels -- but only among those that\nare valid, i.e., that have enough neighboring points in proximity to the ray of\na pixel. To help model areas without points, we add a global voxel at the\ncoarsest scale, thus unifying ``classical'' and point-based NeRF formulations.\nWe validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,\noutperforming the state of the art, with a significant gap compared to other\nNeRF-based methods, especially on more challenging scenes.\n","authors":["Weiwei Sun","Eduard Trulls","Yang-Che Tseng","Sneha Sambandam","Gopal Sharma","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2312.02362v2.pdf","comment":"Project website: https://pointnerfpp.github.io/"},{"id":"http://arxiv.org/abs/2403.14839v1","updated":"2024-03-21T21:18:08Z","published":"2024-03-21T21:18:08Z","title":"Hyperspectral Neural Radiance Fields","summary":"  Hyperspectral Imagery (HSI) has been used in many applications to\nnon-destructively determine the material and/or chemical compositions of\nsamples. There is growing interest in creating 3D hyperspectral\nreconstructions, which could provide both spatial and spectral information\nwhile also mitigating common HSI challenges such as non-Lambertian surfaces and\ntranslucent objects. However, traditional 3D reconstruction with HSI is\ndifficult due to technological limitations of hyperspectral cameras. In recent\nyears, Neural Radiance Fields (NeRFs) have seen widespread success in creating\nhigh quality volumetric 3D representations of scenes captured by a variety of\ncamera models. Leveraging recent advances in NeRFs, we propose computing a\nhyperspectral 3D reconstruction in which every point in space and view\ndirection is characterized by wavelength-dependent radiance and transmittance\nspectra. To evaluate our approach, a dataset containing nearly 2000\nhyperspectral images across 8 scenes and 2 cameras was collected. We perform\ncomparisons against traditional RGB NeRF baselines and apply ablation testing\nwith alternative spectra representations. Finally, we demonstrate the potential\nof hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor\nsimulation. We show that our hyperspectral NeRF approach enables creating fast,\naccurate volumetric 3D hyperspectral scenes and enables several new\napplications and areas for future study.\n","authors":["Gerry Chen","Sunil Kumar Narayanan","Thomas Gautier Ottou","Benjamin Missaoui","Harsh Muriki","Cédric Pradalier","Yongsheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14839v1.pdf","comment":"Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6\n  pages"},{"id":"http://arxiv.org/abs/2403.14619v1","updated":"2024-03-21T17:59:16Z","published":"2024-03-21T17:59:16Z","title":"ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition","summary":"  3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.\n","authors":["Tianhao Wu","Chuanxia Zheng","Tat-Jen Cham","Qianyi Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14619v1.pdf","comment":"Project Page: https://sm0kywu.github.io/ClusteringSDF/"},{"id":"http://arxiv.org/abs/2402.17797v3","updated":"2024-03-21T16:11:23Z","published":"2024-02-26T22:00:59Z","title":"Neural Radiance Fields in Medical Imaging: Challenges and Next Steps","summary":"  Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,\noffer great potential to revolutionize medical imaging by synthesizing\nthree-dimensional representations from the projected two-dimensional image\ndata. However, they face unique challenges when applied to medical\napplications. This paper presents a comprehensive examination of applications\nof NeRFs in medical imaging, highlighting four imminent challenges, including\nfundamental imaging principles, inner structure requirement, object boundary\ndefinition, and color density significance. We discuss current methods on\ndifferent organs and discuss related limitations. We also review several\ndatasets and evaluation metrics and propose several promising directions for\nfuture research.\n","authors":["Xin Wang","Shu Hu","Heng Fan","Hongtu Zhu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2402.17797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02015v2","updated":"2024-03-21T15:32:35Z","published":"2023-12-04T16:38:16Z","title":"ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy","summary":"  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEX\nscores. Qualitatively, our reconstruction visualizations show much clearer\ntextures and more accurate geometric details. These sufficiently demonstrate\nour superior performance over the state-of-the-art methods.\n","authors":["Yufei Shi","Beijia Lu","Jia-Wei Liu","Ming Li","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.02015v2.pdf","comment":"for Project Page, see https://showlab.github.io/ColonNeRF/"},{"id":"http://arxiv.org/abs/2403.14412v1","updated":"2024-03-21T13:59:00Z","published":"2024-03-21T13:59:00Z","title":"CombiNeRF: A Combination of Regularization Techniques for Few-Shot\n  Neural Radiance Field View Synthesis","summary":"  Neural Radiance Fields (NeRFs) have shown impressive results for novel view\nsynthesis when a sufficiently large amount of views are available. When dealing\nwith few-shot settings, i.e. with a small set of input views, the training\ncould overfit those views, leading to artifacts and geometric and chromatic\ninconsistencies in the resulting rendering. Regularization is a valid solution\nthat helps NeRF generalization. On the other hand, each of the most recent NeRF\nregularization techniques aim to mitigate a specific rendering problem.\nStarting from this observation, in this paper we propose CombiNeRF, a framework\nthat synergically combines several regularization techniques, some of them\nnovel, in order to unify the benefits of each. In particular, we regularize\nsingle and neighboring rays distributions and we add a smoothness term to\nregularize near geometries. After these geometric approaches, we propose to\nexploit Lipschitz regularization to both NeRF density and color networks and to\nuse encoding masks for input features regularization. We show that CombiNeRF\noutperforms the state-of-the-art methods with few-shot settings in several\npublicly available datasets. We also present an ablation study on the LLFF and\nNeRF-Synthetic datasets that support the choices made. We release with this\npaper the open-source implementation of our framework.\n","authors":["Matteo Bonotto","Luigi Sarrocco","Daniele Evangelista","Marco Imperoli","Alberto Pretto"],"pdf_url":"https://arxiv.org/pdf/2403.14412v1.pdf","comment":"This paper has been accepted for publication at the 2024\n  International Conference on 3D Vision (3DV)"},{"id":"http://arxiv.org/abs/2403.14376v1","updated":"2024-03-21T13:06:57Z","published":"2024-03-21T13:06:57Z","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space\n  Complexity","summary":"  The conventional mesh-based Level of Detail (LoD) technique, exemplified by\napplications such as Google Earth and many game engines, exhibits the\ncapability to holistically represent a large scene even the Earth, and achieves\nrendering with a space complexity of O(log n). This constrained data\nrequirement not only enhances rendering efficiency but also facilitates dynamic\ndata fetching, thereby enabling a seamless 3D navigation experience for users.\nIn this work, we extend this proven LoD technique to Neural Radiance Fields\n(NeRF) by introducing an octree structure to represent the scenes in different\nscales. This innovative approach provides a mathematically simple and elegant\nrepresentation with a rendering space complexity of O(log n), aligned with the\nefficiency of mesh-based LoD techniques. We also present a novel training\nstrategy that maintains a complexity of O(n). This strategy allows for parallel\ntraining with minimal overhead, ensuring the scalability and efficiency of our\nproposed method. Our contribution is not only in extending the capabilities of\nexisting techniques but also in establishing a foundation for scalable and\nefficient large-scale scene representation using NeRF and octree structures.\n","authors":["Jiabin Liang","Lanqing Zhang","Zhuoran Zhao","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02712v2","updated":"2024-03-21T07:20:35Z","published":"2023-10-04T10:28:38Z","title":"ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space\n  NeRF","summary":"  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n","authors":["Jangho Park","Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2310.02712v2.pdf","comment":"ICLR 2024; Project Page: https://jhq1234.github.io/ed-nerf.github.io/"},{"id":"http://arxiv.org/abs/2309.06030v4","updated":"2024-03-21T05:32:37Z","published":"2023-09-12T08:04:56Z","title":"Federated Learning for Large-Scale Scene Modeling with Neural Radiance\n  Fields","summary":"  We envision a system to continuously build and maintain a map based on\nearth-scale neural radiance fields (NeRF) using data collected from vehicles\nand drones in a lifelong learning manner. However, existing large-scale\nmodeling by NeRF has problems in terms of scalability and maintainability when\nmodeling earth-scale environments. Therefore, to address these problems, we\npropose a federated learning pipeline for large-scale modeling with NeRF. We\ntailor the model aggregation pipeline in federated learning for NeRF, thereby\nallowing local updates of NeRF. In the aggregation step, the accuracy of the\nclients' global pose is critical. Thus, we also propose global pose alignment\nto align the noisy global pose of clients before the aggregation step. In\nexperiments, we show the effectiveness of the proposed pose alignment and the\nfederated learning pipeline on the large-scale scene dataset, Mill19.\n","authors":["Teppei Suzuki"],"pdf_url":"https://arxiv.org/pdf/2309.06030v4.pdf","comment":"Our subsequent work is available at arXiv:2403.11460"},{"id":"http://arxiv.org/abs/2403.14053v1","updated":"2024-03-21T00:35:31Z","published":"2024-03-21T00:35:31Z","title":"Leveraging Thermal Modality to Enhance Reconstruction in Low-Light\n  Conditions","summary":"  Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view\nsynthesis by learning the implicit volumetric representation of a scene from\nmulti-view images, which faithfully convey the colorimetric information.\nHowever, sensor noises will contaminate low-value pixel signals, and the lossy\ncamera image signal processor will further remove near-zero intensities in\nextremely dark situations, deteriorating the synthesis performance. Existing\napproaches reconstruct low-light scenes from raw images but struggle to recover\ntexture and boundary details in dark regions. Additionally, they are unsuitable\nfor high-speed models relying on explicit representations. To address these\nissues, we present Thermal-NeRF, which takes thermal and visible raw images as\ninputs, considering the thermal camera is robust to the illumination variation\nand raw images preserve any possible clues in the dark, to accomplish visible\nand thermal view synthesis simultaneously. Also, the first multi-view thermal\nand visible dataset (MVTV) is established to support the research on multimodal\nNeRF. Thermal-NeRF achieves the best trade-off between detail preservation and\nnoise smoothing and provides better synthesis performance than previous work.\nFinally, we demonstrate that both modalities are beneficial to each other in 3D\nreconstruction.\n","authors":["Jiacong Xu","Mingqian Liao","K Ram Prabhakar","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2403.14053v1.pdf","comment":"25 pages, 13 figures"}],"HDR":[{"id":"http://arxiv.org/abs/2403.14836v1","updated":"2024-03-21T21:11:23Z","published":"2024-03-21T21:11:23Z","title":"Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis","summary":"  This paper presents the use of panoramic 3D estimation in lighting\nsimulation. Conventional lighting simulation necessitates detailed modeling as\ninput, resulting in significant labor effort and time cost. The 3D layout\nestimation method directly takes a single panorama as input and generates a\nlighting simulation model with room geometry and window aperture. We evaluate\nthe simulation results by comparing the luminance errors between on-site High\nDynamic Range (HDR) photographs, 3D estimation model, and detailed model in\npanoramic representation and fisheye perspective. Given the selected scene, the\nresults demonstrate the estimated room layout is reliable for lighting\nsimulation.\n","authors":["Zining Cheng","Guanzhou Ji"],"pdf_url":"https://arxiv.org/pdf/2403.14836v1.pdf","comment":"Annual Modeling and Simulation Conference (ANNSIM), May 20-23, 2024,\n  Washington D.C., USA"}],"Deblur":[{"id":"http://arxiv.org/abs/2403.14614v1","updated":"2024-03-21T17:58:14Z","published":"2024-03-21T17:58:14Z","title":"AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and\n  Modulation","summary":"  In the image acquisition process, various forms of degradation, including\nnoise, haze, and rain, are frequently introduced. These degradations typically\narise from the inherent limitations of cameras or unfavorable ambient\nconditions. To recover clean images from degraded versions, numerous\nspecialized restoration methods have been developed, each targeting a specific\ntype of degradation. Recently, all-in-one algorithms have garnered significant\nattention by addressing different types of degradations within a single model\nwithout requiring prior information of the input degradation type. However,\nthese methods purely operate in the spatial domain and do not delve into the\ndistinct frequency variations inherent to different degradation types. To\naddress this gap, we propose an adaptive all-in-one image restoration network\nbased on frequency mining and modulation. Our approach is motivated by the\nobservation that different degradation types impact the image content on\ndifferent frequency subbands, thereby requiring different treatments for each\nrestoration task. Specifically, we first mine low- and high-frequency\ninformation from the input features, guided by the adaptively decoupled spectra\nof the degraded image. The extracted features are then modulated by a\nbidirectional operator to facilitate interactions between different frequency\ncomponents. Finally, the modulated features are merged into the original input\nfor a progressively guided restoration. With this approach, the model achieves\nadaptive reconstruction by accentuating the informative frequency subbands\naccording to different input degradations. Extensive experiments demonstrate\nthat the proposed method achieves state-of-the-art performance on different\nimage restoration tasks, including denoising, dehazing, deraining, motion\ndeblurring, and low-light image enhancement. Our code is available at\nhttps://github.com/c-yn/AdaIR.\n","authors":["Yuning Cui","Syed Waqas Zamir","Salman Khan","Alois Knoll","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14614v1.pdf","comment":"28 pages,15 figures"}]},"2024-03-19T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.12870v1","updated":"2024-03-19T16:15:08Z","published":"2024-03-19T16:15:08Z","title":"PoNQ: a Neural QEM-based Mesh Representation","summary":"  Although polygon meshes have been a standard representation in geometry\nprocessing, their irregular and combinatorial nature hinders their suitability\nfor learning-based applications. In this work, we introduce a novel learnable\nmesh representation through a set of local 3D sample Points and their\nassociated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,\nwhich we denote PoNQ. A global mesh is directly derived from PoNQ by\nefficiently leveraging the knowledge of the local quadric errors. Besides\nmarking the first use of QEM within a neural shape representation, our\ncontribution guarantees both topological and geometrical properties by ensuring\nthat a PoNQ mesh does not self-intersect and is always the boundary of a\nvolume. Notably, our representation does not rely on a regular grid, is\nsupervised directly by the target surface alone, and also handles open surfaces\nwith boundaries and/or sharp features. We demonstrate the efficacy of PoNQ\nthrough a learning-based mesh prediction from SDF grids and show that our\nmethod surpasses recent state-of-the-art techniques in terms of both surface\nand edge-based metrics.\n","authors":["Nissim Maruani","Maks Ovsjanikov","Pierre Alliez","Mathieu Desbrun"],"pdf_url":"https://arxiv.org/pdf/2403.12870v1.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2305.12621v3","updated":"2024-03-19T19:42:14Z","published":"2023-05-22T01:14:30Z","title":"DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images","summary":"  In recent years, deep learning (DL) has shown great potential in the field of\ndermatological image analysis. However, existing datasets in this domain have\nsignificant limitations, including a small number of image samples, limited\ndisease conditions, insufficient annotations, and non-standardized image\nacquisitions. To address these shortcomings, we propose a novel framework\ncalled DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured\nmeshes of human subjects using a differentiable renderer and generates 2D\nimages from various camera viewpoints under chosen lighting conditions in\ndiverse background scenes. Our method adheres to top-down rules that constrain\nthe blending and rendering process to create 2D images with skin conditions\nthat mimic in-the-wild acquisitions, ensuring more meaningful results. The\nframework generates photo-realistic 2D dermoscopy images and the corresponding\ndense annotations for semantic segmentation of the skin, skin conditions, body\nparts, bounding boxes around lesions, depth maps, and other 3D scene\nparameters, such as camera position and lighting conditions. DermSynth3D allows\nfor the creation of custom datasets for various dermatology tasks. We\ndemonstrate the effectiveness of data generated using DermSynth3D by training\nDL models on synthetic data and evaluating them on various dermatology tasks\nusing real 2D dermatological images. We make our code publicly available at\nhttps://github.com/sfu-mial/DermSynth3D.\n","authors":["Ashish Sinha","Jeremy Kawahara","Arezou Pakzad","Kumar Abhishek","Matthieu Ruthven","Enjie Ghorbel","Anis Kacem","Djamila Aouada","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2305.12621v3.pdf","comment":"Accepted to Medical Image Analysis (MedIA) 2024"},{"id":"http://arxiv.org/abs/2403.13064v1","updated":"2024-03-19T18:01:29Z","published":"2024-03-19T18:01:29Z","title":"SceneScript: Reconstructing Scenes With An Autoregressive Structured\n  Language Model","summary":"  We introduce SceneScript, a method that directly produces full scene models\nas a sequence of structured language commands using an autoregressive,\ntoken-based approach. Our proposed scene representation is inspired by recent\nsuccesses in transformers & LLMs, and departs from more traditional methods\nwhich commonly describe scenes as meshes, voxel grids, point clouds or radiance\nfields. Our method infers the set of structured language commands directly from\nencoded visual data using a scene language encoder-decoder architecture. To\ntrain SceneScript, we generate and release a large-scale synthetic dataset\ncalled Aria Synthetic Environments consisting of 100k high-quality in-door\nscenes, with photorealistic and ground-truth annotated renders of egocentric\nscene walkthroughs. Our method gives state-of-the art results in architectural\nlayout estimation, and competitive results in 3D object detection. Lastly, we\nexplore an advantage for SceneScript, which is the ability to readily adapt to\nnew commands via simple additions to the structured language, which we\nillustrate for tasks such as coarse 3D object part reconstruction.\n","authors":["Armen Avetisyan","Christopher Xie","Henry Howard-Jenkins","Tsun-Yi Yang","Samir Aroudj","Suvam Patra","Fuyang Zhang","Duncan Frost","Luke Holland","Campbell Orme","Jakob Engel","Edward Miller","Richard Newcombe","Vasileios Balntas"],"pdf_url":"https://arxiv.org/pdf/2403.13064v1.pdf","comment":"see project page, https://projectaria.com/scenescript"},{"id":"http://arxiv.org/abs/2403.12032v2","updated":"2024-03-19T16:45:22Z","published":"2024-03-18T17:59:09Z","title":"Generic 3D Diffusion Adapter Using Controlled Multi-View Editing","summary":"  Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.\n","authors":["Hansheng Chen","Ruoxi Shi","Yulin Liu","Bokui Shen","Jiayuan Gu","Gordon Wetzstein","Hao Su","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2403.12032v2.pdf","comment":"V2 note: Fix missing acknowledgements. Project page:\n  https://lakonik.github.io/mvedit"},{"id":"http://arxiv.org/abs/2403.12870v1","updated":"2024-03-19T16:15:08Z","published":"2024-03-19T16:15:08Z","title":"PoNQ: a Neural QEM-based Mesh Representation","summary":"  Although polygon meshes have been a standard representation in geometry\nprocessing, their irregular and combinatorial nature hinders their suitability\nfor learning-based applications. In this work, we introduce a novel learnable\nmesh representation through a set of local 3D sample Points and their\nassociated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,\nwhich we denote PoNQ. A global mesh is directly derived from PoNQ by\nefficiently leveraging the knowledge of the local quadric errors. Besides\nmarking the first use of QEM within a neural shape representation, our\ncontribution guarantees both topological and geometrical properties by ensuring\nthat a PoNQ mesh does not self-intersect and is always the boundary of a\nvolume. Notably, our representation does not rely on a regular grid, is\nsupervised directly by the target surface alone, and also handles open surfaces\nwith boundaries and/or sharp features. We demonstrate the efficacy of PoNQ\nthrough a learning-based mesh prediction from SDF grids and show that our\nmethod surpasses recent state-of-the-art techniques in terms of both surface\nand edge-based metrics.\n","authors":["Nissim Maruani","Maks Ovsjanikov","Pierre Alliez","Mathieu Desbrun"],"pdf_url":"https://arxiv.org/pdf/2403.12870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02778v2","updated":"2024-03-19T15:05:22Z","published":"2023-11-05T21:46:12Z","title":"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis","summary":"  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n","authors":["Xuqian Ren","Wenjia Wang","Dingding Cai","Tuuli Tuominen","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2311.02778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12473v1","updated":"2024-03-19T06:18:25Z","published":"2024-03-19T06:18:25Z","title":"PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human\n  Mesh Recovery","summary":"  With the recent advancements in single-image-based human mesh recovery, there\nis a growing interest in enhancing its performance in certain extreme\nscenarios, such as occlusion, while maintaining overall model accuracy.\nAlthough obtaining accurately annotated 3D human poses under occlusion is\nchallenging, there is still a wealth of rich and precise 2D pose annotations\nthat can be leveraged. However, existing works mostly focus on directly\nleveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, we\npresent PostoMETRO($\\textbf{Pos}$e $\\textbf{to}$ken enhanced $\\textbf{ME}$sh\n$\\textbf{TR}$ansf$\\textbf{O}$rmer), which integrates occlusion-resilient 2D\npose representation into transformers in a token-wise manner. Utilizing a\nspecialized pose tokenizer, we efficiently condense 2D pose data to a compact\nsequence of pose tokens and feed them to the transformer together with the\nimage tokens. This process not only ensures a rich depiction of texture from\nthe image but also fosters a robust integration of pose and image information.\nSubsequently, these combined tokens are queried by vertex and joint tokens to\ndecode 3D coordinates of mesh vertices and human joints. Facilitated by the\nrobust pose token representation and the effective combination, we are able to\nproduce more precise 3D coordinates, even under extreme scenarios like\nocclusion. Experiments on both standard and occlusion-specific benchmarks\ndemonstrate the effectiveness of PostoMETRO. Qualitative results further\nillustrate the clarity of how 2D pose can help 3D reconstruction. Code will be\nmade available.\n","authors":["Wendi Yang","Zihang Jiang","Shang Zhao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09981v2","updated":"2024-03-19T05:17:18Z","published":"2024-03-15T02:57:20Z","title":"Controllable Text-to-3D Generation via Surface-Aligned Gaussian\n  Splatting","summary":"  While text-to-3D and image-to-3D generation tasks have received considerable\nattention, one important but under-explored field between them is controllable\ntext-to-3D generation, which we mainly focus on in this work. To address this\ntask, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network\narchitecture designed to enhance existing pre-trained multi-view diffusion\nmodels by integrating additional input conditions, such as edge, depth, normal,\nand scribble maps. Our innovation lies in the introduction of a conditioning\nmodule that controls the base diffusion model using both local and global\nembeddings, which are computed from the input condition images and camera\nposes. Once trained, MVControl is able to offer 3D diffusion guidance for\noptimization-based 3D generation. And, 2) we propose an efficient multi-stage\n3D generation pipeline that leverages the benefits of recent large\nreconstruction models and score distillation algorithm. Building upon our\nMVControl architecture, we employ a unique hybrid diffusion guidance method to\ndirect the optimization process. In pursuit of efficiency, we adopt 3D\nGaussians as our representation instead of the commonly used implicit\nrepresentations. We also pioneer the use of SuGaR, a hybrid representation that\nbinds Gaussians to mesh triangle faces. This approach alleviates the issue of\npoor geometry in 3D Gaussians and enables the direct sculpting of fine-grained\ngeometry on the mesh. Extensive experiments demonstrate that our method\nachieves robust generalization and enables the controllable generation of\nhigh-quality 3D content.\n","authors":["Zhiqi Li","Yiming Chen","Lingzhe Zhao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.09981v2.pdf","comment":"Project page: https://lizhiqi49.github.io/MVControl/"},{"id":"http://arxiv.org/abs/2304.10523v2","updated":"2024-03-19T02:42:28Z","published":"2023-04-20T17:52:58Z","title":"GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape\n  Generative Models","summary":"  This paper introduces GenCorres, a novel unsupervised joint shape matching\n(JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized\ndeformable shape collection while constraining deformations between adjacent\nsynthetic shapes to preserve geometric structures such as local rigidity and\nlocal conformality. GenCorres presents three appealing advantages over existing\nJSM techniques. First, GenCorres performs JSM among a synthetic shape\ncollection whose size is much bigger than the input shapes and fully leverages\nthe datadriven power of JSM. Second, GenCorres unifies consistent shape\nmatching and pairwise matching (i.e., by enforcing deformation priors between\nadjacent synthetic shapes). Third, the generator provides a concise encoding of\nconsistent shape correspondences. However, learning a mesh generator from an\nunorganized shape collection is challenging, requiring a good initialization.\nGenCorres addresses this issue by learning an implicit generator from the input\nshapes, which provides intermediate shapes between two arbitrary shapes. We\nintroduce a novel approach for computing correspondences between adjacent\nimplicit surfaces, which we use to regularize the implicit generator. Synthetic\nshapes of the implicit generator then guide initial fittings (i.e., via\ntemplate-based deformation) for learning the mesh generator. Experimental\nresults show that GenCorres considerably outperforms state-of-the-art JSM\ntechniques. The synthetic shapes of GenCorres also achieve salient performance\ngains against state-of-the-art deformable shape generators.\n","authors":["Haitao Yang","Xiangru Huang","Bo Sun","Chandrajit Bajaj","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2304.10523v2.pdf","comment":"ICLR 2024"}],"NeRF":[{"id":"http://arxiv.org/abs/2403.13206v1","updated":"2024-03-19T23:54:07Z","published":"2024-03-19T23:54:07Z","title":"Depth-guided NeRF Training via Earth Mover's Distance","summary":"  Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of\npredicted viewpoints. However, the photometric loss often does not provide\nenough information to disambiguate between different possible geometries\nyielding the same image. Previous work has thus incorporated depth supervision\nduring NeRF training, leveraging dense predictions from pre-trained depth\nnetworks as pseudo-ground truth. While these depth priors are assumed to be\nperfect once filtered for noise, in practice, their accuracy is more\nchallenging to capture. This work proposes a novel approach to uncertainty in\ndepth priors for NeRF supervision. Instead of using custom-trained depth or\nuncertainty priors, we use off-the-shelf pretrained diffusion models to predict\ndepth and capture uncertainty during the denoising process. Because we know\nthat depth priors are prone to errors, we propose to supervise the ray\ntermination distance distribution with Earth Mover's Distance instead of\nenforcing the rendered depth to replicate the depth prior exactly through\nL2-loss. Our depth-guided NeRF outperforms all baselines on standard depth\nmetrics by a large margin while maintaining performance on photometric\nmeasures.\n","authors":["Anita Rau","Josiah Aklilu","F. Christopher Holsinger","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2403.13206v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2403.13199v1","updated":"2024-03-19T23:23:35Z","published":"2024-03-19T23:23:35Z","title":"DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced\n  Images","summary":"  Neural radiance fields (NeRFs) show potential for transforming images\ncaptured worldwide into immersive 3D visual experiences. However, most of this\ncaptured visual data remains siloed in our camera rolls as these images contain\npersonal details. Even if made public, the problem of learning 3D\nrepresentations of billions of scenes captured daily in a centralized manner is\ncomputationally intractable. Our approach, DecentNeRF, is the first attempt at\ndecentralized, crowd-sourced NeRFs that require $\\sim 10^4\\times$ less server\ncomputing for a scene than a centralized approach. Instead of sending the raw\ndata, our approach requires users to send a 3D representation, distributing the\nhigh computation cost of training centralized NeRFs between the users. It\nlearns photorealistic scene representations by decomposing users' 3D views into\npersonal and global NeRFs and a novel optimally weighted aggregation of only\nthe latter. We validate the advantage of our approach to learn NeRFs with\nphotorealism and minimal server computation cost on structured synthetic and\nreal-world photo tourism datasets. We further analyze how secure aggregation of\nglobal NeRFs in DecentNeRF minimizes the undesired reconstruction of personal\ncontent by the server.\n","authors":["Zaid Tasneem","Akshat Dave","Abhishek Singh","Kushagra Tiwary","Praneeth Vepakomma","Ashok Veeraraghavan","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2403.13199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02350v2","updated":"2024-03-19T21:54:22Z","published":"2023-12-04T21:29:31Z","title":"Instant Uncertainty Calibration of NeRFs Using a Meta-calibrator","summary":"  Although Neural Radiance Fields (NeRFs) have markedly improved novel view\nsynthesis, accurate uncertainty quantification in their image predictions\nremains an open problem. The prevailing methods for estimating uncertainty,\nincluding the state-of-the-art Density-aware NeRF Ensembles (DANE) [29],\nquantify uncertainty without calibration. This frequently leads to over- or\nunder-confidence in image predictions, which can undermine their real-world\napplications. In this paper, we propose a method which, for the first time,\nachieves calibrated uncertainties for NeRFs. To accomplish this, we overcome a\nsignificant challenge in adapting existing calibration techniques to NeRFs: a\nneed to hold out ground truth images from the target scene, reducing the number\nof images left to train the NeRF. This issue is particularly problematic in\nsparse-view settings, where we can operate with as few as three images. To\naddress this, we introduce the concept of a meta-calibrator that performs\nuncertainty calibration for NeRFs with a single forward pass without the need\nfor holding out any images from the target scene. Our meta-calibrator is a\nneural network that takes as input the NeRF images and uncalibrated uncertainty\nmaps and outputs a scene-specific calibration curve that corrects the NeRF's\nuncalibrated uncertainties. We show that the meta-calibrator can generalize on\nunseen scenes and achieves well-calibrated and state-of-the-art uncertainty for\nNeRFs, significantly beating DANE and other approaches. This opens\nopportunities to improve applications that rely on accurate NeRF uncertainty\nestimates such as next-best view planning and potentially more trustworthy\nimage reconstruction for medical diagnosis.\n","authors":["Niki Amini-Naieni","Tomas Jakab","Andrea Vedaldi","Ronald Clark"],"pdf_url":"https://arxiv.org/pdf/2312.02350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12839v1","updated":"2024-03-19T15:45:54Z","published":"2024-03-19T15:45:54Z","title":"Global-guided Focal Neural Radiance Field for Large-scale Scene\n  Rendering","summary":"  Neural radiance fields~(NeRF) have recently been applied to render\nlarge-scale scenes. However, their limited model capacity typically results in\nblurred rendering results. Existing large-scale NeRFs primarily address this\nlimitation by partitioning the scene into blocks, which are subsequently\nhandled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and\nprocessed independently, lead to inconsistencies in geometry and appearance\nacross the scene. Consequently, the rendering quality fails to exhibit\nsignificant improvement despite the expansion of model capacity. In this work,\nwe present global-guided focal neural radiance field (GF-NeRF) that achieves\nhigh-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a\ntwo-stage (Global and Focal) architecture and a global-guided training\nstrategy. The global stage obtains a continuous representation of the entire\nscene while the focal stage decomposes the scene into multiple blocks and\nfurther processes them with distinct sub-encoders. Leveraging this two-stage\narchitecture, sub-encoders only need fine-tuning based on the global encoder,\nthus reducing training complexity in the focal stage while maintaining\nscene-wide consistency. Spatial information and error information from the\nglobal stage also benefit the sub-encoders to focus on crucial areas and\neffectively capture more details of large-scale scenes. Notably, our approach\ndoes not rely on any prior knowledge about the target scene, attributing\nGF-NeRF adaptable to various large-scale scene types, including street-view and\naerial-view scenes. We demonstrate that our method achieves high-fidelity,\nnatural rendering results on various types of large-scale datasets. Our project\npage: https://shaomq2187.github.io/GF-NeRF/\n","authors":["Mingqi Shao","Feng Xiong","Hang Zhang","Shuang Yang","Mu Xu","Wei Bian","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14823v3","updated":"2024-03-19T15:02:04Z","published":"2022-11-27T13:31:00Z","title":"3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer\n  Avenue","summary":"  This paper studies how to flexibly integrate reconstructed 3D models into\npractical 3D modeling pipelines such as 3D scene creation and rendering. Due to\nthe technical difficulty, one can only obtain rough 3D models (R3DMs) for most\nreal objects using existing 3D reconstruction techniques. As a result,\nphysically-based rendering (PBR) would render low-quality images or videos for\nscenes that are constructed by R3DMs. One promising solution would be\nrepresenting real-world objects as Neural Fields such as NeRFs, which are able\nto generate photo-realistic renderings of an object under desired viewpoints.\nHowever, a drawback is that the synthesized views through Neural Fields\nRendering (NFR) cannot reflect the simulated lighting details on R3DMs in PBR\npipelines, especially when object interactions in the 3D scene creation cause\nlocal shadows. To solve this dilemma, we propose a lighting transfer network\n(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.\nLighTNet reasons about a simplified image composition model, remedies the\nuneven surface issue caused by R3DMs, and is empowered by several\nperceptual-motivated constraints and a new Lab angle loss which enhances the\ncontrast between lighting strength and colors. Comparisons demonstrate that\nLighTNet is superior in synthesizing impressive lighting, and is promising in\npushing NFR further in practical 3D modeling workflows.\n","authors":["Bowen Cai","Yujie Li","Yuqin Liang","Rongfei Jia","Binqiang Zhao","Mingming Gong","Huan Fu"],"pdf_url":"https://arxiv.org/pdf/2211.14823v3.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI), project page:\n  http://3d-front-future.github.io/LighTNet"},{"id":"http://arxiv.org/abs/2403.12800v1","updated":"2024-03-19T15:01:18Z","published":"2024-03-19T15:01:18Z","title":"Learning Neural Volumetric Pose Features for Camera Localization","summary":"  We introduce a novel neural volumetric pose feature, termed PoseMap, designed\nto enhance camera localization by encapsulating the information between images\nand the associated camera poses. Our framework leverages an Absolute Pose\nRegression (APR) architecture, together with an augmented NeRF module. This\nintegration not only facilitates the generation of novel views to enrich the\ntraining dataset but also enables the learning of effective pose features.\nAdditionally, we extend our architecture for self-supervised online alignment,\nallowing our method to be used and fine-tuned for unlabelled images within a\nunified framework. Experiments demonstrate that our method achieves 14.28% and\n20.51% performance gain on average in indoor and outdoor benchmark scenes,\noutperforming existing APR methods with state-of-the-art accuracy.\n","authors":["Jingyu Lin","Jiaqi Gu","Bojian Wu","Lubin Fan","Renjie Chen","Ligang Liu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2403.12800v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.12682v1","updated":"2024-03-19T12:36:51Z","published":"2024-03-19T12:36:51Z","title":"IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single\n  image and a NeRF model","summary":"  We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera\npose of a given image, building on the Neural Radiance Fields (NeRF)\nformulation. IFFNeRF is specifically designed to operate in real-time and\neliminates the need for an initial pose guess that is proximate to the sought\nsolution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface\npoints from within the NeRF model. From these sampled points, we cast rays and\ndeduce the color for each ray through pixel-level view synthesis. The camera\npose can then be estimated as the solution to a Least Squares problem by\nselecting correspondences between the query image and the resulting bundle. We\nfacilitate this process through a learned attention mechanism, bridging the\nquery image embedding with the embedding of parameterized rays, thereby\nmatching rays pertinent to the image. Through synthetic and real evaluation\nsettings, we show that our method can improve the angular and translation error\naccuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing\nat 34fps on consumer hardware and not requiring the initial pose guess.\n","authors":["Matteo Bortolon","Theodore Tsesmelis","Stuart James","Fabio Poiesi","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2403.12682v1.pdf","comment":"Accepted ICRA 2024, Project page:\n  https://mbortolon97.github.io/iffnerf/"},{"id":"http://arxiv.org/abs/2403.11831v2","updated":"2024-03-19T11:31:44Z","published":"2024-03-18T14:43:04Z","title":"BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting","summary":"  While neural rendering has demonstrated impressive capabilities in 3D scene\nreconstruction and novel view synthesis, it heavily relies on high-quality\nsharp images and accurate camera poses. Numerous approaches have been proposed\nto train Neural Radiance Fields (NeRF) with motion-blurred images, commonly\nencountered in real-world scenarios such as low-light or long-exposure\nconditions. However, the implicit representation of NeRF struggles to\naccurately recover intricate details from severely motion-blurred images and\ncannot achieve real-time rendering. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction and real-time\nrendering by explicitly optimizing point clouds as Gaussian spheres.\n  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle\nAdjusted Deblur Gaussian Splatting), which leverages explicit Gaussian\nrepresentation and handles severe motion-blurred images with inaccurate camera\nposes to achieve high-quality scene reconstruction. Our method models the\nphysical image formation process of motion-blurred images and jointly learns\nthe parameters of Gaussians while recovering camera motion trajectories during\nexposure time.\n  In our experiments, we demonstrate that BAD-Gaussians not only achieves\nsuperior rendering quality compared to previous state-of-the-art deblur neural\nrendering methods on both synthetic and real datasets but also enables\nreal-time rendering capabilities.\n  Our project page and source code is available at\nhttps://lingzhezhao.github.io/BAD-Gaussians/\n","authors":["Lingzhe Zhao","Peng Wang","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11831v2.pdf","comment":"Project Page and Source Code:\n  https://lingzhezhao.github.io/BAD-Gaussians/"},{"id":"http://arxiv.org/abs/2403.10103v2","updated":"2024-03-19T08:56:44Z","published":"2024-03-15T08:48:37Z","title":"DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video","summary":"  Recent advancements in dynamic neural radiance field methods have yielded\nremarkable outcomes. However, these approaches rely on the assumption of sharp\ninput images. When faced with motion blur, existing dynamic NeRF methods often\nstruggle to generate high-quality novel views. In this paper, we propose\nDyBluRF, a dynamic radiance field approach that synthesizes sharp novel views\nfrom a monocular video affected by motion blur. To account for motion blur in\ninput images, we simultaneously capture the camera trajectory and object\nDiscrete Cosine Transform (DCT) trajectories within the scene. Additionally, we\nemploy a global cross-time rendering approach to ensure consistent temporal\ncoherence across the entire scene. We curate a dataset comprising diverse\ndynamic scenes that are specifically tailored for our task. Experimental\nresults on our dataset demonstrate that our method outperforms existing\napproaches in generating sharp novel views from motion-blurred inputs while\nmaintaining spatial-temporal consistency of the scene.\n","authors":["Huiqiang Sun","Xingyi Li","Liao Shen","Xinyi Ye","Ke Xian","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2403.10103v2.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://huiqiang-sun.github.io/dyblurf/"},{"id":"http://arxiv.org/abs/2304.04962v2","updated":"2024-03-19T08:28:46Z","published":"2023-04-11T04:12:31Z","title":"Mask-Based Modeling for Neural Radiance Fields","summary":"  Most Neural Radiance Fields (NeRFs) exhibit limited generalization\ncapabilities, which restrict their applicability in representing multiple\nscenes using a single model. To address this problem, existing generalizable\nNeRF methods simply condition the model on image features. These methods still\nstruggle to learn precise global representations over diverse scenes since they\nlack an effective mechanism for interacting among different points and views.\nIn this work, we unveil that 3D implicit representation learning can be\nsignificantly improved by mask-based modeling. Specifically, we propose masked\nray and view modeling for generalizable NeRF (MRVM-NeRF), which is a\nself-supervised pretraining target to predict complete scene representations\nfrom partially masked features along each ray. With this pretraining target,\nMRVM-NeRF enables better use of correlations across different points and views\nas the geometry priors, which thereby strengthens the capability of capturing\nintricate details within the scenes and boosts the generalization capability\nacross different scenes. Extensive experiments demonstrate the effectiveness of\nour proposed MRVM-NeRF on both synthetic and real-world datasets, qualitatively\nand quantitatively. Besides, we also conduct experiments to show the\ncompatibility of our proposed method with various backbones and its superiority\nunder few-shot cases.\n","authors":["Ganlin Yang","Guoqiang Wei","Zhizheng Zhang","Yan Lu","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2304.04962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10147v2","updated":"2024-03-19T03:03:14Z","published":"2024-03-15T09:47:35Z","title":"GGRt: Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time","summary":"  This paper presents GGRt, a novel approach to generalizable novel view\nsynthesis that alleviates the need for real camera poses, complexity in\nprocessing high-resolution images, and lengthy optimization processes, thus\nfacilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in\nreal-world scenarios. Specifically, we design a novel joint learning framework\nthat consists of an Iterative Pose Optimization Network (IPO-Net) and a\nGeneralizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,\nthe proposed framework can inherently estimate robust relative pose information\nfrom the image observations and thus primarily alleviate the requirement of\nreal camera poses. Moreover, we implement a deferred back-propagation mechanism\nthat enables high-resolution training and inference, overcoming the resolution\nconstraints of previous methods. To enhance the speed and efficiency, we\nfurther introduce a progressive Gaussian cache module that dynamically adjusts\nduring training and inference. As the first pose-free generalizable 3D-GS\nframework, GGRt achieves inference at $\\ge$ 5 FPS and real-time rendering at\n$\\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our\nmethod outperforms existing NeRF-based pose-free techniques in terms of\ninference speed and effectiveness. It can also approach the real pose-based\n3D-GS methods. Our contributions provide a significant leap forward for the\nintegration of computer vision and computer graphics into practical\napplications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open\ndatasets and enabling real-time rendering for immersive experiences.\n","authors":["Hao Li","Yuanyuan Gao","Chenming Wu","Dingwen Zhang","Yalun Dai","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2403.10147v2.pdf","comment":"Project page:\n  \\href{https://3d-aigc.github.io/GGRt}{https://3d-aigc.github.io/GGRt}"},{"id":"http://arxiv.org/abs/2311.06455v2","updated":"2024-03-19T02:59:03Z","published":"2023-11-11T01:56:35Z","title":"Aria-NeRF: Multimodal Egocentric View Synthesis","summary":"  We seek to accelerate research in developing rich, multimodal scene models\ntrained from egocentric data, based on differentiable volumetric ray-tracing\ninspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like\nmodel from an egocentric image sequence plays a pivotal role in understanding\nhuman behavior and holds diverse applications within the realms of VR/AR. Such\negocentric NeRF-like models may be used as realistic simulations, contributing\nsignificantly to the advancement of intelligent agents capable of executing\ntasks in the real-world. The future of egocentric view synthesis may lead to\nnovel environment representations going beyond today's NeRFs by augmenting\nvisual data with multimodal sensors such as IMU for egomotion tracking, audio\nsensors to capture surface texture and human language context, and eye-gaze\ntrackers to infer human attention patterns in the scene. To support and\nfacilitate the development and evaluation of egocentric multimodal scene\nmodeling, we present a comprehensive multimodal egocentric video dataset. This\ndataset offers a comprehensive collection of sensory data, featuring RGB\nimages, eye-tracking camera footage, audio recordings from a microphone,\natmospheric pressure readings from a barometer, positional coordinates from\nGPS, connectivity details from Wi-Fi and Bluetooth, and information from\ndual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The\ndataset was collected with the Meta Aria Glasses wearable device platform. The\ndiverse data modalities and the real-world context captured within this dataset\nserve as a robust foundation for furthering our understanding of human behavior\nand enabling more immersive and intelligent experiences in the realms of VR,\nAR, and robotics.\n","authors":["Jiankai Sun","Jianing Qiu","Chuanyang Zheng","John Tucker","Javier Yu","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2311.06455v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.13163v1","updated":"2024-03-19T21:31:31Z","published":"2024-03-19T21:31:31Z","title":"DeblurDiNAT: A Lightweight and Effective Transformer for Image\n  Deblurring","summary":"  Blurry images may contain local and global non-uniform artifacts, which\ncomplicate the deblurring process and make it more challenging to achieve\nsatisfactory results. Recently, Transformers generate improved deblurring\noutcomes than existing CNN architectures. However, the large model size and\nlong inference time are still two bothersome issues which have not been fully\nexplored. To this end, we propose DeblurDiNAT, a compact encoder-decoder\nTransformer which efficiently restores clean images from real-world blurry\nones. We adopt an alternating dilation factor structure with the aim of\nglobal-local feature learning. Also, we observe that simply using\nself-attention layers in networks does not always produce good deblurred\nresults. To solve this problem, we propose a channel modulation self-attention\n(CMSA) block, where a cross-channel learner (CCL) is utilized to capture\nchannel relationships. In addition, we present a divide and multiply\nfeed-forward network (DMFN) allowing fast feature propagation. Moreover, we\ndesign a lightweight gated feature fusion (LGFF) module, which performs\ncontrolled feature merging. Comprehensive experimental results show that the\nproposed model, named DeblurDiNAT, provides a favorable performance boost\nwithout introducing noticeable computational costs over the baseline, and\nachieves state-of-the-art (SOTA) performance on several image deblurring\ndatasets. Compared to nearest competitors, our space-efficient and time-saving\nmethod demonstrates a stronger generalization ability with 3%-68% fewer\nparameters and produces deblurred images that are visually closer to the ground\ntruth.\n","authors":["Hanzhou Liu","Binghan Li","Chengkai Liu","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11232v2","updated":"2024-03-19T17:05:57Z","published":"2023-12-18T14:30:54Z","title":"Self-Supervised Learning for Image Super-Resolution and Deblurring","summary":"  Self-supervised methods have recently proved to be nearly as effective as\nsupervised methods in various imaging inverse problems, paving the way for\nlearning-based methods in scientific and medical imaging applications where\nground truth data is hard or expensive to obtain. This is the case in magnetic\nresonance imaging and computed tomography. These methods critically rely on\ninvariance to translations and/or rotations of the image distribution to learn\nfrom incomplete measurement data alone. However, existing approaches fail to\nobtain competitive performances in the problems of image super-resolution and\ndeblurring, which play a key role in most imaging systems. In this work, we\nshow that invariance to translations and rotations is insufficient to learn\nfrom measurements that only contain low-frequency information. Instead, we\npropose a new self-supervised approach that leverages the fact that many image\ndistributions are approximately scale-invariant, and that enables recovering\nhigh-frequency information lost in the measurement process. We demonstrate\nthroughout a series of experiments on real datasets that the proposed method\noutperforms other self-supervised approaches, and obtains performances on par\nwith fully supervised learning.\n","authors":["Jérémy Scanvic","Mike Davies","Patrice Abry","Julián Tachella"],"pdf_url":"https://arxiv.org/pdf/2312.11232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11831v2","updated":"2024-03-19T11:31:44Z","published":"2024-03-18T14:43:04Z","title":"BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting","summary":"  While neural rendering has demonstrated impressive capabilities in 3D scene\nreconstruction and novel view synthesis, it heavily relies on high-quality\nsharp images and accurate camera poses. Numerous approaches have been proposed\nto train Neural Radiance Fields (NeRF) with motion-blurred images, commonly\nencountered in real-world scenarios such as low-light or long-exposure\nconditions. However, the implicit representation of NeRF struggles to\naccurately recover intricate details from severely motion-blurred images and\ncannot achieve real-time rendering. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction and real-time\nrendering by explicitly optimizing point clouds as Gaussian spheres.\n  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle\nAdjusted Deblur Gaussian Splatting), which leverages explicit Gaussian\nrepresentation and handles severe motion-blurred images with inaccurate camera\nposes to achieve high-quality scene reconstruction. Our method models the\nphysical image formation process of motion-blurred images and jointly learns\nthe parameters of Gaussians while recovering camera motion trajectories during\nexposure time.\n  In our experiments, we demonstrate that BAD-Gaussians not only achieves\nsuperior rendering quality compared to previous state-of-the-art deblur neural\nrendering methods on both synthetic and real datasets but also enables\nreal-time rendering capabilities.\n  Our project page and source code is available at\nhttps://lingzhezhao.github.io/BAD-Gaussians/\n","authors":["Lingzhe Zhao","Peng Wang","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11831v2.pdf","comment":"Project Page and Source Code:\n  https://lingzhezhao.github.io/BAD-Gaussians/"},{"id":"http://arxiv.org/abs/2403.06793v2","updated":"2024-03-19T04:46:42Z","published":"2024-03-11T15:11:57Z","title":"Boosting Image Restoration via Priors from Pre-trained Models","summary":"  Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.\n","authors":["Xiaogang Xu","Shu Kong","Tao Hu","Zhe Liu","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2403.06793v2.pdf","comment":"CVPR2024"}]},"2024-03-18T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.11899v1","updated":"2024-03-18T15:58:03Z","published":"2024-03-18T15:58:03Z","title":"GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with\n  Noisy Polarization Priors","summary":"  Learning surfaces from neural radiance field (NeRF) became a rising topic in\nMulti-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods\ndemonstrated their ability to reconstruct accurate 3D shapes of Lambertian\nscenes. However, their results on reflective scenes are unsatisfactory due to\nthe entanglement of specular radiance and complicated geometry. To address the\nchallenges, we propose a Gaussian-based representation of normals in SDF\nfields. Supervised by polarization priors, this representation guides the\nlearning of geometry behind the specular reflection and captures more details\nthan existing methods. Moreover, we propose a reweighting strategy in the\noptimization process to alleviate the noise issue of polarization priors. To\nvalidate the effectiveness of our design, we capture polarimetric information,\nand ground truth meshes in additional reflective scenes with various geometry.\nWe also evaluated our framework on the PANDORA dataset. Comparisons prove our\nmethod outperforms existing neural 3D reconstruction methods in reflective\nscenes by a large margin.\n","authors":["LI Yang","WU Ruizheng","LI Jiyong","CHEN Ying-cong"],"pdf_url":"https://arxiv.org/pdf/2403.11899v1.pdf","comment":"Accepted to ICLR 2024 Poster. For the Appendix, please see\n  http://yukiumi13.github.io/gnerp_page"},{"id":"http://arxiv.org/abs/2310.18917v4","updated":"2024-03-18T03:37:31Z","published":"2023-10-29T06:10:46Z","title":"TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural\n  Radiance Fields","summary":"  Previous attempts to integrate Neural Radiance Fields (NeRF) into the\nSimultaneous Localization and Mapping (SLAM) framework either rely on the\nassumption of static scenes or require the ground truth camera poses, which\nimpedes their application in real-world scenarios. In this paper, we propose a\ntime-varying representation to track and reconstruct the dynamic scenes.\nFirstly, two processes, tracking process and mapping process, are\nsimultaneously maintained in our framework. For the tracking process, all input\nimages are uniformly sampled, then progressively trained in a self-supervised\nparadigm. For the mapping process, we leverage motion masks to distinguish\ndynamic objects from static background, and sample more pixels from dynamic\nareas. Secondly, the parameter optimization for both processes consists of two\nstages: the first stage associates time with 3D positions to convert the\ndeformation field to the canonical field. And the second stage associates time\nwith the embeddings of canonical field to obtain colors and Signed Distance\nFunction (SDF). Lastly, we propose a novel keyframe selection strategy based on\nthe overlapping rate. We evaluate our approach on two synthetic datasets and\none real-world dataset. And the experiments validate that our method achieves\ncompetitive results in both tracking and mapping when compared to existing\nstate-of-the-art NeRF-based methods.\n","authors":["Chengyao Duan","Zhiliu Yang"],"pdf_url":"https://arxiv.org/pdf/2310.18917v4.pdf","comment":null}],"Mesh":[{"id":"http://arxiv.org/abs/2403.12028v1","updated":"2024-03-18T17:57:30Z","published":"2024-03-18T17:57:30Z","title":"Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and\n  Detail","summary":"  3D human body reconstruction has been a challenge in the field of computer\nvision. Previous methods are often time-consuming and difficult to capture the\ndetailed appearance of the human body. In this paper, we propose a new method\ncalled \\emph{Ultraman} for fast reconstruction of textured 3D human models from\na single image. Compared to existing techniques, \\emph{Ultraman} greatly\nimproves the reconstruction speed and accuracy while preserving high-quality\ntexture details. We present a set of new frameworks for human reconstruction\nconsisting of three parts, geometric reconstruction, texture generation and\ntexture mapping. Firstly, a mesh reconstruction framework is used, which\naccurately extracts 3D human shapes from a single image. At the same time, we\npropose a method to generate a multi-view consistent image of the human body\nbased on a single image. This is finally combined with a novel texture mapping\nmethod to optimize texture details and ensure color consistency during\nreconstruction. Through extensive experiments and evaluations, we demonstrate\nthe superior performance of \\emph{Ultraman} on various standard datasets. In\naddition, \\emph{Ultraman} outperforms state-of-the-art methods in terms of\nhuman rendering quality and speed. Upon acceptance of the article, we will make\nthe code and data publicly available.\n","authors":["Mingjin Chen","Junhao Chen","Xiaojun Ye","Huan-ang Gao","Xiaoxue Chen","Zhaoxin Fan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12028v1.pdf","comment":"Project Page: https://air-discover.github.io/Ultraman/"},{"id":"http://arxiv.org/abs/2403.11990v1","updated":"2024-03-18T17:25:36Z","published":"2024-03-18T17:25:36Z","title":"GetMesh: A Controllable Model for High-quality Mesh Generation and\n  Manipulation","summary":"  Mesh is a fundamental representation of 3D assets in various industrial\napplications, and is widely supported by professional softwares. However, due\nto its irregular structure, mesh creation and manipulation is often\ntime-consuming and labor-intensive. In this paper, we propose a highly\ncontrollable generative model, GetMesh, for mesh generation and manipulation\nacross different categories. By taking a varying number of points as the latent\nrepresentation, and re-organizing them as triplane representation, GetMesh\ngenerates meshes with rich and sharp details, outperforming both\nsingle-category and multi-category counterparts. Moreover, it also enables\nfine-grained control over the generation process that previous mesh generative\nmodels cannot achieve, where changing global/local mesh topologies,\nadding/removing mesh parts, and combining mesh parts across categories can be\nintuitively, efficiently, and robustly accomplished by adjusting the number,\npositions or features of latent points. Project page is\nhttps://getmesh.github.io.\n","authors":["Zhaoyang Lyu","Ben Fei","Jinyi Wang","Xudong Xu","Ya Zhang","Weidong Yang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.11990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11899v1","updated":"2024-03-18T15:58:03Z","published":"2024-03-18T15:58:03Z","title":"GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with\n  Noisy Polarization Priors","summary":"  Learning surfaces from neural radiance field (NeRF) became a rising topic in\nMulti-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods\ndemonstrated their ability to reconstruct accurate 3D shapes of Lambertian\nscenes. However, their results on reflective scenes are unsatisfactory due to\nthe entanglement of specular radiance and complicated geometry. To address the\nchallenges, we propose a Gaussian-based representation of normals in SDF\nfields. Supervised by polarization priors, this representation guides the\nlearning of geometry behind the specular reflection and captures more details\nthan existing methods. Moreover, we propose a reweighting strategy in the\noptimization process to alleviate the noise issue of polarization priors. To\nvalidate the effectiveness of our design, we capture polarimetric information,\nand ground truth meshes in additional reflective scenes with various geometry.\nWe also evaluated our framework on the PANDORA dataset. Comparisons prove our\nmethod outperforms existing neural 3D reconstruction methods in reflective\nscenes by a large margin.\n","authors":["LI Yang","WU Ruizheng","LI Jiyong","CHEN Ying-cong"],"pdf_url":"https://arxiv.org/pdf/2403.11899v1.pdf","comment":"Accepted to ICLR 2024 Poster. For the Appendix, please see\n  http://yukiumi13.github.io/gnerp_page"},{"id":"http://arxiv.org/abs/2403.11790v1","updated":"2024-03-18T13:47:18Z","published":"2024-03-18T13:47:18Z","title":"Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical\n  Shape Modeling","summary":"  Shape reconstruction from imaging volumes is a recurring need in medical\nimage analysis. Common workflows start with a segmentation step, followed by\ncareful post-processing and,finally, ad hoc meshing algorithms. As this\nsequence can be timeconsuming, neural networks are trained to reconstruct\nshapes through template deformation. These networks deliver state-ofthe-art\nresults without manual intervention, but, so far, they have primarily been\nevaluated on anatomical shapes with little topological variety between\nindividuals. In contrast, other works favor learning implicit shape models,\nwhich have multiple benefits for meshing and visualization. Our work follows\nthis direction by introducing deep medial voxels, a semi-implicit\nrepresentation that faithfully approximates the topological skeleton from\nimaging volumes and eventually leads to shape reconstruction via convolution\nsurfaces. Our reconstruction technique shows potential for both visualization\nand computer simulations.\n","authors":["Antonio Pepe","Richard Schussnig","Jianning Li","Christina Gsaxner","Dieter Schmalstieg","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2403.11790v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.11789v1","updated":"2024-03-18T13:46:52Z","published":"2024-03-18T13:46:52Z","title":"EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh\n  and Implicit Encoding","summary":"  Road surface reconstruction plays a vital role in autonomous driving systems,\nenabling road lane perception and high-precision mapping. Recently, neural\nimplicit encoding has achieved remarkable results in scene representation,\nparticularly in the realistic rendering of scene textures. However, it faces\nchallenges in directly representing geometric information for large-scale\nscenes. To address this, we propose EMIE-MAP, a novel method for large-scale\nroad surface reconstruction based on explicit mesh and implicit encoding. The\nroad geometry is represented using explicit mesh, where each vertex stores\nimplicit encoding representing the color and semantic information. To overcome\nthe difficulty in optimizing road elevation, we introduce a trajectory-based\nelevation initialization and an elevation residual learning method based on\nMulti-Layer Perceptron (MLP). Additionally, by employing implicit encoding and\nmulti-camera color MLPs decoding, we achieve separate modeling of scene\nphysical properties and camera characteristics, allowing surround-view\nreconstruction compatible with different camera models. Our method achieves\nremarkable road surface reconstruction performance in a variety of real-world\nchallenging scenarios.\n","authors":["Wenhua Wu","Qi Wang","Guangming Wang","Junping Wang","Tiankun Zhao","Yang Liu","Dongchao Gao","Zhe Liu","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11634v1","updated":"2024-03-18T10:13:53Z","published":"2024-03-18T10:13:53Z","title":"Personalized 3D Human Pose and Shape Refinement","summary":"  Recently, regression-based methods have dominated the field of 3D human pose\nand shape estimation. Despite their promising results, a common issue is the\nmisalignment between predictions and image observations, often caused by minor\njoint rotation errors that accumulate along the kinematic chain. To address\nthis issue, we propose to construct dense correspondences between initial human\nmodel estimates and the corresponding images that can be used to refine the\ninitial predictions. To this end, we utilize renderings of the 3D models to\npredict per-pixel 2D displacements between the synthetic renderings and the RGB\nimages. This allows us to effectively integrate and exploit appearance\ninformation of the persons. Our per-pixel displacements can be efficiently\ntransformed to per-visible-vertex displacements and then used for 3D model\nrefinement by minimizing a reprojection loss. To demonstrate the effectiveness\nof our approach, we refine the initial 3D human mesh predictions of multiple\nmodels using different refinement procedures on 3DPW and RICH. We show that our\napproach not only consistently leads to better image-model alignment, but also\nto improved 3D accuracy.\n","authors":["Tom Wehrbein","Bodo Rosenhahn","Iain Matthews","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2403.11634v1.pdf","comment":"Accepted to 2023 IEEE/CVF International Conference on Computer Vision\n  Workshops (ICCVW)"},{"id":"http://arxiv.org/abs/2403.11589v1","updated":"2024-03-18T09:03:56Z","published":"2024-03-18T09:03:56Z","title":"UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures\n  for Human Avatar Modeling","summary":"  Reconstructing photo-realistic drivable human avatars from multi-view image\nsequences has been a popular and challenging topic in the field of computer\nvision and graphics. While existing NeRF-based methods can achieve high-quality\nnovel view rendering of human models, both training and inference processes are\ntime-consuming. Recent approaches have utilized 3D Gaussians to represent the\nhuman body, enabling faster training and rendering. However, they undermine the\nimportance of the mesh guidance and directly predict Gaussians in 3D space with\ncoarse mesh guidance. This hinders the learning procedure of the Gaussians and\ntends to produce blurry textures. Therefore, we propose UV Gaussians, which\nmodels the 3D human body by jointly learning mesh deformations and 2D UV-space\nGaussian textures. We utilize the embedding of UV map to learn Gaussian\ntextures in 2D space, leveraging the capabilities of powerful 2D networks to\nextract features. Additionally, through an independent Mesh network, we\noptimize pose-dependent geometric deformations, thereby guiding Gaussian\nrendering and significantly enhancing rendering quality. We collect and process\na new dataset of human motion, which includes multi-view images, scanned\nmodels, parametric model registration, and corresponding texture maps.\nExperimental results demonstrate that our method achieves state-of-the-art\nsynthesis of novel view and novel pose. The code and data will be made\navailable on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the\npaper is accepted.\n","authors":["Yujiao Jiang","Qingmin Liao","Xiaoyu Li","Li Ma","Qi Zhang","Chaopeng Zhang","Zongqing Lu","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2403.11589v1.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2403.11875v1","updated":"2024-03-18T15:27:58Z","published":"2024-03-18T15:27:58Z","title":"Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic\n  Vision Sensors","summary":"  Unmanned Aerial Vehicles (UAVs) are gaining popularity in civil and military\napplications. However, uncontrolled access to restricted areas threatens\nprivacy and security. Thus, prevention and detection of UAVs are pivotal to\nguarantee confidentiality and safety. Although active scanning, mainly based on\nradars, is one of the most accurate technologies, it can be expensive and less\nversatile than passive inspections, e.g., object recognition. Dynamic vision\nsensors (DVS) are bio-inspired event-based vision models that leverage\ntimestamped pixel-level brightness changes in fast-moving scenes that adapt\nwell to low-latency object detection. This paper presents F-UAV-D (Fast\nUnmanned Aerial Vehicle Detector), an embedded system that enables fast-moving\ndrone detection. In particular, we propose a setup to exploit DVS as an\nalternative to RGB cameras in a real-time and low-power configuration. Our\napproach leverages the high-dynamic range (HDR) and background suppression of\nDVS and, when trained with various fast-moving drones, outperforms RGB input in\nsuboptimal ambient conditions such as low illumination and fast-moving scenes.\nOur results show that F-UAV-D can (i) detect drones by using less than <15 W on\naverage and (ii) perform real-time inference (i.e., <50 ms) by leveraging the\nCPU and GPU nodes of our edge computer.\n","authors":["Jakub Mandula","Jonas Kühne","Luca Pascarella","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.11875v1.pdf","comment":"Accepted at 2024 IEEE International Instrumentation and Measurement\n  Technology Conference (I2MTC)"}],"IQA":[{"id":"http://arxiv.org/abs/2403.11397v1","updated":"2024-03-18T01:11:53Z","published":"2024-03-18T01:11:53Z","title":"Defense Against Adversarial Attacks on No-Reference Image Quality Models\n  with Gradient Norm Regularization","summary":"  The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the\nquality score of an input image without additional information. NR-IQA models\nplay a crucial role in the media industry, aiding in performance evaluation and\noptimization guidance. However, these models are found to be vulnerable to\nadversarial attacks, which introduce imperceptible perturbations to input\nimages, resulting in significant changes in predicted scores. In this paper, we\npropose a defense method to improve the stability in predicted scores when\nattacked by small perturbations, thus enhancing the adversarial robustness of\nNR-IQA models. To be specific, we present theoretical evidence showing that the\nmagnitude of score changes is related to the $\\ell_1$ norm of the model's\ngradient with respect to the input image. Building upon this theoretical\nfoundation, we propose a norm regularization training strategy aimed at\nreducing the $\\ell_1$ norm of the gradient, thereby boosting the robustness of\nNR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate\nthe effectiveness of our strategy in reducing score changes in the presence of\nadversarial attacks. To the best of our knowledge, this work marks the first\nattempt to defend against adversarial attacks on NR-IQA models. Our study\noffers valuable insights into the adversarial robustness of NR-IQA models and\nprovides a foundation for future research in this area.\n","authors":["Yujia Liu","Chenxi Yang","Dingquan Li","Jianhao Ding","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.11397v1.pdf","comment":"accepted by CVPR 2024"}],"Deblur":[{"id":"http://arxiv.org/abs/2401.00766v2","updated":"2024-03-18T08:51:58Z","published":"2024-01-01T14:14:35Z","title":"Exposure Bracketing is All You Need for Unifying Image Restoration and\n  Enhancement Tasks","summary":"  It is highly desired but challenging to acquire high-quality photos with\nclear content in low-light environments. Although multi-image processing\nmethods (using burst, dual-exposure, or multi-exposure images) have made\nsignificant progress in addressing this issue, they typically focus on specific\nrestoration or enhancement problems, being insufficient in exploiting\nmulti-image. Motivated by that multi-exposure images are complementary in\ndenoising, deblurring, high dynamic range imaging, and super-resolution, we\npropose to utilize exposure bracketing photography to unify restoration and\nenhancement tasks in this work. Due to the difficulty in collecting real-world\npairs, we suggest a solution that first pre-trains the model with synthetic\npaired data and then adapts it to real-world unlabeled images. In particular, a\ntemporally modulated recurrent network (TMRNet) and self-supervised adaptation\nmethod are proposed. Moreover, we construct a data simulation pipeline to\nsynthesize pairs and collect real-world images from 200 nighttime scenarios.\nExperiments on both datasets show that our method performs favorably against\nthe state-of-the-art multi-image processing ones. The dataset, code, and\npre-trained models are available at https://github.com/cszhilu1998/BracketIRE.\n","authors":["Zhilu Zhang","Shuohao Zhang","Renlong Wu","Zifei Yan","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.00766v2.pdf","comment":"28 pages"}]},"2024-03-14T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2312.03357v2","updated":"2024-03-14T13:58:06Z","published":"2023-12-06T08:54:04Z","title":"RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient\n  Neural Fields","summary":"  Recent advances in Neural Fields mostly rely on developing task-specific\nsupervision which often complicates the models. Rather than developing\nhard-to-combine and specific modules, another approach generally overlooked is\nto directly inject generic priors on the scene representation (also called\ninductive biases) into the NeRF architecture. Based on this idea, we propose\nthe RING-NeRF architecture which includes two inductive biases : a continuous\nmulti-scale representation of the scene and an invariance of the decoder's\nlatent space over spatial and scale domains. We also design a single\nreconstruction process that takes advantage of those inductive biases and\nexperimentally demonstrates on-par performances in terms of quality with\ndedicated architecture on multiple tasks (anti-aliasing, few view\nreconstruction, SDF reconstruction without scene-specific initialization) while\nbeing more efficient. Moreover, RING-NeRF has the distinctive ability to\ndynamically increase the resolution of the model, opening the way to adaptive\nreconstruction.\n","authors":["Doriand Petit","Steve Bourgeois","Dumitru Pavel","Vincent Gay-Bellile","Florian Chabot","Loic Barthe"],"pdf_url":"https://arxiv.org/pdf/2312.03357v2.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2403.09392v1","updated":"2024-03-14T13:45:09Z","published":"2024-03-14T13:45:09Z","title":"Event-based Asynchronous HDR Imaging by Temporal Incident Light\n  Modulation","summary":"  Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current\nframe-based cameras struggle to achieve high dynamic range imaging due to the\nconflict between globally uniform exposure and spatially variant scene\nillumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR\nimaging system, based on key insights into the challenges in HDR imaging and\nthe unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our\nproposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD\npanels modulate the irradiance incident upon the DVS by altering their\ntransparency, thereby triggering the pixel-independent event streams. The HDR\nimage is subsequently decoded from the event streams through our\ntemporal-weighted algorithm. Experiments under standard test platform and\nseveral challenging scenes have verified the feasibility of the system in HDR\nimaging task.\n","authors":["Yuliang Wu","Ganchao Tan","Jinze Chen","Wei Zhai","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2403.09392v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.09486v1","updated":"2024-03-14T15:29:09Z","published":"2024-03-14T15:29:09Z","title":"SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with\n  Spike Streams","summary":"  Reconstructing a sequence of sharp images from the blurry input is crucial\nfor enhancing our insights into the captured scene and poses a significant\nchallenge due to the limited temporal features embedded in the image. Spike\ncameras, sampling at rates up to 40,000 Hz, have proven effective in capturing\nmotion features and beneficial for solving this ill-posed problem. Nonetheless,\nexisting methods fall into the supervised learning paradigm, which suffers from\nnotable performance degradation when applied to real-world scenarios that\ndiverge from the synthetic training data domain. Moreover, the quality of\nreconstructed images is capped by the generated images based on motion analysis\ninterpolation, which inherently differs from the actual scene, affecting the\ngeneralization ability of these methods in real high-speed scenarios. To\naddress these challenges, we propose the first self-supervised framework for\nthe task of spike-guided motion deblurring. Our approach begins with the\nformulation of a spike-guided deblurring model that explores the theoretical\nrelationships among spike streams, blurry images, and their corresponding sharp\nsequences. We subsequently develop a self-supervised cascaded framework to\nalleviate the issues of spike noise and spatial-resolution mismatching\nencountered in the deblurring model. With knowledge distillation and\nre-blurring loss, we further design a lightweight deblur network to generate\nhigh-quality sequences with brightness and texture consistency with the\noriginal input. Quantitative and qualitative experiments conducted on our\nreal-world and synthetic datasets with spikes validate the superior\ngeneralization of the proposed framework. Our code, data and trained models\nwill be available at \\url{https://github.com/chenkang455/S-SDM}.\n","authors":["Kang Chen","Shiyan Chen","Jiyuan Zhang","Baoyue Zhang","Yajing Zheng","Tiejun Huang","Zhaofei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.09486v1.pdf","comment":"14 pages"}]},"2024-03-08T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2208.10769v2","updated":"2024-03-08T08:16:34Z","published":"2022-08-23T07:00:44Z","title":"PIFu for the Real World: A Self-supervised Framework to Reconstruct\n  Dressed Human from Single-view Images","summary":"  It is very challenging to accurately reconstruct sophisticated human geometry\ncaused by various poses and garments from a single image. Recently, works based\non pixel-aligned implicit function (PIFu) have made a big step and achieved\nstate-of-the-art fidelity on image-based 3D human digitization. However, the\ntraining of PIFu relies heavily on expensive and limited 3D ground truth data\n(i.e. synthetic data), thus hindering its generalization to more diverse real\nworld images. In this work, we propose an end-to-end self-supervised network\nnamed SelfPIFu to utilize abundant and diverse in-the-wild images, resulting in\nlargely improved reconstructions when tested on unconstrained in-the-wild\nimages. At the core of SelfPIFu is the depth-guided volume-/surface-aware\nsigned distance fields (SDF) learning, which enables self-supervised learning\nof a PIFu without access to GT mesh. The whole framework consists of a normal\nestimator, a depth estimator, and a SDF-based PIFu and better utilizes extra\ndepth GT during training. Extensive experiments demonstrate the effectiveness\nof our self-supervised framework and the superiority of using depth as input.\nOn synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18%\nhigher compared with PIFuHD. For in-the-wild images, we conduct user studies on\nthe reconstructed results, the selection rate of our results is over 68%\ncompared with other state-of-the-art methods.\n","authors":["Zhangyang Xiong","Dong Du","Yushuang Wu","Jingqi Dong","Di Kang","Linchao Bao","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2208.10769v2.pdf","comment":"CVM 2024"},{"id":"http://arxiv.org/abs/2402.08138v2","updated":"2024-03-08T08:00:47Z","published":"2024-02-13T00:23:31Z","title":"H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object\n  Surface Fields","summary":"  Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance\nFields (SDF), and Occupancy Fields have recently emerged as solutions for 3D\nindoor scene reconstruction. We introduce a novel two-phase learning approach,\nH2O-SDF, that discriminates between object and non-object regions within indoor\nenvironments. This method achieves a nuanced balance, carefully preserving the\ngeometric integrity of room layouts while also capturing intricate surface\ndetails of specific objects. A cornerstone of our two-phase learning framework\nis the introduction of the Object Surface Field (OSF), a novel concept designed\nto mitigate the persistent vanishing gradient problem that has previously\nhindered the capture of high-frequency details in other methods. Our proposed\napproach is validated through several experiments that include ablation\nstudies.\n","authors":["Minyoung Park","Mirae Do","YeonJae Shin","Jaeseok Yoo","Jongkwang Hong","Joongrock Kim","Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2402.08138v2.pdf","comment":null}],"HDR":[{"id":"http://arxiv.org/abs/2403.05660v1","updated":"2024-03-08T20:21:45Z","published":"2024-03-08T20:21:45Z","title":"Decoupling Degradations with Recurrent Network for Video Restoration in\n  Under-Display Camera","summary":"  Under-display camera (UDC) systems are the foundation of full-screen display\ndevices in which the lens mounts under the display. The pixel array of\nlight-emitting diodes used for display diffracts and attenuates incident light,\ncausing various degradations as the light intensity changes. Unlike general\nvideo restoration which recovers video by treating different degradation\nfactors equally, video restoration for UDC systems is more challenging that\nconcerns removing diverse degradation over time while preserving temporal\nconsistency. In this paper, we introduce a novel video restoration network,\ncalled D$^2$RNet, specifically designed for UDC systems. It employs a set of\nDecoupling Attention Modules (DAM) that effectively separate the various video\ndegradation factors. More specifically, a soft mask generation function is\nproposed to formulate each frame into flare and haze based on the diffraction\narising from incident light of different intensities, followed by the proposed\nflare and haze removal components that leverage long- and short-term feature\nlearning to handle the respective degradations. Such a design offers an\ntargeted and effective solution to eliminating various types of degradation in\nUDC systems. We further extend our design into multi-scale to overcome the\nscale-changing of degradation that often occur in long-range videos. To\ndemonstrate the superiority of D$^2$RNet, we propose a large-scale UDC video\nbenchmark by gathering HDR videos and generating realistically degraded videos\nusing the point spread function measured by a commercial UDC system. Extensive\nquantitative and qualitative evaluations demonstrate the superiority of\nD$^2$RNet compared to other state-of-the-art video restoration and UDC image\nrestoration methods. Code is available at\nhttps://github.com/ChengxuLiu/DDRNet.git\n","authors":["Chengxu Liu","Xuan Wang","Yuanting Fan","Shuai Li","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2403.05660v1.pdf","comment":"AAAI 2024"}],"IQA":[{"id":"http://arxiv.org/abs/2403.04993v1","updated":"2024-03-08T02:10:25Z","published":"2024-03-08T02:10:25Z","title":"PromptIQA: Boosting the Performance and Generalization for No-Reference\n  Image Quality Assessment via Prompts","summary":"  Due to the diversity of assessment requirements in various application\nscenarios for the IQA task, existing IQA methods struggle to directly adapt to\nthese varied requirements after training. Thus, when facing new requirements, a\ntypical approach is fine-tuning these models on datasets specifically created\nfor those requirements. However, it is time-consuming to establish IQA\ndatasets. In this work, we propose a Prompt-based IQA (PromptIQA) that can\ndirectly adapt to new requirements without fine-tuning after training. On one\nhand, it utilizes a short sequence of Image-Score Pairs (ISP) as prompts for\ntargeted predictions, which significantly reduces the dependency on the data\nrequirements. On the other hand, PromptIQA is trained on a mixed dataset with\ntwo proposed data augmentation strategies to learn diverse requirements, thus\nenabling it to effectively adapt to new requirements. Experiments indicate that\nthe PromptIQA outperforms SOTA methods with higher performance and better\ngeneralization. The code will be available.\n","authors":["Zewen Chen","Haina Qin","Juan Wang","Chunfeng Yuan","Bing Li","Weiming Hu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.04993v1.pdf","comment":null}]},"2024-03-05T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.02561v1","updated":"2024-03-05T00:34:05Z","published":"2024-03-05T00:34:05Z","title":"Semantic Human Mesh Reconstruction with Textures","summary":"  The field of 3D detailed human mesh reconstruction has made significant\nprogress in recent years. However, current methods still face challenges when\nused in industrial applications due to unstable results, low-quality meshes,\nand a lack of UV unwrapping and skinning weights. In this paper, we present\nSHERT, a novel pipeline that can reconstruct semantic human meshes with\ntextures and high-precision details. SHERT applies semantic- and normal-based\nsampling between the detailed surface (eg mesh and SDF) and the corresponding\nSMPL-X model to obtain a partially sampled semantic mesh and then generates the\ncomplete semantic mesh by our specifically designed self-supervised completion\nand refinement networks. Using the complete semantic mesh as a basis, we employ\na texture diffusion model to create human textures that are driven by both\nimages and texts. Our reconstructed meshes have stable UV unwrapping,\nhigh-quality triangle meshes, and consistent semantic information. The given\nSMPL-X model provides semantic information and shape priors, allowing SHERT to\nperform well even with incorrect and incomplete inputs. The semantic\ninformation also makes it easy to substitute and animate different body parts\nsuch as the face, body, and hands. Quantitative and qualitative experiments\ndemonstrate that SHERT is capable of producing high-fidelity and robust\nsemantic meshes that outperform state-of-the-art methods.\n","authors":["Xiaoyu Zhan","Jianxin Yang","Yuanqi Li","Jie Guo","Yanwen Guo","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2403.02561v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2209.01375v3","updated":"2024-03-05T14:50:18Z","published":"2022-09-03T09:10:23Z","title":"A Variational Approach for Joint Image Recovery and Feature Extraction\n  Based on Spatially-Varying Generalised Gaussian Models","summary":"  The joint problem of reconstruction / feature extraction is a challenging\ntask in image processing. It consists in performing, in a joint manner, the\nrestoration of an image and the extraction of its features. In this work, we\nfirstly propose a novel nonsmooth and non-convex variational formulation of the\nproblem. For this purpose, we introduce a versatile generalised Gaussian prior\nwhose parameters, including its exponent, are space-variant. Secondly, we\ndesign an alternating proximal-based optimisation algorithm that efficiently\nexploits the structure of the proposed non-convex objective function. We also\nanalyse the convergence of this algorithm. As shown in numerical experiments\nconducted on joint deblurring/segmentation tasks, the proposed method provides\nhigh-quality results.\n","authors":["Emilie Chouzenoux","Marie-Caroline Corbineau","Jean-Christophe Pesquet","Gabriele Scrivanti"],"pdf_url":"https://arxiv.org/pdf/2209.01375v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02611v1","updated":"2024-03-05T02:59:35Z","published":"2024-03-05T02:59:35Z","title":"A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid\n  Transformer and Contrastive Learning","summary":"  Defocus blur is a persistent problem in microscope imaging that poses harm to\npathology interpretation and medical intervention in cell microscopy and\nmicroscope surgery. To address this problem, a unified framework including\nmulti-pyramid transformer (MPT) and extended frequency contrastive\nregularization (EFCR) is proposed to tackle two outstanding challenges in\nmicroscopy deblur: longer attention span and feature deficiency. The MPT\nemploys an explicit pyramid structure at each network stage that integrates the\ncross-scale window attention (CSWA), the intra-scale channel attention (ISCA),\nand the feature-enhancing feed-forward network (FEFN) to capture long-range\ncross-scale spatial interaction and global channel context. The EFCR addresses\nthe feature deficiency problem by exploring latent deblur signals from\ndifferent frequency bands. It also enables deblur knowledge transfer to learn\ncross-domain information from extra data, improving deblur performance for\nlabeled and unlabeled data. Extensive experiments and downstream task\nvalidation show the framework achieves state-of-the-art performance across\nmultiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.\n","authors":["Yuelin Zhang","Pengyu Zheng","Wanquan Yan","Chengyu Fang","Shing Shin Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.02611v1.pdf","comment":"Accepted in CVPR 2024"}]},"2024-03-04T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.01861v1","updated":"2024-03-04T09:18:13Z","published":"2024-03-04T09:18:13Z","title":"AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes","summary":"  Indoor scenes we are living in are visually homogenous or textureless, while\nthey inherently have structural forms and provide enough structural priors for\n3D scene reconstruction. Motivated by this fact, we propose a structure-aware\nonline signed distance fields (SDF) reconstruction framework in indoor scenes,\nespecially under the Atlanta world (AW) assumption. Thus, we dub this\nincremental SDF reconstruction for AW as AiSDF. Within the online framework, we\ninfer the underlying Atlanta structure of a given scene and then estimate\nplanar surfel regions supporting the Atlanta structure. This Atlanta-aware\nsurfel representation provides an explicit planar map for a given scene. In\naddition, based on these Atlanta planar surfel regions, we adaptively sample\nand constrain the structural regularity in the SDF reconstruction, which\nenables us to improve the reconstruction quality by maintaining a high-level\nstructure while enhancing the details of a given scene. We evaluate the\nproposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate\nthat the proposed framework is capable of reconstructing fine details of\nobjects implicitly, as well as structures explicitly in room-scale scenes.\n","authors":["Jaehoon Jang","Inha Lee","Minje Kim","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2403.01861v1.pdf","comment":"8 pages, 6 figures, Accepted to IEEE RA-L (First two authors\n  contributed equally)"}],"HDR":[{"id":"http://arxiv.org/abs/2403.02449v1","updated":"2024-03-04T20:05:28Z","published":"2024-03-04T20:05:28Z","title":"Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging","summary":"  High dynamic range (HDR) imaging involves capturing a series of frames of the\nsame scene, each with different exposure settings, to broaden the dynamic range\nof light. This can be achieved through burst capturing or using staggered HDR\nsensors that capture long and short exposures simultaneously in the camera\nimage signal processor (ISP). Within camera ISP pipeline, illuminant estimation\nis a crucial step aiming to estimate the color of the global illuminant in the\nscene. This estimation is used in camera ISP white-balance module to remove\nundesirable color cast in the final image. Despite the multiple frames captured\nin the HDR pipeline, conventional illuminant estimation methods often rely only\non a single frame of the scene. In this paper, we explore leveraging\ninformation from frames captured with different exposure times. Specifically,\nwe introduce a simple feature extracted from dual-exposure images to guide\nilluminant estimators, referred to as the dual-exposure feature (DEF). To\nvalidate the efficiency of DEF, we employed two illuminant estimators using the\nproposed DEF: 1) a multilayer perceptron network (MLP), referred to as\nexposure-based MLP (EMLP), and 2) a modified version of the convolutional color\nconstancy (CCC) to integrate our DEF, that we call ECCC. Both EMLP and ECCC\nachieve promising results, in some cases surpassing prior methods that require\nhundreds of thousands or millions of parameters, with only a few hundred\nparameters for EMLP and a few thousand parameters for ECCC.\n","authors":["Mahmoud Afifi","Zhenhua Hu","Liang Liang"],"pdf_url":"https://arxiv.org/pdf/2403.02449v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2402.16641v2","updated":"2024-03-04T07:06:22Z","published":"2024-02-26T15:10:56Z","title":"Towards Open-ended Visual Quality Comparison","summary":"  Comparative settings (e.g. pairwise choice, listwise ranking) have been\nadopted by a wide range of subjective studies for image quality assessment\n(IQA), as it inherently standardizes the evaluation criteria across different\nobservers and offer more clear-cut responses. In this work, we extend the edge\nof emerging large multi-modality models (LMMs) to further advance visual\nquality comparison into open-ended settings, that 1) can respond to open-range\nquestions on quality comparison; 2) can provide detailed reasonings beyond\ndirect answers. To this end, we propose the Co-Instruct. To train this\nfirst-of-its-kind open-source open-ended visual quality comparer, we collect\nthe Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image\nquality description, (b) GPT-4V \"teacher\" responses on unlabeled data.\nFurthermore, to better evaluate this setting, we propose the MICBench, the\nfirst benchmark on multi-image comparison for LMMs. We demonstrate that\nCo-Instruct not only achieves in average 30% higher accuracy than\nstate-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher),\non both existing related benchmarks and the proposed MICBench. Our model is\npublished at https://huggingface.co/q-future/co-instruct.\n","authors":["Haoning Wu","Hanwei Zhu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Chunyi Li","Annan Wang","Wenxiu Sun","Qiong Yan","Xiaohong Liu","Guangtao Zhai","Shiqi Wang","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2402.16641v2.pdf","comment":"Fix typos"}]},"2024-03-03T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2403.01414v1","updated":"2024-03-03T06:58:35Z","published":"2024-03-03T06:58:35Z","title":"Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit\n  Representation for Diverse 3D Shapes","summary":"  Neural implicit representation of geometric shapes has witnessed considerable\nadvancements in recent years. However, common distance field based implicit\nrepresentations, specifically signed distance field (SDF) for watertight shapes\nor unsigned distance field (UDF) for arbitrary shapes, routinely suffer from\ndegradation of reconstruction accuracy when converting to explicit surface\npoints and meshes. In this paper, we introduce a novel neural implicit\nrepresentation based on unsigned orthogonal distance fields (UODFs). In UODFs,\nthe minimal unsigned distance from any spatial point to the shape surface is\ndefined solely in one orthogonal direction, contrasting with the\nmulti-directional determination made by SDF and UDF. Consequently, every point\nin the 3D UODFs can directly access its closest surface points along three\northogonal directions. This distinctive feature leverages the accurate\nreconstruction of surface points without interpolation errors. We verify the\neffectiveness of UODFs through a range of reconstruction examples, extending\nfrom simple watertight or non-watertight shapes to complex shapes that include\nhollows, internal or assembling structures.\n","authors":["Yujie Lu","Long Wan","Nayu Ding","Yulong Wang","Shuhan Shen","Shen Cai","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.01414v1.pdf","comment":"accepted by CVPR 2024"}]},"2024-02-26T00:00:00Z":{"SDF":[{"id":"http://arxiv.org/abs/2402.17062v1","updated":"2024-02-26T22:48:37Z","published":"2024-02-26T22:48:37Z","title":"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed\n  Distance Fields","summary":"  Human hands are highly articulated and versatile at handling objects. Jointly\nestimating the 3D poses of a hand and the object it manipulates from a\nmonocular camera is challenging due to frequent occlusions. Thus, existing\nmethods often rely on intermediate 3D shape representations to increase\nperformance. These representations are typically explicit, such as 3D point\nclouds or meshes, and thus provide information in the direct surroundings of\nthe intermediate hand pose estimate. To address this, we introduce HOISDF, a\nSigned Distance Field (SDF) guided hand-object pose estimation network, which\njointly exploits hand and object SDFs to provide a global, implicit\nrepresentation over the complete reconstruction volume. Specifically, the role\nof the SDFs is threefold: equip the visual encoder with implicit shape\ninformation, help to encode hand-object interactions, and guide the hand and\nobject pose regression via SDF-based sampling and by augmenting the feature\nrepresentations. We show that HOISDF achieves state-of-the-art results on\nhand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available\nat https://github.com/amathislab/HOISDF\n","authors":["Haozhe Qi","Chen Zhao","Mathieu Salzmann","Alexander Mathis"],"pdf_url":"https://arxiv.org/pdf/2402.17062v1.pdf","comment":"Accepted at CVPR 2024. 9 figures, many tables"}]},"2024-03-27T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2401.00374v4","updated":"2024-03-27T04:06:36Z","published":"2023-12-31T02:25:41Z","title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via\n  Expressive Masked Audio Gesture Modeling","summary":"  We propose EMAGE, a framework to generate full-body human gestures from audio\nand masked gestures, encompassing facial, local body, hands, and global\nmovements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new\nmesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with\nFLAME head parameters and further refines the modeling of head, neck, and\nfinger movements, offering a community-standardized, high-quality 3D motion\ncaptured dataset. EMAGE leverages masked body gesture priors during training to\nboost inference performance. It involves a Masked Audio Gesture Transformer,\nfacilitating joint training on audio-to-gesture generation and masked gesture\nreconstruction to effectively encode audio and body gesture hints. Encoded body\nhints from masked gestures are then separately employed to generate facial and\nbody movements. Moreover, EMAGE adaptively merges speech features from the\naudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\nthe results' fidelity and diversity. Experiments demonstrate that EMAGE\ngenerates holistic gestures with state-of-the-art performance and is flexible\nin accepting predefined spatial-temporal gesture inputs, generating complete,\naudio-synchronized results. Our code and dataset are available at\nhttps://pantomatrix.github.io/EMAGE/\n","authors":["Haiyang Liu","Zihao Zhu","Giorgio Becherini","Yichen Peng","Mingyang Su","You Zhou","Xuefei Zhe","Naoya Iwamoto","Bo Zheng","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2401.00374v4.pdf","comment":"Conflict of Interest Disclosure; CVPR Camera Ready; Project Page:\n  https://pantomatrix.github.io/EMAGE/"}]},"2024-03-23T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2312.05541v2","updated":"2024-03-23T04:54:21Z","published":"2023-12-09T11:18:45Z","title":"DPoser: Diffusion Model as Robust 3D Human Pose Prior","summary":"  This work targets to construct a robust human pose prior. However, it remains\na persistent challenge due to biomechanical constraints and diverse human\nmovements. Traditional priors like VAEs and NDFs often exhibit shortcomings in\nrealism and generalization, notably with unseen noisy poses. To address these\nissues, we introduce DPoser, a robust and versatile human pose prior built upon\ndiffusion models. DPoser regards various pose-centric tasks as inverse problems\nand employs variational diffusion sampling for efficient solving. Accordingly,\ndesigned with optimization frameworks, DPoser seamlessly benefits human mesh\nrecovery, pose generation, pose completion, and motion denoising tasks.\nFurthermore, due to the disparity between the articulated poses and structured\nimages, we propose truncated timestep scheduling to enhance the effectiveness\nof DPoser. Our approach demonstrates considerable enhancements over common\nuniform scheduling used in image domains, boasting improvements of 5.4%, 17.2%,\nand 3.8% across human mesh recovery, pose completion, and motion denoising,\nrespectively. Comprehensive experiments demonstrate the superiority of DPoser\nover existing state-of-the-art pose priors across multiple tasks.\n","authors":["Junzhe Lu","Jing Lin","Hongkun Dou","Ailing Zeng","Yue Deng","Yulun Zhang","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05541v2.pdf","comment":"Project Page: https://dposer.github.io; Code Released:\n  https://github.com/moonbow721/DPoser"},{"id":"http://arxiv.org/abs/2311.17050v2","updated":"2024-03-23T02:22:04Z","published":"2023-11-28T18:56:01Z","title":"Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using\n  Diffusion Models","summary":"  We present Surf-D, a novel method for generating high-quality 3D shapes as\nSurfaces with arbitrary topologies using Diffusion models. Previous methods\nexplored shape generation with different representations and they suffer from\nlimited topologies and poor geometry details. To generate high-quality surfaces\nof arbitrary topologies, we use the Unsigned Distance Field (UDF) as our\nsurface representation to accommodate arbitrary topologies. Furthermore, we\npropose a new pipeline that employs a point-based AutoEncoder to learn a\ncompact and continuous latent space for accurately encoding UDF and support\nhigh-resolution mesh extraction. We further show that our new pipeline\nsignificantly outperforms the prior approaches to learning the distance fields,\nsuch as the grid-based AutoEncoder, which is not scalable and incapable of\nlearning accurate UDF. In addition, we adopt a curriculum learning strategy to\nefficiently embed various surfaces. With the pretrained shape latent space, we\nemploy a latent diffusion model to acquire the distribution of various shapes.\nExtensive experiments are presented on using Surf-D for unconditional\ngeneration, category conditional generation, image conditional generation, and\ntext-to-shape tasks. The experiments demonstrate the superior performance of\nSurf-D in shape generation across multiple modalities as conditions. Visit our\nproject page at https://yzmblog.github.io/projects/SurfD/.\n","authors":["Zhengming Yu","Zhiyang Dou","Xiaoxiao Long","Cheng Lin","Zekun Li","Yuan Liu","Norman Müller","Taku Komura","Marc Habermann","Christian Theobalt","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17050v2.pdf","comment":"Project Page: https://yzmblog.github.io/projects/SurfD/"}],"NeRF":[{"id":"http://arxiv.org/abs/2311.10959v3","updated":"2024-03-23T17:36:19Z","published":"2023-11-18T03:39:02Z","title":"Structure-Aware Sparse-View X-ray 3D Reconstruction","summary":"  X-ray, known for its ability to reveal internal structures of objects, is\nexpected to provide richer information for 3D reconstruction than visible\nlight. Yet, existing neural radiance fields (NeRF) algorithms overlook this\nimportant nature of X-ray, leading to their limitations in capturing structural\ncontents of imaged objects. In this paper, we propose a framework,\nStructure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view\nX-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer\n(Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal\nstructures of objects in 3D space by modeling the dependencies within each line\nsegment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray\nsampling strategy to extract contextual and geometric information in 2D\nprojection. Plus, we collect a larger-scale dataset X3D covering wider X-ray\napplications. Experiments on X3D show that SAX-NeRF surpasses previous\nNeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT\nreconstruction. Code, models, and data are released at\nhttps://github.com/caiyuanhao1998/SAX-NeRF\n","authors":["Yuanhao Cai","Jiahao Wang","Alan Yuille","Zongwei Zhou","Angtian Wang"],"pdf_url":"https://arxiv.org/pdf/2311.10959v3.pdf","comment":"CVPR 2024; The first Transformer-based method for X-ray and CT 3D\n  reconstruction"},{"id":"http://arxiv.org/abs/2403.15705v1","updated":"2024-03-23T03:56:25Z","published":"2024-03-23T03:56:25Z","title":"UPNeRF: A Unified Framework for Monocular 3D Object Reconstruction and\n  Pose Estimation","summary":"  Monocular 3D reconstruction for categorical objects heavily relies on\naccurately perceiving each object's pose. While gradient-based optimization\nwithin a NeRF framework updates initially given poses, this paper highlights\nthat such a scheme fails when the initial pose even moderately deviates from\nthe true pose. Consequently, existing methods often depend on a third-party 3D\nobject to provide an initial object pose, leading to increased complexity and\ngeneralization issues. To address these challenges, we present UPNeRF, a\nUnified framework integrating Pose estimation and NeRF-based reconstruction,\nbringing us closer to real-time monocular 3D object reconstruction. UPNeRF\ndecouples the object's dimension estimation and pose refinement to resolve the\nscale-depth ambiguity, and introduces an effective projected-box representation\nthat generalizes well cross different domains. While using a dedicated pose\nestimator that smoothly integrates into an object-centric NeRF, UPNeRF is free\nfrom external 3D detectors. UPNeRF achieves state-of-the-art results in both\nreconstruction and pose estimation tasks on the nuScenes dataset. Furthermore,\nUPNeRF exhibits exceptional Cross-dataset generalization on the KITTI and Waymo\ndatasets, surpassing prior methods with up to 50% reduction in rotation and\ntranslation error.\n","authors":["Yuliang Guo","Abhinav Kumar","Cheng Zhao","Ruoyu Wang","Xinyu Huang","Liu Ren"],"pdf_url":"https://arxiv.org/pdf/2403.15705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15704v1","updated":"2024-03-23T03:55:41Z","published":"2024-03-23T03:55:41Z","title":"Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image\n  Collections","summary":"  Novel view synthesis from unconstrained in-the-wild images remains a\nmeaningful but challenging task. The photometric variation and transient\noccluders in those unconstrained images make it difficult to reconstruct the\noriginal scene accurately. Previous approaches tackle the problem by\nintroducing a global appearance feature in Neural Radiance Fields (NeRF).\nHowever, in the real world, the unique appearance of each tiny point in a scene\nis determined by its independent intrinsic material attributes and the varying\nenvironmental impacts it receives. Inspired by this fact, we propose Gaussian\nin the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the\nscene and introduces separated intrinsic and dynamic appearance feature for\neach point, capturing the unchanged scene appearance along with dynamic\nvariation like illumination and weather. Additionally, an adaptive sampling\nstrategy is presented to allow each Gaussian point to focus on the local and\ndetailed information more effectively. We also reduce the impact of transient\noccluders using a 2D visibility map. More experiments have demonstrated better\nreconstruction quality and details of GS-W compared to previous methods, with a\n$1000\\times$ increase in rendering speed.\n","authors":["Dongbin Zhang","Chuming Wang","Weitao Wang","Peihao Li","Minghan Qin","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15704v1.pdf","comment":"14 pages, 5 figures"}]},"2024-03-22T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2403.15559v1","updated":"2024-03-22T18:28:04Z","published":"2024-03-22T18:28:04Z","title":"An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes Using Pre-Trained Text-to-Image Models","summary":"  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively.\n","authors":["Zhengyi Zhao","Chen Song","Xiaodong Gu","Yuan Dong","Qi Zuo","Weihao Yuan","Zilong Dong","Liefeng Bo","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.15559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15227v1","updated":"2024-03-22T14:20:54Z","published":"2024-03-22T14:20:54Z","title":"LeGO: Leveraging a Surface Deformation Network for Animatable Stylized\n  Face Generation with One Example","summary":"  Recent advances in 3D face stylization have made significant strides in few\nto zero-shot settings. However, the degree of stylization achieved by existing\nmethods is often not sufficient for practical applications because they are\nmostly based on statistical 3D Morphable Models (3DMM) with limited variations.\nTo this end, we propose a method that can produce a highly stylized 3D face\nmodel with desired topology. Our methods train a surface deformation network\nwith 3DMM and translate its domain to the target style using a paired exemplar.\nThe network achieves stylization of the 3D face mesh by mimicking the style of\nthe target using a differentiable renderer and directional CLIP losses.\nAdditionally, during the inference process, we utilize a Mesh Agnostic Encoder\n(MAGE) that takes deformation target, a mesh of diverse topologies as input to\nthe stylization process and encodes its shape into our latent space. The\nresulting stylized face model can be animated by commonly used 3DMM blend\nshapes. A set of quantitative and qualitative evaluations demonstrate that our\nmethod can produce highly stylized face meshes according to a given style and\noutput them in a desired topology. We also demonstrate example applications of\nour method including image-based stylized avatar generation, linear\ninterpolation of geometric styles, and facial animation of stylized avatars.\n","authors":["Soyeon Yoon","Kwan Yun","Kwanggyoon Seo","Sihun Cha","Jung Eun Yoo","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.15227v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.14370v2","updated":"2024-03-22T10:26:33Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v2.pdf","comment":"Project page: https://synctweedies.github.io/"},{"id":"http://arxiv.org/abs/2403.15009v1","updated":"2024-03-22T07:45:51Z","published":"2024-03-22T07:45:51Z","title":"TexRO: Generating Delicate Textures of 3D Models by Recursive\n  Optimization","summary":"  This paper presents TexRO, a novel method for generating delicate textures of\na known 3D mesh by optimizing its UV texture. The key contributions are\ntwo-fold. We propose an optimal viewpoint selection strategy, that finds the\nmost miniature set of viewpoints covering all the faces of a mesh. Our\nviewpoint selection strategy guarantees the completeness of a generated result.\nWe propose a recursive optimization pipeline that optimizes a UV texture at\nincreasing resolutions, with an adaptive denoising method that re-uses existing\ntextures for new texture generation. Through extensive experimentation, we\ndemonstrate the superior performance of TexRO in terms of texture quality,\ndetail preservation, visual consistency, and, notably runtime speed,\noutperforming other current methods. The broad applicability of TexRO is\nfurther confirmed through its successful use on diverse 3D models.\n","authors":["Jinbo Wu","Xing Liu","Chenming Wu","Xiaobo Gao","Jialun Liu","Xinqi Liu","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15009v1.pdf","comment":"Technical report. Project page:\n  \\href{https://3d-aigc.github.io/TexRO}{https://3d-aigc.github.io/TexRO}"}],"NeRF":[{"id":"http://arxiv.org/abs/2403.15624v1","updated":"2024-03-22T21:28:19Z","published":"2024-03-22T21:28:19Z","title":"Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian\n  Splatting","summary":"  Open-vocabulary 3D scene understanding presents a significant challenge in\ncomputer vision, withwide-ranging applications in embodied agents and augmented\nreality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs)\nto analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel\nopen-vocabulary scene understanding approach based on 3D Gaussian Splatting.\nOur keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design\na versatile projection approachthat maps various 2Dsemantic features from\npre-trained image encoders into a novel semantic component of 3D Gaussians,\nwithoutthe additional training required by NeRFs. We further build a 3D\nsemantic network that directly predictsthe semantic component from raw 3D\nGaussians for fast inference. We explore several applications ofSemantic\nGaussians: semantic segmentation on ScanNet-20, where our approach attains a\n4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene\nunderstanding counterparts; object part segmentation,sceneediting, and\nspatial-temporal segmentation with better qualitative results over 2D and 3D\nbaselines,highlighting its versatility and effectiveness on supporting diverse\ndownstream tasks.\n","authors":["Jun Guo","Xiaojian Ma","Yue Fan","Huaping Liu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.15624v1.pdf","comment":"Project page: see https://semantic-gaussians.github.io"},{"id":"http://arxiv.org/abs/2403.15530v1","updated":"2024-03-22T17:59:21Z","published":"2024-03-22T17:59:21Z","title":"Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis\nresults while advancing real-time rendering performance. However, it relies\nheavily on the quality of the initial point cloud, resulting in blurring and\nneedle-like artifacts in areas with insufficient initializing points. This is\nmainly attributed to the point cloud growth condition in 3DGS that only\nconsiders the average gradient magnitude of points from observable views,\nthereby failing to grow for large Gaussians that are observable for many\nviewpoints while many of them are only covered in the boundaries. To this end,\nwe propose a novel method, named Pixel-GS, to take into account the number of\npixels covered by the Gaussian in each view during the computation of the\ngrowth condition. We regard the covered pixel numbers as the weights to\ndynamically average the gradients from different views, such that the growth of\nlarge Gaussians can be prompted. As a result, points within the areas with\ninsufficient initializing points can be grown more effectively, leading to a\nmore accurate and detailed reconstruction. In addition, we propose a simple yet\neffective strategy to scale the gradient field according to the distance to the\ncamera, to suppress the growth of floaters near the camera. Extensive\nexperiments both qualitatively and quantitatively demonstrate that our method\nachieves state-of-the-art rendering quality while maintaining real-time\nrendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.\n","authors":["Zheng Zhang","Wenbo Hu","Yixing Lao","Tong He","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15272v1","updated":"2024-03-22T15:15:44Z","published":"2024-03-22T15:15:44Z","title":"WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization","summary":"  Despite the advancements in deep learning for camera relocalization tasks,\nobtaining ground truth pose labels required for the training process remains a\ncostly endeavor. While current weakly supervised methods excel in lightweight\nlabel generation, their performance notably declines in scenarios with sparse\nviews. In response to this challenge, we introduce WSCLoc, a system capable of\nbeing customized to various deep learning-based relocalization models to\nenhance their performance under weakly-supervised and sparse view conditions.\nThis is realized with two stages. In the initial stage, WSCLoc employs a\nmultilayer perceptron-based structure called WFT-NeRF to co-optimize image\nreconstruction quality and initial pose information. To ensure a stable\nlearning process, we incorporate temporal information as input. Furthermore,\ninstead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to\nexplicitly enforce a scale constraint. In the second stage, we co-optimize the\npre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by\nTime-Encoding based Random View Synthesis and supervised by inter-frame\ngeometric constraints that consider pose, depth, and RGB information. We\nvalidate our approaches on two publicly available datasets, one outdoor and one\nindoor. Our experimental results demonstrate that our weakly-supervised\nrelocalization solutions achieve superior pose estimation accuracy in\nsparse-view scenarios, comparable to state-of-the-art camera relocalization\nmethods. We will make our code publicly available.\n","authors":["Jialu Wang","Kaichen Zhou","Andrew Markham","Niki Trigoni"],"pdf_url":"https://arxiv.org/pdf/2403.15272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07846v3","updated":"2024-03-22T12:41:50Z","published":"2023-09-14T16:40:44Z","title":"MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image\n  Acquisition Systems","summary":"  Neural Radiance Fields (NeRF) use multi-view images for 3D scene\nrepresentation, demonstrating remarkable performance. As one of the primary\nsources of multi-view images, multi-camera systems encounter challenges such as\nvarying intrinsic parameters and frequent pose changes. Most previous\nNeRF-based methods assume a unique camera and rarely consider multi-camera\nscenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic\nparameters still remain susceptible to suboptimal solutions when these\nparameters are poor initialized. In this paper, we propose MC-NeRF, a method\nthat enables joint optimization of both intrinsic and extrinsic parameters\nalongside NeRF. The method also supports each image corresponding to\nindependent camera parameters. First, we tackle coupling issue and the\ndegenerate case that arise from the joint optimization between intrinsic and\nextrinsic parameters. Second, based on the proposed solutions, we introduce an\nefficient calibration image acquisition scheme for multi-camera systems,\nincluding the design of calibration object. Finally, we present an end-to-end\nnetwork with training sequence that enables the estimation of intrinsic and\nextrinsic parameters, along with the rendering network. Furthermore,\nrecognizing that most existing datasets are designed for a unique camera, we\nconstruct a real multi-camera image acquisition system and create a\ncorresponding new dataset, which includes both simulated data and real-world\ncaptured images. Experiments confirm the effectiveness of our method when each\nimage corresponds to different camera parameters. Specifically, we use\nmulti-cameras, each with different intrinsic and extrinsic parameters in\nreal-world system, to achieve 3D scene representation without providing initial\nposes.\n","authors":["Yu Gao","Lutong Su","Hao Liang","Yufeng Yue","Yi Yang","Mengyin Fu"],"pdf_url":"https://arxiv.org/pdf/2309.07846v3.pdf","comment":"This manuscript is currently under review"}],"HDR":[{"id":"http://arxiv.org/abs/2403.15061v1","updated":"2024-03-22T09:38:16Z","published":"2024-03-22T09:38:16Z","title":"Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic\n  Range Videos","summary":"  High Dynamic Range (HDR) videos are able to represent wider ranges of\ncontrasts and colors than Standard Dynamic Range (SDR) videos, giving more\nvivid experiences. Due to this, HDR videos are expected to grow into the\ndominant video modality of the future. However, HDR videos are incompatible\nwith existing SDR displays, which form the majority of affordable consumer\ndisplays on the market. Because of this, HDR videos must be processed by\ntone-mapping them to reduced bit-depths to service a broad swath of SDR-limited\nvideo consumers. Here, we analyze the impact of tone-mapping operators on the\nvisual quality of streaming HDR videos. To this end, we built the first\nlarge-scale subjectively annotated open-source database of compressed\ntone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40\nunique HDR source contents. The videos in the database were labeled with more\nthan 750,000 subjective quality annotations, collected from more than 1,600\nunique human observers. We demonstrate the usefulness of the new subjective\ndatabase by benchmarking objective models of visual quality on it. We envision\nthat the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant\nprogress on HDR video tone mapping and quality assessment in the future. To\nthis end, we make the database freely available to the community at\nhttps://live.ece.utexas.edu/research/LIVE_TMHDR/index.html\n","authors":["Abhinau K. Venkataramanan","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2403.15061v1.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2403.06054v3","updated":"2024-03-22T19:14:59Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v3.pdf","comment":null}]},"2024-03-20T00:00:00Z":{"Mesh":[{"id":"http://arxiv.org/abs/2312.06644v2","updated":"2024-03-20T17:58:05Z","published":"2023-12-11T18:56:37Z","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","summary":"  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n","authors":["Rao Fu","Zehao Wen","Zichen Liu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.06644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13663v1","updated":"2024-03-20T15:14:22Z","published":"2024-03-20T15:14:22Z","title":"T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh\n  Generation from a Single Image","summary":"  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a\nsingle color image through coarse-to-fine mesh deformation. Although P2M is\ncapable of generating plausible global shapes, its Graph Convolution Network\n(GCN) often produces overly smooth results, causing the loss of fine-grained\ngeometry details. Moreover, P2M generates non-credible features for occluded\nregions and struggles with the domain gap from synthetic data to real-world\nimages, which is a common challenge for single-view 3D reconstruction methods.\nTo address these challenges, we propose a novel Transformer-boosted\narchitecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of\nP2M. Specifically, we use a global Transformer to control the holistic shape\nand a local Transformer to progressively refine the local geometry details with\ngraph-based point upsampling. To enhance real-world reconstruction, we present\nthe simple yet effective Linear Scale Search (LSS), which serves as prompt\ntuning during the input preprocessing. Our experiments on ShapeNet demonstrate\nstate-of-the-art performance, while results on real-world data show the\ngeneralization capability.\n","authors":["Shijie Zhang","Boyan Jiang","Keke He","Junwei Zhu","Ying Tai","Chengjie Wang","Yinda Zhang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.13663v1.pdf","comment":"Received by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.09031v2","updated":"2024-03-20T12:00:59Z","published":"2023-12-14T15:31:33Z","title":"iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via\n  Comparing and Matching","summary":"  We present a method named iComMa to address the 6D camera pose estimation\nproblem in computer vision. Conventional pose estimation methods typically rely\non the target's CAD model or necessitate specific network training tailored to\nparticular object classes. Some existing methods have achieved promising\nresults in mesh-free object and scene pose estimation by inverting the Neural\nRadiance Fields (NeRF). However, they still struggle with adverse\ninitializations such as large rotations and translations. To address this\nissue, we propose an efficient method for accurate camera pose estimation by\ninverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based\ndifferentiable framework optimizes camera pose by minimizing the residual\nbetween the query image and the rendered image, requiring no training. An\nend-to-end matching module is designed to enhance the model's robustness\nagainst adverse initializations, while minimizing pixel-level comparing loss\naids in precise pose estimation. Experimental results on synthetic and complex\nreal-world data demonstrate the effectiveness of the proposed approach in\nchallenging conditions and the accuracy of camera pose estimation.\n","authors":["Yuan Sun","Xuan Wang","Yunfan Zhang","Jie Zhang","Caigui Jiang","Yu Guo","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12434v2","updated":"2024-03-20T02:04:21Z","published":"2024-03-19T04:47:56Z","title":"Human Mesh Recovery from Arbitrary Multi-view Images","summary":"  Human mesh recovery from arbitrary multi-view images involves two\ncharacteristics: the arbitrary camera poses and arbitrary number of camera\nviews. Because of the variability, designing a unified framework to tackle this\ntask is challenging. The challenges can be summarized as the dilemma of being\nable to simultaneously estimate arbitrary camera poses and recover human mesh\nfrom arbitrary multi-view images while maintaining flexibility. To solve this\ndilemma, we propose a divide and conquer framework for Unified Human Mesh\nRecovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR\nconsists of a decoupled structure and two main components: camera and body\ndecoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion\n(AVF). As camera poses and human body mesh are independent of each other, CBD\nsplits the estimation of them into two sub-tasks for two individual\nsub-networks (\\ie, CPE and AVF) to handle respectively, thus the two sub-tasks\nare disentangled. In CPE, since each camera pose is unrelated to the others, we\nadopt a shared MLP to process all views in a parallel way. In AVF, in order to\nfuse multi-view information and make the fusion operation independent of the\nnumber of views, we introduce a transformer decoder with a SMPL parameters\nquery token to extract cross-view features for mesh recovery. To demonstrate\nthe efficacy and flexibility of the proposed framework and effect of each\ncomponent, we conduct extensive experiments on three public datasets:\nHuman3.6M, MPI-INF-3DHP, and TotalCapture.\n","authors":["Xiaoben Li","Mancheng Meng","Ziyan Wu","Terrence Chen","Fan Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2403.12434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13238v1","updated":"2024-03-20T01:59:43Z","published":"2024-03-20T01:59:43Z","title":"Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence\n  Generation","summary":"  Directly learning to model 4D content, including shape, color and motion, is\nchallenging. Existing methods depend on skeleton-based motion control and offer\nlimited continuity in detail. To address this, we propose a novel framework\nthat generates coherent 4D sequences with animation of 3D shapes under given\nconditions with dynamic evolution of shape and color over time through\nintegrative latent mapping. We first employ an integrative latent unified\nrepresentation to encode shape and color information of each detailed 3D\ngeometry frame. The proposed skeleton-free latent 4D sequence joint\nrepresentation allows us to leverage diffusion models in a low-dimensional\nspace to control the generation of 4D sequences. Finally, temporally coherent\n4D sequences are generated conforming well to the input images and text\nprompts. Extensive experiments on the ShapeNet, 3DBiCar and DeformingThings4D\ndatasets for several tasks demonstrate that our method effectively learns to\ngenerate quality 3D shapes with color and 4D mesh animations, improving over\nthe current state-of-the-art. Source code will be released.\n","authors":["Qitong Yang","Mingtao Feng","Zijie Wu","Shijie Sun","Weisheng Dong","Yaonan Wang","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2403.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16581v2","updated":"2024-03-20T00:59:44Z","published":"2023-11-28T07:55:25Z","title":"GeoScaler: Geometry and Rendering-Aware Downsampling of 3D Mesh Textures","summary":"  High-resolution texture maps are necessary for representing real-world\nobjects accurately with 3D meshes. The large sizes of textures can bottleneck\nthe real-time rendering of high-quality virtual 3D scenes on devices having low\ncomputational budgets and limited memory. Downsampling the texture maps\ndirectly addresses the issue, albeit at the cost of visual fidelity.\nTraditionally, downsampling of texture maps is performed using methods like\nbicubic interpolation and the Lanczos algorithm. These methods ignore the\ngeometric layout of the mesh and its UV parametrization and also do not account\nfor the rendering process used to obtain the final visualization that the users\nwill experience. Towards filling these gaps, we introduce GeoScaler, which is a\nmethod of downsampling texture maps of 3D meshes while incorporating geometric\ncues, and by maximizing the visual fidelity of the rendered views of the\ntextured meshes. We show that the textures generated by GeoScaler deliver\nsignificantly better quality rendered images compared to those generated by\ntraditional downsampling methods\n","authors":["Sai Karthikey Pentapati","Anshul Rai","Arkady Ten","Chaitanya Atluru","Alan Bovik"],"pdf_url":"https://arxiv.org/pdf/2311.16581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13214v1","updated":"2024-03-20T00:23:42Z","published":"2024-03-20T00:23:42Z","title":"Nellie: Automated organelle segmentation, tracking, and hierarchical\n  feature extraction in 2D/3D live-cell microscopy","summary":"  The analysis of dynamic organelles remains a formidable challenge, though key\nto understanding biological processes. We introduce Nellie, an automated and\nunbiased pipeline for segmentation, tracking, and feature extraction of diverse\nintracellular structures. Nellie adapts to image metadata, eliminating user\ninput. Nellie's preprocessing pipeline enhances structural contrast on multiple\nintracellular scales allowing for robust hierarchical segmentation of\nsub-organellar regions. Internal motion capture markers are generated and\ntracked via a radius-adaptive pattern matching scheme, and used as guides for\nsub-voxel flow interpolation. Nellie extracts a plethora of features at\nmultiple hierarchical levels for deep and customizable analysis. Nellie\nfeatures a Napari-based GUI that allows for code-free operation and\nvisualization, while its modular open-source codebase invites customization by\nexperienced users. We demonstrate Nellie's wide variety of use cases with two\nexamples: unmixing multiple organelles from a single channel using\nfeature-based classification and training an unsupervised graph autoencoder on\nmitochondrial multi-mesh graphs to quantify latent space embedding changes\nfollowing ionomycin treatment.\n","authors":["Austin E. Y. T. Lefebvre","Gabriel Sturm","Ting-Yu Lin","Emily Stoops","Magdalena Preciado Lopez","Benjamin Kaufmann-Malaga","Kayley Hake"],"pdf_url":"https://arxiv.org/pdf/2403.13214v1.pdf","comment":"for associated code, see https://github.com/aelefebv/nellie; 82\n  pages, 5 main figures, 11 extended figures"}],"NeRF":[{"id":"http://arxiv.org/abs/2312.09031v2","updated":"2024-03-20T12:00:59Z","published":"2023-12-14T15:31:33Z","title":"iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via\n  Comparing and Matching","summary":"  We present a method named iComMa to address the 6D camera pose estimation\nproblem in computer vision. Conventional pose estimation methods typically rely\non the target's CAD model or necessitate specific network training tailored to\nparticular object classes. Some existing methods have achieved promising\nresults in mesh-free object and scene pose estimation by inverting the Neural\nRadiance Fields (NeRF). However, they still struggle with adverse\ninitializations such as large rotations and translations. To address this\nissue, we propose an efficient method for accurate camera pose estimation by\ninverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based\ndifferentiable framework optimizes camera pose by minimizing the residual\nbetween the query image and the rendered image, requiring no training. An\nend-to-end matching module is designed to enhance the model's robustness\nagainst adverse initializations, while minimizing pixel-level comparing loss\naids in precise pose estimation. Experimental results on synthetic and complex\nreal-world data demonstrate the effectiveness of the proposed approach in\nchallenging conditions and the accuracy of camera pose estimation.\n","authors":["Yuan Sun","Xuan Wang","Yunfan Zhang","Jie Zhang","Caigui Jiang","Yu Guo","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10297v2","updated":"2024-03-20T02:34:27Z","published":"2024-03-15T13:40:37Z","title":"Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints\n  Scene Coordinate Regression","summary":"  Classical structural-based visual localization methods offer high accuracy\nbut face trade-offs in terms of storage, speed, and privacy. A recent\ninnovation, keypoint scene coordinate regression (KSCR) named D2S addresses\nthese issues by leveraging graph attention networks to enhance keypoint\nrelationships and predict their 3D coordinates using a simple multilayer\nperceptron (MLP). Camera pose is then determined via PnP+RANSAC, using\nestablished 2D-3D correspondences. While KSCR achieves competitive results,\nrivaling state-of-the-art image-retrieval methods like HLoc across multiple\nbenchmarks, its performance is hindered when data samples are limited due to\nthe deep learning model's reliance on extensive data. This paper proposes a\nsolution to this challenge by introducing a pipeline for keypoint descriptor\nsynthesis using Neural Radiance Field (NeRF). By generating novel poses and\nfeeding them into a trained NeRF model to create new views, our approach\nenhances the KSCR's generalization capabilities in data-scarce environments.\nThe proposed system could significantly improve localization accuracy by up to\n50% and cost only a fraction of time for data synthesis. Furthermore, its\nmodular design allows for the integration of multiple NeRFs, offering a\nversatile and efficient solution for visual localization. The implementation is\npublicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.\n","authors":["Huy-Hoang Bui","Bach-Thuan Bui","Dinh-Tuan Tran","Joo-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2403.10297v2.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2312.04334v3","updated":"2024-03-20T18:11:07Z","published":"2023-12-07T14:51:12Z","title":"Towards a Perceptual Evaluation Framework for Lighting Estimation","summary":"  Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.\n","authors":["Justine Giroux","Mohammad Reza Karimi Dastjerdi","Yannick Hold-Geoffroy","Javier Vazquez-Corral","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2312.04334v3.pdf","comment":null}]},"2024-03-15T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2403.10376v1","updated":"2024-03-15T15:05:29Z","published":"2024-03-15T15:05:29Z","title":"PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively\n  Aggregated Spatio-Temporal Aligment","summary":"  Leveraging Transformer attention has led to great advancements in HDR\ndeghosting. However, the intricate nature of self-attention introduces\npractical challenges, as existing state-of-the-art methods often demand\nhigh-end GPUs or exhibit slow inference speeds, especially for high-resolution\nimages like 2K. Striking an optimal balance between performance and latency\nremains a critical concern. In response, this work presents PASTA, a novel\nProgressively Aggregated Spatio-Temporal Alignment framework for HDR\ndeghosting. Our approach achieves effectiveness and efficiency by harnessing\nhierarchical representation during feature distanglement. Through the\nutilization of diverse granularities within the hierarchical structure, our\nmethod substantially boosts computational speed and optimizes the HDR imaging\nworkflow. In addition, we explore within-scale feature modeling with local and\nglobal attention, gradually merging and refining them in a coarse-to-fine\nfashion. Experimental results showcase PASTA's superiority over current SOTA\nmethods in both visual quality and performance metrics, accompanied by a\nsubstantial 3-fold (x3) increase in inference speed.\n","authors":["Xiaoning Liu","Ao Li","Zongwei Wu","Yapeng Du","Le Zhang","Yulun Zhang","Radu Timofte","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.10376v1.pdf","comment":null}]},"2024-03-11T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2403.06831v1","updated":"2024-03-11T15:48:17Z","published":"2024-03-11T15:48:17Z","title":"HDRTransDC: High Dynamic Range Image Reconstruction with Transformer\n  Deformation Convolution","summary":"  High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image\nwith realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.\nCaused by large motion and severe under-/over-exposure among input LDR images,\nHDR imaging suffers from ghosting artifacts and fusion distortions. To address\nthese critical issues, we propose an HDR Transformer Deformation Convolution\n(HDRTransDC) network to generate high-quality HDR images, which consists of the\nTransformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic\nWeight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM\nextracts long-distance content similar to the reference feature in the entire\nnon-reference features, which can accurately remove misalignment and fill the\ncontent occluded by moving objects. For the purpose of eliminating fusion\ndistortions, we propose DWFB to spatially adaptively select useful information\nacross frames to effectively fuse multi-exposed features. Extensive experiments\nshow that our method quantitatively and qualitatively achieves state-of-the-art\nperformance.\n","authors":["Shuaikang Shang","Xuejing Kang","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2403.06831v1.pdf","comment":null}],"IQA":[{"id":"http://arxiv.org/abs/2403.06406v1","updated":"2024-03-11T03:35:41Z","published":"2024-03-11T03:35:41Z","title":"Comparison of No-Reference Image Quality Models via MAP Estimation in\n  Diffusion Latents","summary":"  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify the perceived image quality, with high correlations\nbetween model predictions and human perceptual scores on fixed test sets.\nHowever, little progress has been made in comparing NR-IQA models from a\nperceptual optimization perspective. Here, for the first time, we demonstrate\nthat NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by taking the\ngradients in differentiable and bijective diffusion latents rather than in the\nraw pixel domain. Different NR-IQA models are likely to induce different\nenhanced images, which are ultimately subject to psychophysical testing. This\nleads to a new computational method for comparing NR-IQA models within the\nanalysis-by-synthesis framework. Compared to conventional correlation-based\nmetrics, our method provides complementary insights into the relative strengths\nand weaknesses of the competing NR-IQA models in the context of perceptual\noptimization.\n","authors":["Weixia Zhang","Dingquan Li","Guangtao Zhai","Xiaokang Yang","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2403.06406v1.pdf","comment":null}]},"2024-03-07T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2403.04228v1","updated":"2024-03-07T05:04:36Z","published":"2024-03-07T05:04:36Z","title":"Single-Image HDR Reconstruction Assisted Ghost Suppression and Detail\n  Preservation Network for Multi-Exposure HDR Imaging","summary":"  The reconstruction of high dynamic range (HDR) images from multi-exposure low\ndynamic range (LDR) images in dynamic scenes presents significant challenges,\nespecially in preserving and restoring information in oversaturated regions and\navoiding ghosting artifacts. While current methods often struggle to address\nthese challenges, our work aims to bridge this gap by developing a\nmulti-exposure HDR image reconstruction network for dynamic scenes,\ncomplemented by single-frame HDR image reconstruction. This network, comprising\nsingle-frame HDR reconstruction with enhanced stop image (SHDR-ESI) and\nSHDR-ESI-assisted multi-exposure HDR reconstruction (SHDRA-MHDR), effectively\nleverages the ghost-free characteristic of single-frame HDR reconstruction and\nthe detail-enhancing capability of ESI in oversaturated areas. Specifically,\nSHDR-ESI innovatively integrates single-frame HDR reconstruction with the\nutilization of ESI. This integration not only optimizes the single image HDR\nreconstruction process but also effectively guides the synthesis of\nmulti-exposure HDR images in SHDR-AMHDR. In this method, the single-frame HDR\nreconstruction is specifically applied to reduce potential ghosting effects in\nmultiexposure HDR synthesis, while the use of ESI images assists in enhancing\nthe detail information in the HDR synthesis process. Technically, SHDR-ESI\nincorporates a detail enhancement mechanism, which includes a\nself-representation module and a mutual-representation module, designed to\naggregate crucial information from both reference image and ESI. To fully\nleverage the complementary information from non-reference images, a feature\ninteraction fusion module is integrated within SHDRA-MHDR. Additionally, a\nghost suppression module, guided by the ghost-free results of SHDR-ESI, is\nemployed to suppress the ghosting artifacts.\n","authors":["Huafeng Li","Zhenmei Yang","Yafei Zhang","Dapeng Tao","Zhengtao Yu"],"pdf_url":"https://arxiv.org/pdf/2403.04228v1.pdf","comment":"IEEE Transactions on Computational Imaging"}],"Deblur":[{"id":"http://arxiv.org/abs/2402.15784v3","updated":"2024-03-07T11:00:02Z","published":"2024-02-24T10:52:50Z","title":"IRConStyle: Image Restoration Framework Using Contrastive Learning and\n  Style Transfer","summary":"  Recently, the contrastive learning paradigm has achieved remarkable success\nin high-level tasks such as classification, detection, and segmentation.\nHowever, contrastive learning applied in low-level tasks, like image\nrestoration, is limited, and its effectiveness is uncertain. This raises a\nquestion: Why does the contrastive learning paradigm not yield satisfactory\nresults in image restoration? In this paper, we conduct in-depth analyses and\npropose three guidelines to address the above question. In addition, inspired\nby style transfer and based on contrastive learning, we propose a novel module\nfor image restoration called \\textbf{ConStyle}, which can be efficiently\nintegrated into any U-Net structure network. By leveraging the flexibility of\nConStyle, we develop a \\textbf{general restoration network} for image\nrestoration. ConStyle and the general restoration network together form an\nimage restoration framework, namely \\textbf{IRConStyle}. To demonstrate the\ncapability and compatibility of ConStyle, we replace the general restoration\nnetwork with transformer-based, CNN-based, and MLP-based networks,\nrespectively. We perform extensive experiments on various image restoration\ntasks, including denoising, deblurring, deraining, and dehazing. The results on\n19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based\nnetwork and significantly enhance performance. For instance, ConStyle NAFNet\nsignificantly outperforms the original NAFNet on SOTS outdoor (dehazing) and\nRain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB\nwith 85% fewer parameters.\n","authors":["Dongqi Fan","Xin Zhao","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2402.15784v3.pdf","comment":null}]},"2024-03-06T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2403.03447v1","updated":"2024-03-06T04:13:29Z","published":"2024-03-06T04:13:29Z","title":"HDRFlow: Real-Time HDR Video Reconstruction with Large Motions","summary":"  Reconstructing High Dynamic Range (HDR) video from image sequences captured\nwith alternating exposures is challenging, especially in the presence of large\ncamera or object motion. Existing methods typically align low dynamic range\nsequences using optical flow or attention mechanism for deghosting. However,\nthey often struggle to handle large complex motions and are computationally\nexpensive. To address these challenges, we propose a robust and efficient flow\nestimator tailored for real-time HDR video reconstruction, named HDRFlow.\nHDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an\nefficient flow network with a multi-size large kernel (MLK), and a new HDR flow\ntraining scheme. The HALoss supervises our flow network to learn an\nHDR-oriented flow for accurate alignment in saturated and dark regions. The MLK\ncan effectively model large motions at a negligible cost. In addition, we\nincorporate synthetic data, Sintel, into our training dataset, utilizing both\nits provided forward flow and backward flow generated by us to supervise our\nflow network, enhancing our performance in large motion regions. Extensive\nexperiments demonstrate that our HDRFlow outperforms previous methods on\nstandard benchmarks. To the best of our knowledge, HDRFlow is the first\nreal-time HDR video reconstruction method for video sequences captured with\nalternating exposures, capable of processing 720p resolution inputs at 25ms.\n","authors":["Gangwei Xu","Yujin Wang","Jinwei Gu","Tianfan Xue","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2403.03447v1.pdf","comment":"CVPR 2024; Project website: https://openimaginglab.github.io/HDRFlow/"}]},"2024-02-28T00:00:00Z":{"HDR":[{"id":"http://arxiv.org/abs/2310.01840v2","updated":"2024-02-28T18:45:20Z","published":"2023-10-03T07:10:49Z","title":"Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in\n  Dynamic Scenes","summary":"  Merging multi-exposure images is a common approach for obtaining high dynamic\nrange (HDR) images, with the primary challenge being the avoidance of ghosting\nartifacts in dynamic scenes. Recent methods have proposed using deep neural\nnetworks for deghosting. However, the methods typically rely on sufficient data\nwith HDR ground-truths, which are difficult and costly to collect. In this\nwork, to eliminate the need for labeled data, we propose SelfHDR, a\nself-supervised HDR reconstruction method that only requires dynamic\nmulti-exposure images during training. Specifically, SelfHDR learns a\nreconstruction network under the supervision of two complementary components,\nwhich can be constructed from multi-exposure images and focus on HDR color as\nwell as structure, respectively. The color component is estimated from aligned\nmulti-exposure images, while the structure one is generated through a\nstructure-focused network that is supervised by the color component and an\ninput reference (\\eg, medium-exposure) image. During testing, the learned\nreconstruction network is directly deployed to predict an HDR image.\nExperiments on real-world images demonstrate our SelfHDR achieves superior\nresults against the state-of-the-art self-supervised methods, and comparable\nperformance to supervised ones. Codes are available at\nhttps://github.com/cszhilu1998/SelfHDR\n","authors":["Zhilu Zhang","Haoyu Wang","Shuai Liu","Xiaotao Wang","Lei Lei","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2310.01840v2.pdf","comment":"ICLR 2024"}],"IQA":[{"id":"http://arxiv.org/abs/2402.17533v2","updated":"2024-02-28T13:44:48Z","published":"2024-02-27T14:16:39Z","title":"Black-box Adversarial Attacks Against Image Quality Assessment Models","summary":"  The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the\nperceptual quality of an image in line with its subjective evaluation. To put\nthe NR-IQA models into practice, it is essential to study their potential\nloopholes for model refinement. This paper makes the first attempt to explore\nthe black-box adversarial attacks on NR-IQA models. Specifically, we first\nformulate the attack problem as maximizing the deviation between the estimated\nquality scores of original and perturbed images, while restricting the\nperturbed image distortions for visual quality preservation. Under such\nformulation, we then design a Bi-directional loss function to mislead the\nestimated quality scores of adversarial examples towards an opposite direction\nwith maximum deviation. On this basis, we finally develop an efficient and\neffective black-box attack method against NR-IQA models. Extensive experiments\nreveal that all the evaluated NR-IQA models are vulnerable to the proposed\nattack method. And the generated perturbations are not transferable, enabling\nthem to serve the investigation of specialities of disparate IQA models.\n","authors":["Yu Ran","Ao-Xiang Zhang","Mingjie Li","Weixuan Tang","Yuan-Gen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17533v2.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2402.18134v1","updated":"2024-02-28T07:56:28Z","published":"2024-02-28T07:56:28Z","title":"Learning to Deblur Polarized Images","summary":"  A polarization camera can capture four polarized images with different\npolarizer angles in a single shot, which is useful in polarization-based vision\napplications since the degree of polarization (DoP) and the angle of\npolarization (AoP) can be directly computed from the captured polarized images.\nHowever, since the on-chip micro-polarizers block part of the light so that the\nsensor often requires a longer exposure time, the captured polarized images are\nprone to motion blur caused by camera shakes, leading to noticeable degradation\nin the computed DoP and AoP. Deblurring methods for conventional images often\nshow degenerated performance when handling the polarized images since they only\nfocus on deblurring without considering the polarization constrains. In this\npaper, we propose a polarized image deblurring pipeline to solve the problem in\na polarization-aware manner by adopting a divide-and-conquer strategy to\nexplicitly decompose the problem into two less ill-posed sub-problems, and\ndesign a two-stage neural network to handle the two sub-problems respectively.\nExperimental results show that our method achieves state-of-the-art performance\non both synthetic and real-world images, and can improve the performance of\npolarization-based vision applications such as image dehazing and reflection\nremoval.\n","authors":["Chu Zhou","Minggui Teng","Xinyu Zhou","Chao Xu","Boxin Sh"],"pdf_url":"https://arxiv.org/pdf/2402.18134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18102v1","updated":"2024-02-28T06:45:47Z","published":"2024-02-28T06:45:47Z","title":"Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging","summary":"  Passive, compact, single-shot 3D sensing is useful in many application areas\nsuch as microscopy, medical imaging, surgical navigation, and autonomous\ndriving where form factor, time, and power constraints can exist. Obtaining\nRGB-D scene information over a short imaging distance, in an ultra-compact form\nfactor, and in a passive, snapshot manner is challenging. Dual-pixel (DP)\nsensors are a potential solution to achieve the same. DP sensors collect light\nrays from two different halves of the lens in two interleaved pixel arrays,\nthus capturing two slightly different views of the scene, like a stereo camera\nsystem. However, imaging with a DP sensor implies that the defocus blur size is\ndirectly proportional to the disparity seen between the views. This creates a\ntrade-off between disparity estimation vs. deblurring accuracy. To improve this\ntrade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which\nwe use a coded aperture in the imaging lens along with a DP sensor. In our\napproach, we jointly learn an optimal coded pattern and the reconstruction\nalgorithm in an end-to-end optimization setting. Our resulting CADS imaging\nsystem demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF)\nestimates and 5-6% in depth estimation quality over naive DP sensing for a wide\nrange of aperture settings. Furthermore, we build the proposed CADS prototypes\nfor DSLR photography settings and in an endoscope and a dermoscope form factor.\nOur novel coded dual-pixel sensing approach demonstrates accurate RGB-D\nreconstruction results in simulations and real-world experiments in a passive,\nsnapshot, and compact manner.\n","authors":["Bhargav Ghanekar","Salman Siddique Khan","Vivek Boominathan","Pranav Sharma","Shreyas Singh","Kaushik Mitra","Ashok Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2402.18102v1.pdf","comment":null}]},"2024-03-17T00:00:00Z":{"IQA":[{"id":"http://arxiv.org/abs/2403.11176v1","updated":"2024-03-17T11:32:18Z","published":"2024-03-17T11:32:18Z","title":"Quality-Aware Image-Text Alignment for Real-World Image Quality\n  Assessment","summary":"  No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods\nto measure image quality in alignment with human perception when a high-quality\nreference image is unavailable. The reliance on annotated Mean Opinion Scores\n(MOS) in the majority of state-of-the-art NR-IQA approaches limits their\nscalability and broader applicability to real-world scenarios. To overcome this\nlimitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based\nself-supervised opinion-unaware method that does not require labeled MOS. In\nparticular, we introduce a quality-aware image-text alignment strategy to make\nCLIP generate representations that correlate with the inherent quality of the\nimages. Starting from pristine images, we synthetically degrade them with\nincreasing levels of intensity. Then, we train CLIP to rank these degraded\nimages based on their similarity to quality-related antonym text prompts, while\nguaranteeing consistent representations for images with comparable quality. Our\nmethod achieves state-of-the-art performance on several datasets with authentic\ndistortions. Moreover, despite not requiring MOS, QualiCLIP outperforms\nsupervised methods when their training dataset differs from the testing one,\nthus proving to be more suitable for real-world scenarios. Furthermore, our\napproach demonstrates greater robustness and improved explainability than\ncompeting methods. The code and the model are publicly available at\nhttps://github.com/miccunifi/QualiCLIP.\n","authors":["Lorenzo Agnolucci","Leonardo Galteri","Marco Bertini"],"pdf_url":"https://arxiv.org/pdf/2403.11176v1.pdf","comment":null}]},"2024-03-16T00:00:00Z":{"IQA":[{"id":"http://arxiv.org/abs/2403.10854v1","updated":"2024-03-16T08:30:45Z","published":"2024-03-16T08:30:45Z","title":"A Comprehensive Study of Multimodal Large Language Models for Image\n  Quality Assessment","summary":"  While Multimodal Large Language Models (MLLMs) have experienced significant\nadvancement on visual understanding and reasoning, their potentials to serve as\npowerful, flexible, interpretable, and text-driven models for Image Quality\nAssessment (IQA) remains largely unexplored. In this paper, we conduct a\ncomprehensive and systematic study of prompting MLLMs for IQA. Specifically, we\nfirst investigate nine prompting systems for MLLMs as the combinations of three\nstandardized testing procedures in psychophysics (i.e., the single-stimulus,\ndouble-stimulus, and multiple-stimulus methods) and three popular prompting\nstrategies in natural language processing (i.e., the standard, in-context, and\nchain-of-thought prompting). We then present a difficult sample selection\nprocedure, taking into account sample diversity and uncertainty, to further\nchallenge MLLMs equipped with the respective optimal prompting systems. We\nassess three open-source and one close-source MLLMs on several visual\nattributes of image quality (e.g., structural and textural distortions, color\ndifferences, and geometric transformations) in both full-reference and\nno-reference scenarios. Experimental results show that only the close-source\nGPT-4V provides a reasonable account for human perception of image quality, but\nis weak at discriminating fine-grained quality variations (e.g., color\ndifferences) and at comparing visual quality of multiple images, tasks humans\ncan perform effortlessly.\n","authors":["Tianhe Wu","Kede Ma","Jie Liang","Yujiu Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10854v1.pdf","comment":null}]},"2024-03-13T00:00:00Z":{"IQA":[{"id":"http://arxiv.org/abs/2308.11408v3","updated":"2024-03-13T10:31:21Z","published":"2023-08-22T12:54:48Z","title":"MatFuse: Controllable Material Generation with Diffusion Models","summary":"  Creating high-quality materials in computer graphics is a challenging and\ntime-consuming task, which requires great expertise. To simplify this process,\nwe introduce MatFuse, a unified approach that harnesses the generative power of\ndiffusion models for creation and editing of 3D materials. Our method\nintegrates multiple sources of conditioning, including color palettes,\nsketches, text, and pictures, enhancing creative possibilities and granting\nfine-grained control over material synthesis. Additionally, MatFuse enables\nmap-level material editing capabilities through latent manipulation by means of\na multi-encoder compression model which learns a disentangled latent\nrepresentation for each map. We demonstrate the effectiveness of MatFuse under\nmultiple conditioning settings and explore the potential of material editing.\nFinally, we assess the quality of the generated materials both quantitatively\nin terms of CLIP-IQA and FID scores and qualitatively by conducting a user\nstudy. Source code for training MatFuse and supplemental materials are publicly\navailable at https://gvecchio.com/matfuse.\n","authors":["Giuseppe Vecchio","Renato Sortino","Simone Palazzo","Concetto Spampinato"],"pdf_url":"https://arxiv.org/pdf/2308.11408v3.pdf","comment":null}],"Deblur":[{"id":"http://arxiv.org/abs/2401.00027v2","updated":"2024-03-13T18:51:29Z","published":"2023-12-29T02:59:40Z","title":"Efficient Multi-scale Network with Learnable Discrete Wavelet Transform\n  for Blind Motion Deblurring","summary":"  Coarse-to-fine schemes are widely used in traditional single-image motion\ndeblur; however, in the context of deep learning, existing multi-scale\nalgorithms not only require the use of complex modules for feature fusion of\nlow-scale RGB images and deep semantics, but also manually generate\nlow-resolution pairs of images that do not have sufficient confidence. In this\nwork, we propose a multi-scale network based on single-input and\nmultiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of\nalgorithms based on a coarse-to-fine scheme. To alleviate restoration defects\nimpacting detail information brought about by using a multi-scale architecture,\nwe combine the characteristics of real-world blurring trajectories with a\nlearnable wavelet transform module to focus on the directional continuity and\nfrequency features of the step-by-step transitions between blurred images to\nsharp images. In conclusion, we propose a multi-scale network with a learnable\ndiscrete wavelet transform (MLWNet), which exhibits state-of-the-art\nperformance on multiple real-world deblurred datasets, in terms of both\nsubjective and objective quality as well as computational efficiency.\n","authors":["Xin Gao","Tianheng Qiu","Xinyu Zhang","Hanlin Bai","Kang Liu","Xuan Huang","Hu Wei","Guoying Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2401.00027v2.pdf","comment":null}]},"2024-03-10T00:00:00Z":{"IQA":[{"id":"http://arxiv.org/abs/2312.08962v2","updated":"2024-03-10T09:18:17Z","published":"2023-12-14T14:10:02Z","title":"Depicting Beyond Scores: Advancing Image Quality Assessment through\n  Multi-modal Language Models","summary":"  We introduce a Depicted image Quality Assessment method (DepictQA),\novercoming the constraints of traditional score-based methods. DepictQA allows\nfor detailed, language-based, human-like evaluation of image quality by\nleveraging Multi-modal Large Language Models (MLLMs). Unlike conventional Image\nQuality Assessment (IQA) methods relying on scores, DepictQA interprets image\ncontent and distortions descriptively and comparatively, aligning closely with\nhumans' reasoning process. To build the DepictQA model, we establish a\nhierarchical task framework, and collect a multi-modal IQA training dataset. To\ntackle the challenges of limited training data and multi-image processing, we\npropose to use multi-source training data and specialized image tags. These\ndesigns result in a better performance of DepictQA than score-based approaches\non multiple benchmarks. Moreover, compared with general MLLMs, DepictQA can\ngenerate more accurate reasoning descriptive languages. Our work demonstrates\nthe utility of our full-reference dataset in non-reference applications, and\nindicates that language-based IQA methods have the potential to be customized\nfor individual preferences.\n","authors":["Zhiyuan You","Zheyuan Li","Jinjin Gu","Zhenfei Yin","Tianfan Xue","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2312.08962v2.pdf","comment":null}]},"2024-03-12T00:00:00Z":{"Deblur":[{"id":"http://arxiv.org/abs/2403.07874v1","updated":"2024-03-12T17:59:51Z","published":"2024-03-12T17:59:51Z","title":"Beyond Text: Frozen Large Language Models in Visual Signal Comprehension","summary":"  In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.\n","authors":["Lei Zhu","Fangyun Wei","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2403.07874v1.pdf","comment":"Accepted by CVPR 2024"}]}}