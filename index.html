<!DOCTYPE html>
<html lang="en">

<head>
    <title>Wenxun-Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                wenxun-private-arxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via
  Expressive Masked Audio Gesture Modeling <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00374v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00374v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech <span class="highlight-title">dataset</span>. BEAT2 combines MoShed SMPLX body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured <span class="highlight-title">dataset</span>. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture <span class="highlight-title">Transformer</span>,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results' fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and <span class="highlight-title">dataset</span> are available at
https://pantomatrix.github.io/EMAGE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conflict of Interest Disclosure; CVPR Camera Ready; Project Page:
  https://pantomatrix.github.io/EMAGE/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniArtAvatar: Animatable 3D Art Avatar from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for generating animatable 3D-aware art avatars
from a single image, with controllable facial expressions, head poses, and
shoulder movements. Unlike previous reenactment methods, our approach utilizes
a view-conditioned 2D <span class="highlight-title">diffusion</span> model to synthesize multi-view images from a
single art portrait with a neutral expression. With the generated colors and
normals, we synthesize a static avatar using an SDF-based neural surface. For
avatar animation, we extract control points, transfer the motion with these
points, and deform the implicit canonical space. Firstly, we render the front
image of the avatar, extract the 2D landmarks, and project them to the 3D space
using a trained SDF network. We extract 3D driving landmarks using 3DMM and
transfer the motion to the avatar landmarks. To animate the avatar pose, we
manually set the body height and bound the head and torso of an avatar with two
cages. The head and torso can be animated by transforming the two cages. Our
approach is a one-shot pipeline that can be applied to various styles.
Experiments demonstrate that our method can generate high-quality 3D art
avatars with desired control over different motions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh
recovery) involves the human body, hand, and expression estimation. Most
existing methods have tackled this task in a two-stage manner, first detecting
the human body part with an off-the-shelf detection model and inferring the
different human body parts individually. Despite the impressive results
achieved, these methods suffer from 1) loss of valuable contextual information
via cropping, 2) introducing distractions, and 3) lacking inter-association
among different persons and body parts, inevitably causing performance
degradation, especially for crowded scenes. To address these issues, we
introduce a novel all-in-one-stage framework, AiOS, for multiple expressive
human pose and shape recovery without an additional human detection step.
Specifically, our method is built upon DETR, which treats multi-person
whole-body mesh recovery task as a progressive set prediction problem with
various sequential detection. We devise the decoder tokens and extend them to
our task. Specifically, we first employ a human token to probe a human location
in the image and encode global features for each instance, which provides a
coarse location for the later <span class="highlight-title">transformer</span> block. Then, we introduce a
joint-related token to probe the human joint in the image and encoder a
fine-grained local feature, which collaborates with the global feature to
regress the whole-body mesh. This straightforward but effective model
outperforms previous state-of-the-art methods by a 9% reduction in NMVE on
AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a
3% reduction in PVE on EgoBody.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://ttxskk.github.io/AiOS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting, a novel differentiable <span class="highlight-title">rendering</span> technique, has
achieved state-of-the-art novel view synthesis results with high <span class="highlight-title">rendering</span>
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor <span class="highlight-title">dataset</span>s is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor <span class="highlight-title">dataset</span>s and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenesisTex: Adapting Image Denoising <span class="highlight-title">Diffusion</span> to Texture Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GenesisTex, a novel method for synthesizing textures for 3D
geometries from text descriptions. GenesisTex adapts the <span class="highlight-title">pretrain</span>ed image
<span class="highlight-title">diffusion</span> model to texture space by texture space sampling. Specifically, we
maintain a latent texture map for each viewpoint, which is updated with
predicted noise on the <span class="highlight-title">rendering</span> of the corresponding viewpoint. The sampled
latent texture maps are then decoded into a final texture map. During the
sampling process, we focus on both global and local consistency across multiple
viewpoints: global consistency is achieved through the integration of style
consistency mechanisms within the noise prediction network, and low-level
consistency is achieved by dynamically aligning latent textures. Finally, we
apply reference-based inpainting and img2img on denser views for texture
refinement. Our approach overcomes the limitations of slow optimization in
distillation-based methods and instability in inpainting-based methods.
Experiments on meshes from various sources demonstrate that our method
surpasses the baseline methods quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordRobe: Text-Guided Generation of Textured 3D Garments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a new and challenging problem of text-driven
generation of 3D garments with high-quality textures. We propose "WordRobe", a
novel framework for the generation of unposed & textured 3D garment meshes from
user-friendly text <span class="highlight-title">prompt</span>s. We achieve this by first learning a latent
representation of 3D garments using a novel coarse-to-fine training strategy
and a loss for latent disentanglement, promoting better latent interpolation.
Subsequently, we align the garment latent space to the <span class="highlight-title">CLIP</span> embedding space in
a weakly supervised manner, enabling text-driven 3D garment generation and
editing. For appearance modeling, we leverage the zero-shot generation
capability of ControlNet to synthesize view-consistent texture maps in a single
feed-forward inference step, thereby drastically decreasing the generation time
as compared to existing methods. We demonstrate superior performance over
current SOTAs for learning 3D garment latent space, garment interpolation, and
text-driven texture synthesis, supported by quantitative evaluation and
qualitative user study. The unposed 3D garment meshes generated using WordRobe
can be directly fed to standard cloth simulation & animation pipelines without
any post-processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing Auto-regressive Models for 3D Shape Generation at Capacity and
  Scalability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive models have achieved impressive results in 2D image
generation by modeling joint distributions in grid space. In this paper, we
extend auto-regressive models to 3D domains, and seek a stronger ability of 3D
shape generation by improving auto-regressive models at capacity and
scalability simultaneously. Firstly, we leverage an ensemble of publicly
available 3D <span class="highlight-title">dataset</span>s to facilitate the training of large-scale models. It
consists of a comprehensive collection of approximately 900,000 objects, with
multiple properties of meshes, points, voxels, rendered images, and text
captions. This diverse labeled <span class="highlight-title">dataset</span>, termed Objaverse-Mix, empowers our
model to learn from a wide range of object variations. However, directly
applying 3D auto-regression encounters critical challenges of high
computational demands on volumetric grids and ambiguous auto-regressive order
along grid dimensions, resulting in inferior quality of 3D shapes. To this end,
we then present a novel framework Argus3D in terms of capacity. Concretely, our
approach introduces discrete representation learning based on a latent vector
instead of volumetric grids, which not only reduces computational costs but
also preserves essential geometric details by learning the joint distributions
in a more tractable order. The capacity of conditional generation can thus be
realized by simply concatenating various conditioning inputs to the latent
vector, such as point clouds, categories, images, and texts. In addition,
thanks to the simplicity of our model architecture, we naturally scale up our
approach to a larger model with an impressive 3.6 billion parameters, further
enhancing the quality of versatile 3D generation. Extensive experiments on four
generation tasks demonstrate that Argus3D can synthesize diverse and faithful
shapes across multiple categories, achieving remarkable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://argus-3d.github.io/ . Datasets:
  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:
  substantial text overlap with arXiv:2303.14700</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated
  <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method, namely NeuS-PIR, for recovering relightable
neural surfaces using pre-integrated <span class="highlight-title">rendering</span> from multi-view images or video.
Unlike methods based on <span class="highlight-title">NeRF</span> and discrete meshes, our method utilizes implicit
neural surface representation to reconstruct high-quality geometry, which
facilitates the factorization of the radiance field into two components: a
spatially varying material field and an all-frequency lighting representation.
This factorization, jointly optimized using an adapted differentiable
pre-integrated <span class="highlight-title">rendering</span> framework with material encoding regularization, in
turn addresses the ambiguity of geometry reconstruction and leads to better
disentanglement and refinement of each scene property. Additionally, we
introduced a method to distil indirect illumination fields from the learned
representations, further recovering the complex illumination effect like
inter-reflection. Consequently, our method enables advanced applications such
as relighting, which can be seamlessly integrated with modern graphics engines.
Qualitative and quantitative experiments have shown that NeuS-PIR outperforms
existing methods across various tasks on both synthetic and real <span class="highlight-title">dataset</span>s.
Source code is available at https://github.com/Sheldonmao/NeuSPIR
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octree-GS: Towards Consistent Real-time <span class="highlight-title">Rendering</span> with LOD-Structured 3D
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D Gaussian splatting (3D-GS) has shown remarkable <span class="highlight-title">rendering</span>
fidelity and efficiency compared to <span class="highlight-title">NeRF</span>-based neural scene representations.
While demonstrating the potential for real-time <span class="highlight-title">rendering</span>, 3D-GS encounters
<span class="highlight-title">rendering</span> bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent <span class="highlight-title">rendering</span> speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final <span class="highlight-title">rendering</span> results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent <span class="highlight-title">rendering</span> performance with
adaptive LOD adjustments while maintaining high-fidelity <span class="highlight-title">rendering</span> results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/octree-gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">NeRF</span>-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using
  Heuristics-Guided Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (<span class="highlight-title">NeRF</span>) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, <span class="highlight-title">rendering</span> them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely "Heuristics-Guided Segmentation" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for <span class="highlight-title">NeRF</span>s trained
in non-static scenes. Project page: https://cnhaox.github.io/<span class="highlight-title">NeRF</span>-HuGS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> In<span class="highlight-title">NeRF</span>360: Text-Guided 3D-Consistent Object Inpainting on 360-degree
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong<span class="highlight-author">qing Wang</span>, Tong Zhang, Alaa Abboud, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose In<span class="highlight-title">NeRF</span>360, an automatic system that accurately removes
text-specified objects from 360-degree Neural Radiance Fields (<span class="highlight-title">NeRF</span>). The
challenge is to effectively remove objects while inpainting perceptually
consistent content for the missing regions, which is particularly demanding for
existing <span class="highlight-title">NeRF</span> models due to their implicit volumetric representation. Moreover,
unbounded scenes are more prone to floater artifacts in the inpainted region
than frontal-facing scenes, as the change of object appearance and background
across views is more sensitive to inaccurate segmentations and inconsistent
inpainting. With a trained <span class="highlight-title">NeRF</span> and a text description, our method efficiently
removes specified objects and inpaints visually consistent content without
artifacts. We apply depth-space warping to enforce consistency across multiview
text-encoded segmentations, and then refine the inpainted <span class="highlight-title">NeRF</span> model using
perceptual priors and 3D <span class="highlight-title">diffusion</span>-based geometric priors to ensure visual
plausibility. Through extensive experiments in segmentation and inpainting on
360-degree and frontal-facing <span class="highlight-title">NeRF</span>s, we show that our approach is effective and
enhances <span class="highlight-title">NeRF</span>'s editability. Project page: https://ivrl.github.io/In<span class="highlight-title">NeRF</span>360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated
  <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method, namely NeuS-PIR, for recovering relightable
neural surfaces using pre-integrated <span class="highlight-title">rendering</span> from multi-view images or video.
Unlike methods based on <span class="highlight-title">NeRF</span> and discrete meshes, our method utilizes implicit
neural surface representation to reconstruct high-quality geometry, which
facilitates the factorization of the radiance field into two components: a
spatially varying material field and an all-frequency lighting representation.
This factorization, jointly optimized using an adapted differentiable
pre-integrated <span class="highlight-title">rendering</span> framework with material encoding regularization, in
turn addresses the ambiguity of geometry reconstruction and leads to better
disentanglement and refinement of each scene property. Additionally, we
introduced a method to distil indirect illumination fields from the learned
representations, further recovering the complex illumination effect like
inter-reflection. Consequently, our method enables advanced applications such
as relighting, which can be seamlessly integrated with modern graphics engines.
Qualitative and quantitative experiments have shown that NeuS-PIR outperforms
existing methods across various tasks on both synthetic and real <span class="highlight-title">dataset</span>s.
Source code is available at https://github.com/Sheldonmao/NeuSPIR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic
  Human Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality human reconstruction and photo-realistic <span class="highlight-title">rendering</span> of a dynamic
scene is a long-standing problem in computer vision and graphics. Despite
considerable efforts invested in developing various capture systems and
reconstruction algorithms, recent advancements still struggle with loose or
oversized clothing and overly complex poses. In part, this is due to the
challenges of acquiring high-quality human <span class="highlight-title">dataset</span>s. To facilitate the
development of these fields, in this paper, we present PKU-DyMVHumans, a
versatile human-centric <span class="highlight-title">dataset</span> for high-fidelity reconstruction and <span class="highlight-title">rendering</span>
of dynamic human scenarios from dense multi-view videos. It comprises 8.2
million frames captured by more than 56 synchronized cameras across diverse
scenarios. These sequences comprise 32 human subjects across 45 different
scenarios, each with a high-detailed appearance and realistic human motion.
Inspired by recent advancements in neural radiance field (<span class="highlight-title">NeRF</span>)-based scene
representations, we carefully set up an off-the-shelf framework that is easy to
provide those state-of-the-art <span class="highlight-title">NeRF</span>-based implementations and benchmark on
PKU-DyMVHumans <span class="highlight-title">dataset</span>. It is paving the way for various applications like
fine-grained foreground/background decomposition, high-quality human
reconstruction and photo-realistic novel view synthesis of a dynamic scene.
Extensive studies are performed on the benchmark, demonstrating new
observations and challenges that emerge from using such high-fidelity dynamic
data. The <span class="highlight-title">dataset</span> is available at: https://pku-dymvhumans.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of <span class="highlight-title">dataset</span>s, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic <span class="highlight-title">dataset</span>
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed <span class="highlight-title">dataset</span>, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed <span class="highlight-title">dataset</span> and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The <span class="highlight-title">dataset</span>, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying <span class="highlight-title">Diffusion</span> Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that <span class="highlight-title">diffusion</span> models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves <span class="highlight-title">diffusion</span> sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in <span class="highlight-title">diffusion</span> U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable <span class="highlight-title">Diffusion</span>, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty <span class="highlight-title">prompt</span>s and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSDF: 3DGS Meets SDF for Improved <span class="highlight-title">Rendering</span> and Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Presenting a 3D scene from multiview images remains a core and long-standing
challenge in computer vision and computer graphics. Two main requirements lie
in <span class="highlight-title">rendering</span> and reconstruction. Notably, SOTA <span class="highlight-title">rendering</span> quality is usually
achieved with neural volumetric <span class="highlight-title">rendering</span> techniques, which rely on aggregated
point/primitive-wise color and neglect the underlying scene geometry. Learning
of neural implicit surfaces is sparked from the success of neural <span class="highlight-title">rendering</span>.
Current works either constrain the distribution of density fields or the shape
of primitives, resulting in degraded <span class="highlight-title">rendering</span> quality and flaws on the learned
scene surfaces. The efficacy of such methods is limited by the inherent
constraints of the chosen neural representation, which struggles to capture
fine surface details, especially for larger, more intricate scenes. To address
these issues, we introduce GSDF, a novel dual-branch architecture that combines
the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)
representation with neural Signed Distance Fields (SDF). The core idea is to
leverage and enhance the strengths of each branch while alleviating their
limitation through mutual guidance and joint supervision. We show on diverse
scenes that our design unlocks the potential for more accurate and detailed
surface reconstructions, and at the meantime benefits 3DGS <span class="highlight-title">rendering</span> with
structures that are more aligned with the underlying geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/GSDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Point-based Inverse <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present differentiable point-based inverse <span class="highlight-title">rendering</span>, DPIR, an
analysis-by-synthesis method that processes images captured under diverse
illuminations to estimate shape and spatially-varying BRDF. To this end, we
adopt point-based <span class="highlight-title">rendering</span>, eliminating the need for multiple samplings per
ray, typical of volumetric <span class="highlight-title">rendering</span>, thus significantly enhancing the speed of
inverse <span class="highlight-title">rendering</span>. To realize this idea, we devise a hybrid point-volumetric
representation for geometry and a regularized basis-BRDF representation for
reflectance. The hybrid geometric representation enables fast <span class="highlight-title">rendering</span> through
point-based splatting while retaining the geometric details and stability
inherent to SDF-based representations. The regularized basis-BRDF mitigates the
ill-posedness of inverse <span class="highlight-title">rendering</span> stemming from limited light-view angular
samples. We also propose an efficient shadow detection method using point-based
shadow map <span class="highlight-title">rendering</span>. Our extensive evaluations demonstrate that DPIR
outperforms prior works in terms of reconstruction accuracy, computational
efficiency, and memory footprint. Furthermore, our explicit point-based
representation and <span class="highlight-title">rendering</span> enables intuitive geometry and reflectance
editing.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnimateMe: 4D Facial Expressions via <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of photorealistic 3D avatar reconstruction and generation has
garnered significant attention in recent years; however, animating such avatars
remains challenging. Recent advances in <span class="highlight-title">diffusion</span> models have notably enhanced
the capabilities of generative models in 2D animation. In this work, we
directly utilize these models within the 3D domain to achieve controllable and
high-fidelity 4D facial animation. By integrating the strengths of <span class="highlight-title">diffusion</span>
processes and geometric deep learning, we employ Graph Neural Networks (GNNs)
as denoising <span class="highlight-title">diffusion</span> models in a novel approach, formulating the <span class="highlight-title">diffusion</span>
process directly on the mesh space and enabling the generation of 3D facial
expressions. This facilitates the generation of facial deformations through a
mesh-<span class="highlight-title">diffusion</span>-based model. Additionally, to ensure temporal coherence in our
animations, we propose a consistent noise sampling method. Under a series of
both quantitative and qualitative experiments, we showcase that the proposed
method outperforms prior work in 4D expression synthesis by generating
high-fidelity extreme expressions. Furthermore, we applied our method to
textured 4D facial expression generation, implementing a straightforward
extension that involves training on a large-scale textured 4D facial expression
database.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remy Sabathier, Niloy J. Mitra, David Novotny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to build animatable dog avatars from monocular videos.
This is challenging as animals display a range of (unpredictable) non-rigid
movements and have a variety of appearance details (e.g., fur, spots, tails).
We develop an approach that links the video frames via a 4D solution that
jointly solves for animal's pose variation, and its appearance (in a canonical
pose). To this end, we significantly improve the quality of template-based
shape fitting by endowing the SMAL parametric model with Continuous Surface
Embeddings, which brings image-to-mesh reprojection constaints that are denser,
and thus stronger, than the previously used sparse semantic keypoint
correspondences. To model appearance, we propose an implicit duplex-mesh
texture that is defined in the canonical pose, but can be deformed using SMAL
pose coefficients and later rendered to enforce a photometric compatibility
with the input video frames. On the challenging CoP3D and APTv2 <span class="highlight-title">dataset</span>s, we
demonstrate superior results (both in terms of pose estimates and predicted
appearance) to existing template-free (RAC) and template-based approaches
(BARC, BITE).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make-Your-Anchor: A <span class="highlight-title">Diffusion</span>-based 2D Avatar Generation Framework <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable process of talking-head-based avatar-creating
solutions, directly generating anchor-style videos with full-body motions
remains challenging. In this study, we propose Make-Your-Anchor, a novel system
necessitating only a one-minute video <span class="highlight-title">clip</span> of an individual for training,
subsequently enabling the automatic generation of anchor-style videos with
precise torso and hand movements. Specifically, we finetune a proposed
structure-guided <span class="highlight-title">diffusion</span> model on input video to render 3D mesh conditions
into human appearances. We adopt a two-stage training strategy for the
<span class="highlight-title">diffusion</span> model, effectively binding movements with specific appearances. To
produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise
<span class="highlight-title">diffusion</span> model to a 3D style without additional training cost, and a simple
yet effective batch-overlapped temporal denoising module is proposed to bypass
the constraints on video length during inference. Finally, a novel
identity-specific face enhancement module is introduced to improve the visual
quality of facial regions in the output videos. Comparative experiments
demonstrate the effectiveness and superiority of the system in terms of visual
quality, temporal coherence, and identity preservation, outperforming SOTA
<span class="highlight-title">diffusion</span>/non-<span class="highlight-title">diffusion</span> methods. Project page:
\url{https://github.com/ICTMCG/Make-Your-Anchor}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFRAME: Reflective Surface Real-Time <span class="highlight-title">Rendering</span> for Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojie Ji, Yufeng Li, Yiyi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the challenging task of achieving real-time novel view
synthesis on various scenes, including highly reflective objects and unbounded
outdoor scenes. Existing real-time <span class="highlight-title">rendering</span> methods, especially those based on
meshes, often have subpar performance in modeling surfaces with rich
view-dependent appearances. Our key idea lies in leveraging meshes for
<span class="highlight-title">rendering</span> acceleration while incorporating a novel approach to parameterize
view-dependent information. We decompose the color into diffuse and specular,
and model the specular color in the reflected direction based on a neural
environment map. Our experiments demonstrate that our method achieves
comparable reconstruction quality for highly reflective surfaces compared to
state-of-the-art offline methods, while also efficiently enabling real-time
<span class="highlight-title">rendering</span> on edge devices such as smartphones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:https://xdimlab.github.io/REFRAME/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOCTR: Disentangled Object-Centric <span class="highlight-title">Transformer</span> for Point Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point scene understanding is a challenging task to process real-world scene
point cloud, which aims at segmenting each object, estimating its pose, and
reconstructing its mesh simultaneously. Recent state-of-the-art method first
segments each object and then processes them independently with multiple stages
for the different sub-tasks. This leads to a complex pipeline to optimize and
makes it hard to leverage the relationship constraints between multiple
objects. In this work, we propose a novel Disentangled Object-Centric
<span class="highlight-title">TRansformer</span> (DOCTR) that explores object-centric representation to facilitate
learning with multiple objects for the multiple sub-tasks in a unified manner.
Each object is represented as a query, and a <span class="highlight-title">Transformer</span> decoder is adapted to
iteratively optimize all the queries involving their relationship. In
particular, we introduce a semantic-geometry disentangled query (SGDQ) design
that enables the query features to attend separately to semantic information
and geometric information relevant to the corresponding sub-tasks. A hybrid
bipartite matching module is employed to well use the supervisions from all the
sub-tasks during training. Qualitative and quantitative experimental results
demonstrate that our method achieves state-of-the-art performance on the
challenging ScanNet <span class="highlight-title">dataset</span>. Code is available at
https://github.com/SAITPublic/DOCTR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISN: Deep Implicit Surface Network for High-quality Single-view 3D
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1905.10711v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1905.10711v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D shapes from single-view images has been a long-standing
research problem. In this paper, we present DISN, a Deep Implicit Surface
Network which can generate a high-quality detail-rich 3D mesh from an 2D image
by predicting the underlying signed distance fields. In addition to utilizing
global image features, DISN predicts the projected location for each 3D point
on the 2D image, and extracts local features from the image feature maps.
Combining global and local features significantly improves the accuracy of the
signed distance field prediction, especially for the detail-rich areas. To the
best of our knowledge, DISN is the first method that constantly captures
details such as holes and thin structures present in 3D shapes from single-view
images. DISN achieves the state-of-the-art single-view reconstruction
performance on a variety of shape categories reconstructed from both synthetic
and real images. Code is available at https://github.com/xharlie/DISN The
supplementary can be found at
https://xharlie.github.io/images/neurips_2019_supp.pdf
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This project was in part supported by the gift funding to the
  University of Southern California from Adobe Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans commonly work with multiple objects in daily life and can intuitively
transfer manipulation skills to novel objects by understanding object
functional regularities. However, existing technical approaches for analyzing
and synthesizing hand-object manipulation are mostly limited to handling a
single hand and object due to the lack of data support. To address this, we
construct TACO, an extensive bimanual hand-object-interaction <span class="highlight-title">dataset</span> spanning
a large variety of tool-action-object compositions for daily human activities.
TACO contains 2.5K motion sequences paired with third-person and egocentric
views, precise hand-object 3D meshes, and action labels. To rapidly expand the
data scale, we present a fully automatic data acquisition pipeline combining
multi-view sensing with an optical motion capture system. With the vast
research fields provided by TACO, we benchmark three generalizable
hand-object-interaction tasks: compositional action recognition, generalizable
hand-object motion forecasting, and cooperative grasp synthesis. Extensive
experiments reveal new insights, challenges, and opportunities for advancing
the studies of generalizable hand-object motion analysis and synthesis. Our
data and code are available at https://taco2024.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera
  Calibration and Orientation Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a long time, in reconstructing 3D human bodies from monocular images,
most methods opted to simplify the task by minimizing the influence of the
camera. Using a coarse focal length setting results in the reconstructed bodies
not aligning well with distorted images. Ignoring camera rotation leads to an
unrealistic reconstructed body pose in world space. Consequently, the
application scenarios of existing methods are confined to controlled
environments. When confronted with complex and diverse in-the-wild images, they
struggle to achieve accurate and reasonable reconstruction in world space. To
address the above issues, we propose W-HMR, which decouples global body
recovery into camera calibration, local body recovery, and global body
orientation correction. We design the first weak-supervised camera calibration
method for body distortion, eliminating dependence on focal length labels and
achieving finer mesh-image alignment. We propose a novel orientation correction
module to allow the reconstructed human body to remain normal in world space.
Decoupling body orientation and body pose enables our model to consider the
accuracy in camera coordinate and the reasonableness in world coordinate
simultaneously, expanding the range of applications. As a result, W-HMR
achieves high-quality reconstruction in dual coordinate systems, particularly
in challenging scenes. Codes and demos have been released on the project page
https://yw0208.github.io/w-hmr/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yw0208.github.io/w-hmr/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VP3D: Unleashing 2D Visual <span class="highlight-title">Prompt</span> for Text-to-3D Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(<span class="highlight-title">NeRF</span>) by directly distilling prior knowledge from 2D <span class="highlight-title">diffusion</span> models.
However, current SDS-based models still struggle with intricate text <span class="highlight-title">prompt</span>s
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
<span class="highlight-title">Prompt</span>-guided text-to-3D <span class="highlight-title">diffusion</span> model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual <span class="highlight-title">prompt</span> to boost text-to-3D generation.
Instead of solely supervising SDS with text <span class="highlight-title">prompt</span>, VP3D first capitalizes on
2D <span class="highlight-title">diffusion</span> model to generate a high-quality image from input text, which
subsequently acts as visual <span class="highlight-title">prompt</span> to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages <span class="highlight-title">rendering</span> images of 3D models to
better visually align with 2D visual <span class="highlight-title">prompt</span> and semantically match with text
<span class="highlight-title">prompt</span>. Through extensive experiments, we show that the 2D Visual <span class="highlight-title">Prompt</span> in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual <span class="highlight-title">prompt</span> with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project page: https://vp3d-cvpr24.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CVT-xRF: Contrastive In-Voxel <span class="highlight-title">Transformer</span> for 3D Consistent Radiance
  Fields from Sparse Inputs <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingji Zhong, Lanqing Hong, Zhenguo Li, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) have shown impressive capabilities for
photorealistic novel view synthesis when trained on dense inputs. However, when
trained on sparse inputs, <span class="highlight-title">NeRF</span> typically encounters issues of incorrect density
or color predictions, mainly due to insufficient coverage of the scene causing
partial and sparse supervision, thus leading to significant performance
degradation. While existing works mainly consider ray-level consistency to
construct 2D learning regularization based on rendered color, depth, or
semantics on image planes, in this paper we propose a novel approach that
models 3D spatial field consistency to improve <span class="highlight-title">NeRF</span>'s performance with sparse
inputs. Specifically, we first adopt a voxel-based ray sampling strategy to
ensure that the sampled rays intersect with a certain voxel in 3D space. We
then randomly sample additional points within the voxel and apply a <span class="highlight-title">Transformer</span>
to infer the properties of other points on each ray, which are then
incorporated into the volume <span class="highlight-title">rendering</span>. By backpropagating through the
<span class="highlight-title">rendering</span> loss, we enhance the consistency among neighboring points.
Additionally, we propose to use a contrastive loss on the encoder output of the
<span class="highlight-title">Transformer</span> to further improve consistency within each voxel. Experiments
demonstrate that our method yields significant improvement over different
radiance fields in the sparse inputs setting, and achieves comparable
performance with current works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by CVPR 2024. Project page is available at
  https://zhongyingji.github.io/CVT-xRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-<span class="highlight-title">NeRF</span>: Neural Radiance Field Based On Spike Camera <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-<span class="highlight-title">NeRF</span>, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of <span class="highlight-title">NeRF</span>, the inputs of Spike-<span class="highlight-title">NeRF</span> are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-<span class="highlight-title">NeRF</span> produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAE<span class="highlight-title">NeRF</span>: Local Appearance Editing for Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Radl, Michael Steiner, Andreas Kurz, Markus Steinberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the omnipresence of Neural Radiance Fields (<span class="highlight-title">NeRF</span>s), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for <span class="highlight-title">NeRF</span>s is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, <span class="highlight-title">rendering</span> them less
useful in practice. We address these limitations with LAE<span class="highlight-title">NeRF</span>, a unified
framework for photorealistic and non-photorealistic appearance editing of
<span class="highlight-title">NeRF</span>s. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAE<span class="highlight-title">NeRF</span> enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024! Project website:
  https://r4dl.github.io/LAENeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bio<span class="highlight-title">NeRF</span>: Biologically Plausible Neural Radiance Fields for View
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Bio<span class="highlight-title">NeRF</span>, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since <span class="highlight-title">NeRF</span> relies on the network weights to store the scene's
3-dimensional representation, Bio<span class="highlight-title">NeRF</span> implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
Bio<span class="highlight-title">NeRF</span> also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that Bio<span class="highlight-title">NeRF</span> outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
<span class="highlight-title">dataset</span>s: real-world images and synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URS-<span class="highlight-title">NeRF</span>: Unordered Rolling Shutter Bundle Adjustment for Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (<span class="highlight-title">NeRF</span>), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing <span class="highlight-title">NeRF</span> methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
<span class="highlight-title">NeRF</span> requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Semantic Priors from SAM to Efficient Image Restoration
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image restoration (IR), leveraging semantic priors from segmentation
models has been a common approach to improve performance. The recent segment
anything model (SAM) has emerged as a powerful tool for extracting advanced
semantic priors to enhance IR tasks. However, the computational cost of SAM is
prohibitive for IR, compared to existing smaller IR models. The incorporation
of SAM for extracting semantic priors considerably hampers the model inference
efficiency. To address this issue, we propose a general framework to distill
SAM's semantic knowledge to boost exiting IR models without interfering with
their inference process. Specifically, our proposed framework consists of the
semantic priors fusion (SPF) scheme and the semantic priors distillation (SPD)
scheme. SPF fuses two kinds of information between the restored image predicted
by the original IR model and the semantic mask predicted by SAM for the refined
restored image. SPD leverages a self-distillation manner to distill the fused
semantic priors to boost the performance of original IR models. Additionally,
we design a semantic-guided relation (SGR) module for SPD, which ensures
semantic feature representation space consistency to fully distill the priors.
We demonstrate the effectiveness of our framework across multiple IR models and
tasks, including deraining, deblurring, and denoising.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frankenstein: Generating Semantic-Compositional 3D Scenes in One
  Tri-Plane 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Frankenstein, a <span class="highlight-title">diffusion</span>-based framework that can generate
semantic-compositional 3D scenes in a single pass. Unlike existing methods that
output a single, unified 3D shape, Frankenstein simultaneously generates
multiple separated shapes, each corresponding to a semantically meaningful
part. The 3D scene information is encoded in one single tri-plane tensor, from
which multiple Singed Distance Function (SDF) fields can be decoded to
represent the compositional shapes. During training, an auto-encoder compresses
tri-planes into a latent space, and then the denoising <span class="highlight-title">diffusion</span> process is
employed to approximate the distribution of the compositional scenes.
Frankenstein demonstrates promising results in generating room interiors as
well as human avatars with automatically separated parts. The generated scenes
facilitate many downstream applications, such as part-wise re-texturing, object
rearrangement in the room or avatar cloth re-targeting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video: https://youtu.be/lRn-HqyCrLI</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ghost on the Shell: An Expressive Representation of General 3D Shapes <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of photorealistic virtual worlds requires the accurate modeling
of 3D surface geometry for a wide range of objects. For this, meshes are
appealing since they 1) enable fast physics-based <span class="highlight-title">rendering</span> with realistic
material and lighting, 2) support physical simulation, and 3) are
memory-efficient for modern graphics pipelines. Recent work on reconstructing
and statistically modeling 3D shape, however, has critiqued meshes as being
topologically inflexible. To capture a wide range of object shapes, any 3D
representation must be able to model solid, watertight, shapes as well as thin,
open, surfaces. Recent work has focused on the former, and methods for
reconstructing open surfaces do not support fast reconstruction with material
and lighting or unconditional generative modelling. Inspired by the observation
that open surfaces can be seen as islands floating on watertight surfaces, we
parameterize open surfaces by defining a manifold signed distance field on
watertight templates. With this parameterization, we further develop a
grid-based and differentiable representation that parameterizes both watertight
and non-watertight meshes of arbitrary topology. Our new representation, called
Ghost-on-the-Shell (G-Shell), enables two important applications:
differentiable rasterization-based reconstruction from multiview images and
generative modelling of non-watertight meshes. We empirically demonstrate that
G-Shell achieves state-of-the-art performance on non-watertight mesh
reconstruction and generation tasks, while also performing effectively for
watertight meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:
  https://gshell3d.github.io/)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse <span class="highlight-title">Rendering</span> of Glossy Objects via the Neural Plenoptic Function
  and Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse <span class="highlight-title">rendering</span> aims at recovering both geometry and materials of objects.
It provides a more compatible reconstruction for conventional <span class="highlight-title">rendering</span>
engines, compared with the neural radiance fields (<span class="highlight-title">NeRF</span>s). On the other hand,
existing <span class="highlight-title">NeRF</span>-based inverse <span class="highlight-title">rendering</span> methods cannot handle glossy objects with
local light interactions well, as they typically oversimplify the illumination
as a 2D environmental map, which assumes infinite lights only. Observing the
superiority of <span class="highlight-title">NeRF</span>s in recovering radiance fields, we propose a novel 5D
Neural Plenoptic Function (NeP) based on <span class="highlight-title">NeRF</span>s and ray tracing, such that more
accurate lighting-object interactions can be formulated via the <span class="highlight-title">rendering</span>
equation. We also design a material-aware cone sampling strategy to efficiently
integrate lights inside the BRDF lobes with the help of pre-filtered radiance
fields. Our method has two stages: the geometry of the target object and the
pre-filtered environmental radiance fields are reconstructed in the first
stage, and materials of the target object are estimated in the second stage
with the proposed NeP and material-aware cone sampling strategy. Extensive
experiments on the proposed real-world and synthetic <span class="highlight-title">dataset</span>s demonstrate that
our method can reconstruct high-fidelity geometry/materials of challenging
glossy objects with complex lighting interactions from nearby objects. Project
webpage: https://whyy.site/paper/nep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 paper. Project webpage https://whyy.site/paper/nep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity-<span class="highlight-title">NeRF</span>: Detecting and Removing Moving Entities in Urban Scenes <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the study of Neural Radiance Fields (<span class="highlight-title">NeRF</span>) for dynamic
scenes often involve explicit modeling of scene dynamics. However, this
approach faces challenges in modeling scene dynamics in urban environments,
where moving objects of various categories and scales are present. In such
settings, it becomes crucial to effectively eliminate moving objects to
accurately reconstruct static backgrounds. Our research introduces an
innovative method, termed here as Entity-<span class="highlight-title">NeRF</span>, which combines the strengths of
knowledge-based and statistical strategies. This approach utilizes entity-wise
statistics, leveraging entity segmentation and stationary entity classification
through thing/stuff segmentation. To assess our methodology, we created an
urban scene <span class="highlight-title">dataset</span> masked with moving objects. Our comprehensive experiments
demonstrate that Entity-<span class="highlight-title">NeRF</span> notably outperforms existing techniques in
removing moving objects and reconstructing static urban backgrounds, both
quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024), Project website:
  https://otonari726.github.io/entitynerf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D
  Gaussian Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently neural radiance fields (<span class="highlight-title">NeRF</span>) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
<span class="highlight-title">NeRF</span>-based methods are hindered by their computationally intensive and
time-consuming volume <span class="highlight-title">rendering</span> pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various <span class="highlight-title">dataset</span>s
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://zju3dv.github.io/cg-slam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are <span class="highlight-title">NeRF</span>s ready for autonomous driving? Towards closing the
  real-to-simulation gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of <span class="highlight-title">rendering</span> methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving <span class="highlight-title">rendering</span> fidelity, we
explore simple yet effective methods to enhance perception model robustness to
<span class="highlight-title">NeRF</span> artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural <span class="highlight-title">rendering</span> technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different <span class="highlight-title">pre-train</span>ing strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Is Enough: Only Semantic Information For <span class="highlight-title">NeRF</span> Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibo Wang, Song Zhang, Ping Huang, Donghai Zhang, Wei Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research that combines implicit 3D representation with semantic
information, like Semantic-<span class="highlight-title">NeRF</span>, has proven that <span class="highlight-title">NeRF</span> model could perform
excellently in <span class="highlight-title">rendering</span> 3D structures with semantic labels. This research aims
to extend the Semantic Neural Radiance Fields (Semantic-<span class="highlight-title">NeRF</span>) model by focusing
solely on semantic output and removing the RGB output component. We reformulate
the model and its training procedure to leverage only the cross-entropy loss
between the model semantic output and the ground truth semantic images,
removing the colour data traditionally used in the original Semantic-<span class="highlight-title">NeRF</span>
approach. We then conduct a series of identical experiments using the original
and the modified Semantic-<span class="highlight-title">NeRF</span> model. Our primary objective is to obverse the
impact of this modification on the model performance by Semantic-<span class="highlight-title">NeRF</span>, focusing
on tasks such as scene understanding, object detection, and segmentation. The
results offer valuable insights into the new way of <span class="highlight-title">rendering</span> the scenes and
provide an avenue for further research and development in semantic-focused 3D
scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        unhong Zhao, Wei Ying, Yaoqiang Pan, Zhenfeng Yi, Chao Chen, Kewei Hu, Hanwen Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate collection of plant phenotyping is critical to optimising
sustainable farming practices in precision agriculture. Traditional phenotyping
in controlled laboratory environments, while valuable, falls short in
understanding plant growth under real-world conditions. Emerging sensor and
digital technologies offer a promising approach for direct phenotyping of
plants in farm environments. This study investigates a learning-based
phenotyping method using the Neural Radiance Field to achieve accurate in-situ
phenotyping of pepper plants in greenhouse environments. To quantitatively
evaluate the performance of this method, traditional point cloud registration
on 3D scanning data is implemented for comparison. Experimental result shows
that <span class="highlight-title">NeRF</span>(Neural Radiance Fields) achieves competitive accuracy compared to the
3D scanning methods. The mean distance error between the scanner-based method
and the <span class="highlight-title">NeRF</span>-based method is 0.865mm. This study shows that the learning-based
<span class="highlight-title">NeRF</span> method achieves similar accuracy to 3D scanning-based methods but with
improved scalability and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular
  Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable <span class="highlight-title">NeRF</span> is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate neural textures using
2D-3D correspondences. This enables the separation of diffused color generation
and lighting correction branches that jointly compose an RGB prediction. The
design allows to control intermediate results for human pose, body shape,
texture, and lighting with text <span class="highlight-title">prompt</span>s. An image-conditioned <span class="highlight-title">diffusion</span> model
thereby helps to animate appearance and pose of the 3D avatar to create video
sequences with previously unseen human motion. Extensive experiments show that
ReCaLaB outperforms previous monocular approaches in terms of image quality for
image synthesis tasks. Moreover, natural language offers an intuitive user
interface for creative manipulation of 3D human avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video link: https://youtu.be/Oz83z1es2J4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insert<span class="highlight-title">NeRF</span>: Instilling Generalizability into <span class="highlight-title">NeRF</span> with HyperNet Modules <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanqi Bao, Tianyu Ding, Jing Huo, Wenbin Li, Yuxin Li, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing Neural Radiance Fields (<span class="highlight-title">NeRF</span>) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla <span class="highlight-title">NeRF</span> framework. We introduce Insert<span class="highlight-title">NeRF</span>, a method for
INStilling gEneRalizabiliTy into <span class="highlight-title">NeRF</span>. By utilizing multiple plug-and-play
HyperNet modules, Insert<span class="highlight-title">NeRF</span> dynamically tailors <span class="highlight-title">NeRF</span>'s weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other <span class="highlight-title">NeRF</span>-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/Insert<span class="highlight-title">NeRF</span>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown
  Domains <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative framework designed to train an image
deblurring algorithm tailored to a specific camera device. This algorithm works
by transforming a blurry input image, which is challenging to deblur, into
another blurry image that is more amenable to deblurring. The transformation
process, from one blurry state to another, leverages unpaired data consisting
of sharp and blurry images captured by the target camera device. Learning this
blur-to-blur transformation is inherently simpler than direct blur-to-sharp
conversion, as it primarily involves modifying blur patterns rather than the
intricate task of reconstructing fine image details. The efficacy of the
proposed approach has been demonstrated through comprehensive experiments on
various benchmarks, where it significantly outperforms state-of-the-art methods
both quantitatively and qualitatively. Our code and data are available at
https://zero1778.github.io/blur2blur/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPoser: <span class="highlight-title">Diffusion</span> Model as Robust 3D Human Pose Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05541v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05541v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Lu, Jing Lin, Hongkun Dou, Ailing Zeng, Yue Deng, Yulun Zhang, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work targets to construct a robust human pose prior. However, it remains
a persistent challenge due to biomechanical constraints and diverse human
movements. Traditional priors like VAEs and NDFs often exhibit shortcomings in
realism and generalization, notably with unseen noisy poses. To address these
issues, we introduce DPoser, a robust and versatile human pose prior built upon
<span class="highlight-title">diffusion</span> models. DPoser regards various pose-centric tasks as inverse problems
and employs variational <span class="highlight-title">diffusion</span> sampling for efficient solving. Accordingly,
designed with optimization frameworks, DPoser seamlessly benefits human mesh
recovery, pose generation, pose completion, and motion denoising tasks.
Furthermore, due to the disparity between the articulated poses and structured
images, we propose truncated timestep scheduling to enhance the effectiveness
of DPoser. Our approach demonstrates considerable enhancements over common
uniform scheduling used in image domains, boasting improvements of 5.4%, 17.2%,
and 3.8% across human mesh recovery, pose completion, and motion denoising,
respectively. Comprehensive experiments demonstrate the superiority of DPoser
over existing state-of-the-art pose priors across multiple tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://dposer.github.io; Code Released:
  https://github.com/moonbow721/DPoser</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using
  <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengming Yu, Zhiyang Dou, Xiaoxiao Long, Cheng Lin, Zekun Li, Yuan Liu, Norman Müller, Taku Komura, Marc Habermann, Christian Theobalt, Xin Li, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Surf-D, a novel method for generating high-quality 3D shapes as
Surfaces with arbitrary topologies using <span class="highlight-title">Diffusion</span> models. Previous methods
explored shape generation with different representations and they suffer from
limited topologies and poor geometry details. To generate high-quality surfaces
of arbitrary topologies, we use the Unsigned Distance Field (UDF) as our
surface representation to accommodate arbitrary topologies. Furthermore, we
propose a new pipeline that employs a point-based AutoEncoder to learn a
compact and continuous latent space for accurately encoding UDF and support
high-resolution mesh extraction. We further show that our new pipeline
significantly outperforms the prior approaches to learning the distance fields,
such as the grid-based AutoEncoder, which is not scalable and incapable of
learning accurate UDF. In addition, we adopt a curriculum learning strategy to
efficiently embed various surfaces. With the <span class="highlight-title">pretrain</span>ed shape latent space, we
employ a latent <span class="highlight-title">diffusion</span> model to acquire the distribution of various shapes.
Extensive experiments are presented on using Surf-D for unconditional
generation, category conditional generation, image conditional generation, and
text-to-shape tasks. The experiments demonstrate the superior performance of
Surf-D in shape generation across multiple modalities as conditions. Visit our
project page at https://yzmblog.github.io/projects/SurfD/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yzmblog.github.io/projects/SurfD/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UP<span class="highlight-title">NeRF</span>: A Unified Framework for Monocular 3D Object Reconstruction and
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Guo, Abhinav Kumar, Cheng Zhao, Ruoyu Wang, Xinyu Huang, Liu Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D reconstruction for categorical objects heavily relies on
accurately perceiving each object's pose. While gradient-based optimization
within a <span class="highlight-title">NeRF</span> framework updates initially given poses, this paper highlights
that such a scheme fails when the initial pose even moderately deviates from
the true pose. Consequently, existing methods often depend on a third-party 3D
object to provide an initial object pose, leading to increased complexity and
generalization issues. To address these challenges, we present UP<span class="highlight-title">NeRF</span>, a
Unified framework integrating Pose estimation and <span class="highlight-title">NeRF</span>-based reconstruction,
bringing us closer to real-time monocular 3D object reconstruction. UP<span class="highlight-title">NeRF</span>
decouples the object's dimension estimation and pose refinement to resolve the
scale-depth ambiguity, and introduces an effective projected-box representation
that generalizes well cross different domains. While using a dedicated pose
estimator that smoothly integrates into an object-centric <span class="highlight-title">NeRF</span>, UP<span class="highlight-title">NeRF</span> is free
from external 3D detectors. UP<span class="highlight-title">NeRF</span> achieves state-of-the-art results in both
reconstruction and pose estimation tasks on the nuScenes <span class="highlight-title">dataset</span>. Furthermore,
UP<span class="highlight-title">NeRF</span> exhibits exceptional Cross-<span class="highlight-title">dataset</span> generalization on the KITTI and Waymo
<span class="highlight-title">dataset</span>s, surpassing prior methods with up to 50% reduction in rotation and
translation error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image
  Collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis from unconstrained in-the-wild images remains a
meaningful but challenging task. The photometric variation and transient
occluders in those unconstrained images make it difficult to reconstruct the
original scene accurately. Previous approaches tackle the problem by
introducing a global appearance feature in Neural Radiance Fields (<span class="highlight-title">NeRF</span>).
However, in the real world, the unique appearance of each tiny point in a scene
is determined by its independent intrinsic material attributes and the varying
environmental impacts it receives. Inspired by this fact, we propose Gaussian
in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the
scene and introduces separated intrinsic and dynamic appearance feature for
each point, capturing the unchanged scene appearance along with dynamic
variation like illumination and weather. Additionally, an adaptive sampling
strategy is presented to allow each Gaussian point to focus on the local and
detailed information more effectively. We also reduce the impact of transient
occluders using a 2D visibility map. More experiments have demonstrated better
reconstruction quality and details of GS-W compared to previous methods, with a
$1000\times$ increase in <span class="highlight-title">rendering</span> speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Aware Sparse-View X-ray 3D Reconstruction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Cai, Jiahao Wang, Alan Yuille, Zongwei Zhou, Angtian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray, known for its ability to reveal internal structures of objects, is
expected to provide richer information for 3D reconstruction than visible
light. Yet, existing neural radiance fields (<span class="highlight-title">NeRF</span>) algorithms overlook this
important nature of X-ray, leading to their limitations in capturing structural
contents of imaged objects. In this paper, we propose a framework,
Structure-Aware X-ray Neural Radiodensity Fields (SAX-<span class="highlight-title">NeRF</span>), for sparse-view
X-ray 3D reconstruction. Firstly, we design a Line Segment-based <span class="highlight-title">Transformer</span>
(Lineformer) as the backbone of SAX-<span class="highlight-title">NeRF</span>. Linefomer captures internal
structures of objects in 3D space by modeling the dependencies within each line
segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray
sampling strategy to extract contextual and geometric information in 2D
projection. Plus, we collect a larger-scale <span class="highlight-title">dataset</span> X3D covering wider X-ray
applications. Experiments on X3D show that SAX-<span class="highlight-title">NeRF</span> surpasses previous
<span class="highlight-title">NeRF</span>-based methods by 12.56 and 2.49 dB on novel view synthesis and CT
reconstruction. Code, models, and data are released at
https://github.com/caiyuanhao1998/SAX-<span class="highlight-title">NeRF</span>
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; The first Transformer-based method for X-ray and CT 3D
  reconstruction</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Optimization Framework to Enforce Multi-View Consistency for
  Texturing 3D Meshes Using <span class="highlight-title">Pre-Train</span>ed Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Zhao, Chen Song, Xiaodong Gu, Yuan Dong, Qi Zuo, Weihao Yuan, Zilong Dong, Liefeng Bo, Qixing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental problem in the texturing of 3D meshes using <span class="highlight-title">pre-train</span>ed
text-to-image models is to ensure multi-view consistency. State-of-the-art
approaches typically use <span class="highlight-title">diffusion</span> models to aggregate multi-view inputs, where
common issues are the blurriness caused by the averaging operation in the
aggregation step or inconsistencies in local features. This paper introduces an
optimization framework that proceeds in four stages to achieve multi-view
consistency. Specifically, the first stage generates an over-complete set of 2D
textures from a predefined set of viewpoints using an MV-consistent <span class="highlight-title">diffusion</span>
process. The second stage selects a subset of views that are mutually
consistent while covering the underlying 3D model. We show how to achieve this
goal by solving semi-definite programs. The third stage performs non-rigid
alignment to align the selected views across overlapping regions. The fourth
stage solves an MRF problem to associate each mesh face with a selected view.
In particular, the third and fourth stages are iterated, with the cuts obtained
in the fourth stage encouraging non-rigid alignment in the third stage to focus
on regions close to the cuts. Experimental results show that our approach
significantly outperforms baseline approaches both qualitatively and
quantitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per <span class="highlight-title">prompt</span>.
Amortized methods like ATT3D optimize multiple <span class="highlight-title">prompt</span>s simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large <span class="highlight-title">prompt</span> sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger <span class="highlight-title">prompt</span> set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware <span class="highlight-title">diffusion</span> priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training <span class="highlight-title">prompt</span>s. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See the project website at
  https://research.nvidia.com/labs/toronto-ai/LATTE3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeGO: Leveraging a Surface Deformation Network for Animatable Stylized
  Face Generation with One Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D face stylization have made significant strides in few
to zero-shot settings. However, the degree of stylization achieved by existing
methods is often not sufficient for practical applications because they are
mostly based on statistical 3D Morphable Models (3DMM) with limited variations.
To this end, we propose a method that can produce a highly stylized 3D face
model with desired topology. Our methods train a surface deformation network
with 3DMM and translate its domain to the target style using a paired exemplar.
The network achieves stylization of the 3D face mesh by mimicking the style of
the target using a differentiable renderer and directional <span class="highlight-title">CLIP</span> losses.
Additionally, during the inference process, we utilize a Mesh Agnostic Encoder
(MAGE) that takes deformation target, a mesh of diverse topologies as input to
the stylization process and encodes its shape into our latent space. The
resulting stylized face model can be animated by commonly used 3DMM blend
shapes. A set of quantitative and qualitative evaluations demonstrate that our
method can produce highly stylized face meshes according to a given style and
output them in a desired topology. We also demonstrate example applications of
our method including image-based stylized avatar generation, linear
interpolation of geometric styles, and facial animation of stylized avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexRO: Generating Delicate Textures of 3D Models by Recursive
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Wu, Xing Liu, Chenming Wu, Xiaobo Gao, Jialun Liu, Xinqi Liu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents TexRO, a novel method for generating delicate textures of
a known 3D mesh by optimizing its UV texture. The key contributions are
two-fold. We propose an optimal viewpoint selection strategy, that finds the
most miniature set of viewpoints covering all the faces of a mesh. Our
viewpoint selection strategy guarantees the completeness of a generated result.
We propose a recursive optimization pipeline that optimizes a UV texture at
increasing resolutions, with an adaptive denoising method that re-uses existing
textures for new texture generation. Through extensive experimentation, we
demonstrate the superior performance of TexRO in terms of texture quality,
detail preservation, visual consistency, and, notably runtime speed,
outperforming other current methods. The broad applicability of TexRO is
further confirmed through its successful use on diverse 3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Project page:
  \href{https://3d-aigc.github.io/TexRO}{https://3d-aigc.github.io/TexRO}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision<span class="highlight-title">GPT</span>-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of text to visual components facilitates people's daily lives,
such as generating image, videos from text and identifying the desired elements
within the images. Computer vision models involving the multimodal abilities in
the previous days are focused on image detection, classification based on
well-defined objects. Large language models (LLMs) introduces the
transformation from nature language to visual objects, which present the visual
layout for text contexts. OpenAI <span class="highlight-title">GPT</span>-4 has emerged as the pinnacle in LLMs,
while the computer vision (CV) domain boasts a plethora of state-of-the-art
(SOTA) models and algorithms to convert 2D images to their 3D representations.
However, the mismatching between the algorithms with the problem could lead to
undesired results. In response to this challenge, we propose an unified
Vision<span class="highlight-title">GPT</span>-3D framework to consolidate the state-of-the-art vision models,
thereby facilitating the development of vision-oriented AI. Vision<span class="highlight-title">GPT</span>-3D
provides a versatile multimodal framework building upon the strengths of
multimodal foundation models. It seamlessly integrates various SOTA vision
models and brings the automation in the selection of SOTA vision models,
identifies the suitable 3D mesh creation algorithms corresponding to 2D depth
maps analysis, generates optimal results based on diverse multimodal inputs
such as text <span class="highlight-title">prompt</span>s.
  Keywords: Vision<span class="highlight-title">GPT</span>-3D, 3D vision understanding, Multimodal agent
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, pending conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SyncTweedies: A General Generative Framework Based on Synchronized
  <span class="highlight-title">Diffusion</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple <span class="highlight-title">diffusion</span> processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple <span class="highlight-title">diffusion</span>
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://synctweedies.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary 3D scene understanding presents a significant challenge in
computer vision, withwide-ranging applications in embodied agents and augmented
reality systems. Previous approaches haveadopted Neural Radiance Fields (<span class="highlight-title">NeRF</span>s)
to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel
open-vocabulary scene understanding approach based on 3D Gaussian Splatting.
Our keyidea is distilling <span class="highlight-title">pre-train</span>ed 2D semantics into 3D Gaussians. We design
a versatile projection approachthat maps various 2Dsemantic features from
<span class="highlight-title">pre-train</span>ed image encoders into a novel semantic component of 3D Gaussians,
withoutthe additional training required by <span class="highlight-title">NeRF</span>s. We further build a 3D
semantic network that directly predictsthe semantic component from raw 3D
Gaussians for fast inference. We explore several applications ofSemantic
Gaussians: semantic segmentation on ScanNet-20, where our approach attains a
4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene
understanding counterparts; object part segmentation,sceneediting, and
spatial-temporal segmentation with better qualitative results over 2D and 3D
baselines,highlighting its versatility and effectiveness on supporting diverse
downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://semantic-gaussians.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
results while advancing real-time <span class="highlight-title">rendering</span> performance. However, it relies
heavily on the quality of the initial point cloud, resulting in blurring and
needle-like artifacts in areas with insufficient initializing points. This is
mainly attributed to the point cloud growth condition in 3DGS that only
considers the average gradient magnitude of points from observable views,
thereby failing to grow for large Gaussians that are observable for many
viewpoints while many of them are only covered in the boundaries. To this end,
we propose a novel method, named Pixel-GS, to take into account the number of
pixels covered by the Gaussian in each view during the computation of the
growth condition. We regard the covered pixel numbers as the weights to
dynamically average the gradients from different views, such that the growth of
large Gaussians can be <span class="highlight-title">prompt</span>ed. As a result, points within the areas with
insufficient initializing points can be grown more effectively, leading to a
more accurate and detailed reconstruction. In addition, we propose a simple yet
effective strategy to scale the gradient field according to the distance to the
camera, to suppress the growth of floaters near the camera. Extensive
experiments both qualitatively and quantitatively demonstrate that our method
achieves state-of-the-art <span class="highlight-title">rendering</span> quality while maintaining real-time
<span class="highlight-title">rendering</span> speed, on the challenging Mip-<span class="highlight-title">NeRF</span> 360 and Tanks & Temples <span class="highlight-title">dataset</span>s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancements in deep learning for camera relocalization tasks,
obtaining ground truth pose labels required for the training process remains a
costly endeavor. While current weakly supervised methods excel in lightweight
label generation, their performance notably declines in scenarios with sparse
views. In response to this challenge, we introduce WSCLoc, a system capable of
being customized to various deep learning-based relocalization models to
enhance their performance under weakly-supervised and sparse view conditions.
This is realized with two stages. In the initial stage, WSCLoc employs a
multilayer perceptron-based structure called WFT-<span class="highlight-title">NeRF</span> to co-optimize image
reconstruction quality and initial pose information. To ensure a stable
learning process, we incorporate temporal information as input. Furthermore,
instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to
explicitly enforce a scale constraint. In the second stage, we co-optimize the
<span class="highlight-title">pre-train</span>ed WFT-<span class="highlight-title">NeRF</span> and WFT-Pose. This optimization is enhanced by
Time-Encoding based Random View Synthesis and supervised by inter-frame
geometric constraints that consider pose, depth, and RGB information. We
validate our approaches on two publicly available <span class="highlight-title">dataset</span>s, one outdoor and one
indoor. Our experimental results demonstrate that our weakly-supervised
relocalization solutions achieve superior pose estimation accuracy in
sparse-view scenarios, comparable to state-of-the-art camera relocalization
methods. We will make our code publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-<span class="highlight-title">NeRF</span>: Multi-Camera Neural Radiance Fields for Multi-Camera Image
  Acquisition Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Gao, Lutong Su, Hao Liang, Yufeng Yue, Yi Yang, Mengyin Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) use multi-view images for 3D scene
representation, demonstrating remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
<span class="highlight-title">NeRF</span>-based methods assume a unique camera and rarely consider multi-camera
scenarios. Besides, some <span class="highlight-title">NeRF</span> methods that can optimize intrinsic and extrinsic
parameters still remain susceptible to suboptimal solutions when these
parameters are poor initialized. In this paper, we propose MC-<span class="highlight-title">NeRF</span>, a method
that enables joint optimization of both intrinsic and extrinsic parameters
alongside <span class="highlight-title">NeRF</span>. The method also supports each image corresponding to
independent camera parameters. First, we tackle coupling issue and the
degenerate case that arise from the joint optimization between intrinsic and
extrinsic parameters. Second, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Finally, we present an end-to-end
network with training sequence that enables the estimation of intrinsic and
extrinsic parameters, along with the <span class="highlight-title">rendering</span> network. Furthermore,
recognizing that most existing <span class="highlight-title">dataset</span>s are designed for a unique camera, we
construct a real multi-camera image acquisition system and create a
corresponding new <span class="highlight-title">dataset</span>, which includes both simulated data and real-world
captured images. Experiments confirm the effectiveness of our method when each
image corresponds to different camera parameters. Specifically, we use
multi-cameras, each with different intrinsic and extrinsic parameters in
real-world system, to achieve 3D scene representation without providing initial
poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is currently under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic
  Range Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinau K. Venkataramanan, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) videos are able to represent wider ranges of
contrasts and colors than Standard Dynamic Range (SDR) videos, giving more
vivid experiences. Due to this, HDR videos are expected to grow into the
dominant video modality of the future. However, HDR videos are incompatible
with existing SDR displays, which form the majority of affordable consumer
displays on the market. Because of this, HDR videos must be processed by
tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited
video consumers. Here, we analyze the impact of tone-mapping operators on the
visual quality of streaming HDR videos. To this end, we built the first
large-scale subjectively annotated open-source database of compressed
tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40
unique HDR source contents. The videos in the database were labeled with more
than 750,000 subjective quality annotations, collected from more than 1,600
unique human observers. We demonstrate the usefulness of the new subjective
database by benchmarking objective models of visual quality on it. We envision
that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant
progress on HDR video tone mapping and quality assessment in the future. To
this end, we make the database freely available to the community at
https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with <span class="highlight-title">Diffusion</span> Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Diffusion</span> models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of <span class="highlight-title">diffusion</span> models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated <span class="highlight-title">diffusion</span> model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel <span class="highlight-title">diffusion</span>-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via <span class="highlight-title">diffusion</span> purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on <span class="highlight-title">NeRF</span>s, they face a potential
weakness that the instance/semantic embedding features derive from independent
<span class="highlight-title">MLP</span>s, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation <span class="highlight-title">rendering</span>
is directly integrated with the volume <span class="highlight-title">rendering</span> of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from <span class="highlight-title">pre-train</span>ed models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
<span class="highlight-title">dataset</span>s show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sm0kywu.github.io/ClusteringSDF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance
  Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-centric 3D environment understanding is both vital and challenging for
autonomous driving systems. Recently, object-free methods have attracted
considerable attention. Such methods perceive the world by predicting the
semantics of discrete voxel grids but fail to construct continuous and accurate
obstacle surfaces. To this end, in this paper, we propose SurroundSDF to
implicitly predict the signed distance field (SDF) and semantic field for the
continuous perception from surround images. Specifically, we introduce a
query-based approach and utilize SDF constrained by the Eikonal formulation to
accurately describe the surfaces of obstacles. Furthermore, considering the
absence of precise SDF ground truth, we propose a novel weakly supervised
paradigm for SDF, referred to as the Sandwich Eikonal formulation, which
emphasizes applying correct and dense constraints on both sides of the surface,
thereby enhancing the perceptual accuracy of the surface. Experiments suggest
that our method achieves SOTA for both occupancy prediction and 3D scene
reconstruction tasks on the nuScenes <span class="highlight-title">dataset</span>.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface Reconstruction from Point Clouds via Grid-based Intersection
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Tian, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface reconstruction from point clouds is a crucial task in the fields of
computer vision and computer graphics. SDF-based methods excel at
reconstructing smooth meshes with minimal error and artifacts but struggle with
representing open surfaces. On the other hand, UDF-based methods can
effectively represent open surfaces but often introduce noise near the surface,
leading to artifacts in the mesh. In this work, we propose a novel approach
that directly predicts the intersection points between sampled line segments of
point pairs and implicit surfaces. This method not only preserves the ability
to represent open surfaces but also eliminates artifacts in the mesh. Our
approach demonstrates state-of-the-art performance on three <span class="highlight-title">dataset</span>s: ShapeNet,
MGN, and ScanNet. The code will be made available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Frosting: Editable Complex Radiance Fields with Real-Time
  <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Guédon, Vincent Lepetit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Gaussian Frosting, a novel mesh-based representation for
high-quality <span class="highlight-title">rendering</span> and editing of complex 3D effects in real-time. Our
approach builds on the recent 3D Gaussian Splatting framework, which optimizes
a set of 3D Gaussians to approximate a radiance field from images. We propose
first extracting a base mesh from Gaussians during optimization, then building
and refining an adaptive layer of Gaussians with a variable thickness around
the mesh to better capture the fine details and volumetric effects near the
surface, such as hair or grass. We call this layer Gaussian Frosting, as it
resembles a coating of frosting on a cake. The fuzzier the material, the
thicker the frosting. We also introduce a parameterization of the Gaussians to
enforce them to stay inside the frosting layer and automatically adjust their
parameters when deforming, rescaling, editing or animating the mesh. Our
representation allows for efficient <span class="highlight-title">rendering</span> using Gaussian splatting, as well
as editing and animation by modifying the base mesh. We demonstrate the
effectiveness of our method on various synthetic and real scenes, and show that
it outperforms existing surface-based approaches. We will release our code and
a web-based viewer as additional contributions. Our project page is the
following: https://anttwo.github.io/frosting/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://anttwo.github.io/frosting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inf<span class="highlight-title">NeRF</span>: Towards Infinite Scale <span class="highlight-title">NeRF</span> <span class="highlight-title">Rendering</span> with O(log n) Space
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabin Liang, Lanqing Zhang, Zhuoran Zhao, Xiangyu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
<span class="highlight-title">rendering</span> with a space complexity of O(log n). This constrained data
requirement not only enhances <span class="highlight-title">rendering</span> efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(<span class="highlight-title">NeRF</span>) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a <span class="highlight-title">rendering</span> space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using <span class="highlight-title">NeRF</span> and octree structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface Reconstruction from Point Clouds via Grid-based Intersection
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Tian, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface reconstruction from point clouds is a crucial task in the fields of
computer vision and computer graphics. SDF-based methods excel at
reconstructing smooth meshes with minimal error and artifacts but struggle with
representing open surfaces. On the other hand, UDF-based methods can
effectively represent open surfaces but often introduce noise near the surface,
leading to artifacts in the mesh. In this work, we propose a novel approach
that directly predicts the intersection points between sampled line segments of
point pairs and implicit surfaces. This method not only preserves the ability
to represent open surfaces but also eliminates artifacts in the mesh. Our
approach demonstrates state-of-the-art performance on three <span class="highlight-title">dataset</span>s: ShapeNet,
MGN, and ScanNet. The code will be made available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ins-HOI: Instance Aware Human-Object Interactions Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Yuxiang Zhang, Hongwen Zhang, Xiao Zhou, Boyao Zhou, Ruizhi Shao, Zonghai Hu, Yebin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately modeling detailed interactions between human/hand and object is an
appealing yet challenging task. Current multi-view capture systems are only
capable of reconstructing multiple subjects into a single, unified mesh, which
fails to model the states of each instance individually during interactions. To
address this, previous methods use template-based representations to track
human/hand and object. However, the quality of the reconstructions is limited
by the descriptive capabilities of the templates so that these methods are
inherently struggle with geometry details, pressing deformations and invisible
contact surfaces. In this work, we propose an end-to-end Instance-aware
Human-Object Interactions recovery (Ins-HOI) framework by introducing an
instance-level occupancy field representation. However, the real-captured data
is presented as a holistic mesh, unable to provide instance-level supervision.
To address this, we further propose a complementary training strategy that
leverages synthetic data to introduce instance-level shape priors, enabling the
disentanglement of occupancy fields for different instances. Specifically,
synthetic data, created by randomly combining individual scans of humans/hands
and objects, guides the network to learn a coarse prior of instances.
Meanwhile, real-captured data helps in learning the overall geometry and
restricting interpenetration in contact areas. As demonstrated in experiments,
our method Ins-HOI supports instance-level reconstruction and provides
reasonable and realistic invisible contact surfaces even in cases of extremely
close interaction. To facilitate the research of this task, we collect a
large-scale, high-fidelity 3D scan <span class="highlight-title">dataset</span>, including 5.2k high-quality scans
with real-world human-chair and hand-object interactions. The code and data
will be public for research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and
  Dataset Page: https://github.com/jiajunzhang16/ins-hoi</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspectral Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Cédric Pradalier, Yongsheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral Imagery (HSI) has been used in many applications to
non-destructively determine the material and/or chemical compositions of
samples. There is growing interest in creating 3D hyperspectral
reconstructions, which could provide both spatial and spectral information
while also mitigating common HSI challenges such as non-Lambertian surfaces and
translucent objects. However, traditional 3D reconstruction with HSI is
difficult due to technological limitations of hyperspectral cameras. In recent
years, Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have seen widespread success in creating
high quality volumetric 3D representations of scenes captured by a variety of
camera models. Leveraging recent advances in <span class="highlight-title">NeRF</span>s, we propose computing a
hyperspectral 3D reconstruction in which every point in space and view
direction is characterized by wavelength-dependent radiance and transmittance
spectra. To evaluate our approach, a <span class="highlight-title">dataset</span> containing nearly 2000
hyperspectral images across 8 scenes and 2 cameras was collected. We perform
comparisons against traditional RGB <span class="highlight-title">NeRF</span> baselines and apply ablation testing
with alternative spectra representations. Finally, we demonstrate the potential
of hyperspectral <span class="highlight-title">NeRF</span>s for hyperspectral super-resolution and imaging sensor
simulation. We show that our hyperspectral <span class="highlight-title">NeRF</span> approach enables creating fast,
accurate volumetric 3D hyperspectral scenes and enables several new
applications and areas for future study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6
  pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on <span class="highlight-title">NeRF</span>s, they face a potential
weakness that the instance/semantic embedding features derive from independent
<span class="highlight-title">MLP</span>s, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation <span class="highlight-title">rendering</span>
is directly integrated with the volume <span class="highlight-title">rendering</span> of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from <span class="highlight-title">pre-train</span>ed models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
<span class="highlight-title">dataset</span>s show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sm0kywu.github.io/ClusteringSDF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combi<span class="highlight-title">NeRF</span>: A Combination of Regularization Techniques for Few-Shot
  Neural Radiance Field View Synthesis <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have shown impressive results for novel view
synthesis when a sufficiently large amount of views are available. When dealing
with few-shot settings, i.e. with a small set of input views, the training
could overfit those views, leading to artifacts and geometric and chromatic
inconsistencies in the resulting <span class="highlight-title">rendering</span>. Regularization is a valid solution
that helps <span class="highlight-title">NeRF</span> generalization. On the other hand, each of the most recent <span class="highlight-title">NeRF</span>
regularization techniques aim to mitigate a specific <span class="highlight-title">rendering</span> problem.
Starting from this observation, in this paper we propose Combi<span class="highlight-title">NeRF</span>, a framework
that synergically combines several regularization techniques, some of them
novel, in order to unify the benefits of each. In particular, we regularize
single and neighboring rays distributions and we add a smoothness term to
regularize near geometries. After these geometric approaches, we propose to
exploit Lipschitz regularization to both <span class="highlight-title">NeRF</span> density and color networks and to
use encoding masks for input features regularization. We show that Combi<span class="highlight-title">NeRF</span>
outperforms the state-of-the-art methods with few-shot settings in several
publicly available <span class="highlight-title">dataset</span>s. We also present an ablation study on the LLFF and
<span class="highlight-title">NeRF</span>-Synthetic <span class="highlight-title">dataset</span>s that support the choices made. We release with this
paper the open-source implementation of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication at the 2024
  International Conference on 3D Vision (3DV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inf<span class="highlight-title">NeRF</span>: Towards Infinite Scale <span class="highlight-title">NeRF</span> <span class="highlight-title">Rendering</span> with O(log n) Space
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabin Liang, Lanqing Zhang, Zhuoran Zhao, Xiangyu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
<span class="highlight-title">rendering</span> with a space complexity of O(log n). This constrained data
requirement not only enhances <span class="highlight-title">rendering</span> efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(<span class="highlight-title">NeRF</span>) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a <span class="highlight-title">rendering</span> space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using <span class="highlight-title">NeRF</span> and octree structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Thermal Modality to Enhance Reconstruction in Low-Light
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>) accomplishes photo-realistic novel view
synthesis by learning the implicit volumetric representation of a scene from
multi-view images, which faithfully convey the colorimetric information.
However, sensor noises will contaminate low-value pixel signals, and the lossy
camera image signal processor will further remove near-zero intensities in
extremely dark situations, deteriorating the synthesis performance. Existing
approaches reconstruct low-light scenes from raw images but struggle to recover
texture and boundary details in dark regions. Additionally, they are unsuitable
for high-speed models relying on explicit representations. To address these
issues, we present Thermal-<span class="highlight-title">NeRF</span>, which takes thermal and visible raw images as
inputs, considering the thermal camera is robust to the illumination variation
and raw images preserve any possible clues in the dark, to accomplish visible
and thermal view synthesis simultaneously. Also, the first multi-view thermal
and visible <span class="highlight-title">dataset</span> (MVTV) is established to support the research on multimodal
<span class="highlight-title">NeRF</span>. Thermal-<span class="highlight-title">NeRF</span> achieves the best trade-off between detail preservation and
noise smoothing and provides better synthesis performance than previous work.
Finally, we demonstrate that both modalities are beneficial to each other in 3D
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point<span class="highlight-title">NeRF</span>++: A multi-scale, point-based Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Sun, Eduard Trulls, Yang-Che Tseng, Sneha Sambandam, Gopal Sharma, Andrea Tagliasacchi, Kwang Moo Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds offer an attractive source of information to complement images
in neural scene representations, especially when few images are available.
Neural <span class="highlight-title">rendering</span> methods based on point clouds do exist, but they do not
perform well when the point cloud quality is low -- e.g., sparse or incomplete,
which is often the case with real-world data. We overcome these problems with a
simple representation that aggregates point clouds at multiple scale levels
with sparse voxel grids at different resolutions. To deal with point cloud
sparsity, we average across multiple scale levels -- but only among those that
are valid, i.e., that have enough neighboring points in proximity to the ray of
a pixel. To help model areas without points, we add a global voxel at the
coarsest scale, thus unifying ``classical'' and point-based <span class="highlight-title">NeRF</span> formulations.
We validate our method on the <span class="highlight-title">NeRF</span> Synthetic, ScanNet, and KITTI-360 <span class="highlight-title">dataset</span>s,
outperforming the state of the art, with a significant gap compared to other
<span class="highlight-title">NeRF</span>-based methods, especially on more challenging scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://pointnerfpp.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Radiance Fields in Medical Imaging: Challenges and Next Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Shu Hu, Heng Fan, Hongtu Zhu, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>), as a pioneering technique in computer vision,
offer great potential to revolutionize medical imaging by synthesizing
three-dimensional representations from the projected two-dimensional image
data. However, they face unique challenges when applied to medical
applications. This paper presents a comprehensive examination of applications
of <span class="highlight-title">NeRF</span>s in medical imaging, highlighting four imminent challenges, including
fundamental imaging principles, inner structure requirement, object boundary
definition, and color density significance. We discuss current methods on
different organs and discuss related limitations. We also <span class="highlight-title">review</span> several
<span class="highlight-title">dataset</span>s and evaluation metrics and propose several promising directions for
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Colon<span class="highlight-title">NeRF</span>: High-Fidelity Neural Reconstruction of Long Colonoscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Shi, Beijia Lu, Jia-Wei Liu, Ming Li, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.
However, accurate long-sequence colonoscopy reconstruction faces three major
challenges: (1) dissimilarity among segments of the colon due to its meandering
and convoluted shape; (2) co-existence of simple and intricately folded
geometry structures; (3) sparse viewpoints due to constrained camera
trajectories. To tackle these challenges, we introduce a new reconstruction
framework based on neural radiance field (<span class="highlight-title">NeRF</span>), named Colon<span class="highlight-title">NeRF</span>, which
leverages neural <span class="highlight-title">rendering</span> for novel view synthesis of long-sequence
colonoscopy. Specifically, to reconstruct the entire colon in a piecewise
manner, our Colon<span class="highlight-title">NeRF</span> introduces a region division and integration module,
effectively reducing shape dissimilarity and ensuring geometric consistency in
each segment. To learn both the simple and complex geometry in a unified
framework, our Colon<span class="highlight-title">NeRF</span> incorporates a multi-level fusion module that
progressively models the colon regions from easy to hard. Additionally, to
overcome the challenges from sparse views, we devise a DensiNet module for
densifying camera poses under the guidance of semantic consistency. We conduct
extensive experiments on both synthetic and real-world <span class="highlight-title">dataset</span>s to evaluate our
Colon<span class="highlight-title">NeRF</span>. Quantitatively, Colon<span class="highlight-title">NeRF</span> exhibits a 67%-85% increase in LPIPS-ALEX
scores. Qualitatively, our reconstruction visualizations show much clearer
textures and more accurate geometric details. These sufficiently demonstrate
our superior performance over the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for Project Page, see https://showlab.github.io/ColonNeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ED-<span class="highlight-title">NeRF</span>: Efficient Text-Guided Editing of 3D Scene with Latent Space
  <span class="highlight-title">NeRF</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jangho Park, Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant advancement in text-to-image <span class="highlight-title">diffusion</span>
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into <span class="highlight-title">NeRF</span> editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing <span class="highlight-title">NeRF</span> editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D <span class="highlight-title">NeRF</span> editing approach dubbed ED-<span class="highlight-title">NeRF</span> by successfully embedding
real-world scenes into the latent space of the latent <span class="highlight-title">diffusion</span> model (LDM)
through a unique refinement layer. This approach enables us to obtain a <span class="highlight-title">NeRF</span>
backbone that is not only faster but also more amenable to editing compared to
traditional image space <span class="highlight-title">NeRF</span> editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-<span class="highlight-title">NeRF</span> achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024; Project Page: https://jhq1234.github.io/ed-nerf.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning for Large-Scale Scene Modeling with Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06030v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06030v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teppei Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (<span class="highlight-title">NeRF</span>) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by <span class="highlight-title">NeRF</span> has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with <span class="highlight-title">NeRF</span>. We
tailor the model aggregation pipeline in federated learning for <span class="highlight-title">NeRF</span>, thereby
allowing local updates of <span class="highlight-title">NeRF</span>. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene <span class="highlight-title">dataset</span>, Mill19.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our subsequent work is available at arXiv:2403.11460</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Cheng, Guanzhou Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the use of panoramic 3D estimation in lighting
simulation. Conventional lighting simulation necessitates detailed modeling as
input, resulting in significant labor effort and time cost. The 3D layout
estimation method directly takes a single panorama as input and generates a
lighting simulation model with room geometry and window aperture. We evaluate
the simulation results by comparing the luminance errors between on-site High
Dynamic Range (HDR) photographs, 3D estimation model, and detailed model in
panoramic representation and fisheye perspective. Given the selected scene, the
results demonstrate the estimated room layout is reliable for lighting
simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Annual Modeling and Simulation Conference (ANNSIM), May 20-23, 2024,
  Washington D.C., USA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the image acquisition process, various forms of degradation, including
noise, haze, and rain, are frequently introduced. These degradations typically
arise from the inherent limitations of cameras or unfavorable ambient
conditions. To recover clean images from degraded versions, numerous
specialized restoration methods have been developed, each targeting a specific
type of degradation. Recently, all-in-one algorithms have garnered significant
attention by addressing different types of degradations within a single model
without requiring prior information of the input degradation type. However,
these methods purely operate in the spatial domain and do not delve into the
distinct frequency variations inherent to different degradation types. To
address this gap, we propose an adaptive all-in-one image restoration network
based on frequency mining and modulation. Our approach is motivated by the
observation that different degradation types impact the image content on
different frequency subbands, thereby requiring different treatments for each
restoration task. Specifically, we first mine low- and high-frequency
information from the input features, guided by the adaptively decoupled spectra
of the degraded image. The extracted features are then modulated by a
bidirectional operator to facilitate interactions between different frequency
components. Finally, the modulated features are merged into the original input
for a progressively guided restoration. With this approach, the model achieves
adaptive reconstruction by accentuating the informative frequency subbands
according to different input degradations. Extensive experiments demonstrate
that the proposed method achieves state-of-the-art performance on different
image restoration tasks, including denoising, dehazing, deraining, motion
deblurring, and low-light image enhancement. Our code is available at
https://github.com/c-yn/AdaIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages,15 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-Pixel2Mesh: Combining Global and Local <span class="highlight-title">Transformer</span> for 3D Mesh
  Generation from a Single Image <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Zhang, Boyan Jiang, Keke He, Junwei Zhu, Ying Tai, Chengjie Wang, Yinda Zhang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a
single color image through coarse-to-fine mesh deformation. Although P2M is
capable of generating plausible global shapes, its Graph Convolution Network
(GCN) often produces overly smooth results, causing the loss of fine-grained
geometry details. Moreover, P2M generates non-credible features for occluded
regions and struggles with the domain gap from synthetic data to real-world
images, which is a common challenge for single-view 3D reconstruction methods.
To address these challenges, we propose a novel <span class="highlight-title">Transformer</span>-boosted
architecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of
P2M. Specifically, we use a global <span class="highlight-title">Transformer</span> to control the holistic shape
and a local <span class="highlight-title">Transformer</span> to progressively refine the local geometry details with
graph-based point upsampling. To enhance real-world reconstruction, we present
the simple yet effective Linear Scale Search (LSS), which serves as <span class="highlight-title">prompt</span>
tuning during the input preprocessing. Our experiments on ShapeNet demonstrate
state-of-the-art performance, while results on real-world data show the
generalization capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Received by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Skeletons: Integrative Latent Mapping for Coherent 4D Sequence
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitong Yang, Mingtao Feng, Zijie Wu, Shijie Sun, Weisheng Dong, Yaonan Wang, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly learning to model 4D content, including shape, color and motion, is
challenging. Existing methods depend on skeleton-based motion control and offer
limited continuity in detail. To address this, we propose a novel framework
that generates coherent 4D sequences with animation of 3D shapes under given
conditions with dynamic evolution of shape and color over time through
integrative latent mapping. We first employ an integrative latent unified
representation to encode shape and color information of each detailed 3D
geometry frame. The proposed skeleton-free latent 4D sequence joint
representation allows us to leverage <span class="highlight-title">diffusion</span> models in a low-dimensional
space to control the generation of 4D sequences. Finally, temporally coherent
4D sequences are generated conforming well to the input images and text
<span class="highlight-title">prompt</span>s. Extensive experiments on the ShapeNet, 3DBiCar and DeformingThings4D
<span class="highlight-title">dataset</span>s for several tasks demonstrate that our method effectively learns to
generate quality 3D shapes with color and 4D mesh animations, improving over
the current state-of-the-art. Source code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nellie: Automated organelle segmentation, tracking, and hierarchical
  feature extraction in 2D/3D live-cell microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin E. Y. T. Lefebvre, Gabriel Sturm, Ting-Yu Lin, Emily Stoops, Magdalena Preciado Lopez, Benjamin Kaufmann-Malaga, Kayley Hake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of dynamic organelles remains a formidable challenge, though key
to understanding biological processes. We introduce Nellie, an automated and
unbiased pipeline for segmentation, tracking, and feature extraction of diverse
intracellular structures. Nellie adapts to image metadata, eliminating user
input. Nellie's preprocessing pipeline enhances structural contrast on multiple
intracellular scales allowing for robust hierarchical segmentation of
sub-organellar regions. Internal motion capture markers are generated and
tracked via a radius-adaptive pattern matching scheme, and used as guides for
sub-voxel flow interpolation. Nellie extracts a plethora of features at
multiple hierarchical levels for deep and customizable analysis. Nellie
features a Napari-based GUI that allows for code-free operation and
visualization, while its modular open-source codebase invites customization by
experienced users. We demonstrate Nellie's wide variety of use cases with two
examples: unmixing multiple organelles from a single channel using
feature-based classification and training an unsupervised graph autoencoder on
mitochondrial multi-mesh graphs to quantify latent space embedding changes
following ionomycin treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for associated code, see https://github.com/aelefebv/nellie; 82
  pages, 5 main figures, 11 extended figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Fu, Zehao Wen, Zichen Liu, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by cognitive theories, we introduce AnyHome, a framework that
translates any text into well-structured and textured indoor scenes at a
house-scale. By <span class="highlight-title">prompt</span>ing Large Language Models (LLMs) with designed templates,
our approach converts provided textual narratives into amodal structured
representations. These representations guarantee consistent and realistic
spatial layouts by directing the synthesis of a geometry mesh within defined
constraints. A Score Distillation Sampling process is then employed to refine
the geometry, followed by an egocentric inpainting process that adds lifelike
textures to it. AnyHome stands out with its editability, customizability,
diversity, and realism. The structured representations for scenes allow for
extensive editing at varying levels of granularity. Capable of interpreting
texts ranging from simple labels to detailed narratives, AnyHome generates
detailed geometries and textures that outperform existing methods in both
quantitative and qualitative measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via
  Comparing and Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method named iComMa to address the 6D camera pose estimation
problem in computer vision. Conventional pose estimation methods typically rely
on the target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods have achieved promising
results in mesh-free object and scene pose estimation by inverting the Neural
Radiance Fields (<span class="highlight-title">NeRF</span>). However, they still struggle with adverse
initializations such as large rotations and translations. To address this
issue, we propose an efficient method for accurate camera pose estimation by
inverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based
differentiable framework optimizes camera pose by minimizing the residual
between the query image and the rendered image, requiring no training. An
end-to-end matching module is designed to enhance the model's robustness
against adverse initializations, while minimizing pixel-level comparing loss
aids in precise pose estimation. Experimental results on synthetic and complex
real-world data demonstrate the effectiveness of the proposed approach in
challenging conditions and the accuracy of camera pose estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Mesh Recovery from Arbitrary Multi-view Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoben Li, Mancheng Meng, Ziyan Wu, Terrence Chen, Fan Yang, Dinggang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery from arbitrary multi-view images involves two
characteristics: the arbitrary camera poses and arbitrary number of camera
views. Because of the variability, designing a unified framework to tackle this
task is challenging. The challenges can be summarized as the dilemma of being
able to simultaneously estimate arbitrary camera poses and recover human mesh
from arbitrary multi-view images while maintaining flexibility. To solve this
dilemma, we propose a divide and conquer framework for Unified Human Mesh
Recovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR
consists of a decoupled structure and two main components: camera and body
decoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion
(AVF). As camera poses and human body mesh are independent of each other, CBD
splits the estimation of them into two sub-tasks for two individual
sub-networks (\ie, CPE and AVF) to handle respectively, thus the two sub-tasks
are disentangled. In CPE, since each camera pose is unrelated to the others, we
adopt a shared <span class="highlight-title">MLP</span> to process all views in a parallel way. In AVF, in order to
fuse multi-view information and make the fusion operation independent of the
number of views, we introduce a <span class="highlight-title">transformer</span> decoder with a SMPL parameters
query token to extract cross-view features for mesh recovery. To demonstrate
the efficacy and flexibility of the proposed framework and effect of each
component, we conduct extensive experiments on three public <span class="highlight-title">dataset</span>s:
Human3.6M, MPI-INF-3DHP, and TotalCapture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoScaler: Geometry and <span class="highlight-title">Rendering</span>-Aware Downsampling of 3D Mesh Textures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Karthikey Pentapati, Anshul Rai, Arkady Ten, Chaitanya Atluru, Alan Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution texture maps are necessary for representing real-world
objects accurately with 3D meshes. The large sizes of textures can bottleneck
the real-time <span class="highlight-title">rendering</span> of high-quality virtual 3D scenes on devices having low
computational budgets and limited memory. Downsampling the texture maps
directly addresses the issue, albeit at the cost of visual fidelity.
Traditionally, downsampling of texture maps is performed using methods like
bicubic interpolation and the Lanczos algorithm. These methods ignore the
geometric layout of the mesh and its UV parametrization and also do not account
for the <span class="highlight-title">rendering</span> process used to obtain the final visualization that the users
will experience. Towards filling these gaps, we introduce GeoScaler, which is a
method of downsampling texture maps of 3D meshes while incorporating geometric
cues, and by maximizing the visual fidelity of the rendered views of the
textured meshes. We show that the textures generated by GeoScaler deliver
significantly better quality rendered images compared to those generated by
traditional downsampling methods
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via
  Comparing and Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method named iComMa to address the 6D camera pose estimation
problem in computer vision. Conventional pose estimation methods typically rely
on the target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods have achieved promising
results in mesh-free object and scene pose estimation by inverting the Neural
Radiance Fields (<span class="highlight-title">NeRF</span>). However, they still struggle with adverse
initializations such as large rotations and translations. To address this
issue, we propose an efficient method for accurate camera pose estimation by
inverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based
differentiable framework optimizes camera pose by minimizing the residual
between the query image and the rendered image, requiring no training. An
end-to-end matching module is designed to enhance the model's robustness
against adverse initializations, while minimizing pixel-level comparing loss
aids in precise pose estimation. Experimental results on synthetic and complex
real-world data demonstrate the effectiveness of the proposed approach in
challenging conditions and the accuracy of camera pose estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints
  Scene Coordinate Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (<span class="highlight-title">MLP</span>). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (<span class="highlight-title">NeRF</span>). By generating novel poses and
feeding them into a trained <span class="highlight-title">NeRF</span> model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple <span class="highlight-title">NeRF</span>s, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Perceptual Evaluation Framework for Lighting Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justine Giroux, Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Javier Vazquez-Corral, Jean-François Lalonde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in lighting estimation is tracked by computing existing image
quality assessment (IQA) metrics on images from standard <span class="highlight-title">dataset</span>s. While this
may appear to be a reasonable approach, we demonstrate that doing so does not
correlate to human preference when the estimated lighting is used to relight a
virtual scene into a real photograph. To study this, we design a controlled
psychophysical experiment where human observers must choose their preference
amongst rendered scenes lit using a set of lighting estimation algorithms
selected from the recent literature, and use it to analyse how these algorithms
perform according to human perception. Then, we demonstrate that none of the
most popular IQA metrics from the literature, taken individually, correctly
represent human perception. Finally, we show that by learning a combination of
existing IQA metrics, we can more accurately represent human preference. This
provides a new perceptual framework to help evaluate future lighting estimation
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoNQ: a Neural QEM-based Mesh Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although polygon meshes have been a standard representation in geometry
processing, their irregular and combinatorial nature hinders their suitability
for learning-based applications. In this work, we introduce a novel learnable
mesh representation through a set of local 3D sample Points and their
associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,
which we denote PoNQ. A global mesh is directly derived from PoNQ by
efficiently leveraging the knowledge of the local quadric errors. Besides
marking the first use of QEM within a neural shape representation, our
contribution guarantees both topological and geometrical properties by ensuring
that a PoNQ mesh does not self-intersect and is always the boundary of a
volume. Notably, our representation does not rely on a regular grid, is
supervised directly by the target surface alone, and also handles open surfaces
with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ
through a learning-based mesh prediction from SDF grids and show that our
method surpasses recent state-of-the-art techniques in terms of both surface
and edge-based metrics.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneScript: Reconstructing Scenes With An Autoregressive Structured
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, Jakob Engel, Edward Miller, Richard Newcombe, Vasileios Balntas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SceneScript, a method that directly produces full scene models
as a sequence of structured language commands using an autoregressive,
token-based approach. Our proposed scene representation is inspired by recent
successes in <span class="highlight-title">transformer</span>s & LLMs, and departs from more traditional methods
which commonly describe scenes as meshes, voxel grids, point clouds or radiance
fields. Our method infers the set of structured language commands directly from
encoded visual data using a scene language encoder-decoder architecture. To
train SceneScript, we generate and release a large-scale synthetic <span class="highlight-title">dataset</span>
called Aria Synthetic Environments consisting of 100k high-quality in-door
scenes, with photorealistic and ground-truth annotated renders of egocentric
scene walkthroughs. Our method gives state-of-the art results in architectural
layout estimation, and competitive results in 3D object detection. Lastly, we
explore an advantage for SceneScript, which is the ability to readily adapt to
new commands via simple additions to the structured language, which we
illustrate for tasks such as coarse 3D object part reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>see project page, https://projectaria.com/scenescript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoNQ: a Neural QEM-based Mesh Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although polygon meshes have been a standard representation in geometry
processing, their irregular and combinatorial nature hinders their suitability
for learning-based applications. In this work, we introduce a novel learnable
mesh representation through a set of local 3D sample Points and their
associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape,
which we denote PoNQ. A global mesh is directly derived from PoNQ by
efficiently leveraging the knowledge of the local quadric errors. Besides
marking the first use of QEM within a neural shape representation, our
contribution guarantees both topological and geometrical properties by ensuring
that a PoNQ mesh does not self-intersect and is always the boundary of a
volume. Notably, our representation does not rely on a regular grid, is
supervised directly by the target surface alone, and also handles open surfaces
with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ
through a learning-based mesh prediction from SDF grids and show that our
method surpasses recent state-of-the-art techniques in terms of both surface
and edge-based metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PostoMETRO: Pose Token Enhanced Mesh <span class="highlight-title">Transformer</span> for Robust 3D Human
  Mesh Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Yang, Zihang Jiang, Shang Zhao, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advancements in single-image-based human mesh recovery, there
is a growing interest in enhancing its performance in certain extreme
scenarios, such as occlusion, while maintaining overall model accuracy.
Although obtaining accurately annotated 3D human poses under occlusion is
challenging, there is still a wealth of rich and precise 2D pose annotations
that can be leveraged. However, existing works mostly focus on directly
leveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, we
present PostoMETRO($\textbf{Pos}$e $\textbf{to}$ken enhanced $\textbf{ME}$sh
$\textbf{TR}$ansf$\textbf{O}$rmer), which integrates occlusion-resilient 2D
pose representation into <span class="highlight-title">transformer</span>s in a token-wise manner. Utilizing a
specialized pose tokenizer, we efficiently condense 2D pose data to a compact
sequence of pose tokens and feed them to the <span class="highlight-title">transformer</span> together with the
image tokens. This process not only ensures a rich depiction of texture from
the image but also fosters a robust integration of pose and image information.
Subsequently, these combined tokens are queried by vertex and joint tokens to
decode 3D coordinates of mesh vertices and human joints. Facilitated by the
robust pose token representation and the effective combination, we are able to
produce more precise 3D coordinates, even under extreme scenarios like
occlusion. Experiments on both standard and occlusion-specific benchmarks
demonstrate the effectiveness of PostoMETRO. Qualitative results further
illustrate the clarity of how 2D pose can help 3D reconstruction. Code will be
made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Sinha, Jeremy Kawahara, Arezou Pakzad, Kumar Abhishek, Matthieu Ruthven, Enjie Ghorbel, Anis Kacem, Djamila Aouada, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning (DL) has shown great potential in the field of
dermatological image analysis. However, existing <span class="highlight-title">dataset</span>s in this domain have
significant limitations, including a small number of image samples, limited
disease conditions, insufficient annotations, and non-standardized image
acquisitions. To address these shortcomings, we propose a novel framework
called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured
meshes of human subjects using a differentiable renderer and generates 2D
images from various camera viewpoints under chosen lighting conditions in
diverse background scenes. Our method adheres to top-down rules that constrain
the blending and <span class="highlight-title">rendering</span> process to create 2D images with skin conditions
that mimic in-the-wild acquisitions, ensuring more meaningful results. The
framework generates photo-realistic 2D dermoscopy images and the corresponding
dense annotations for semantic segmentation of the skin, skin conditions, body
parts, bounding boxes around lesions, depth maps, and other 3D scene
parameters, such as camera position and lighting conditions. DermSynth3D allows
for the creation of custom <span class="highlight-title">dataset</span>s for various dermatology tasks. We
demonstrate the effectiveness of data generated using DermSynth3D by training
DL models on synthetic data and evaluating them on various dermatology tasks
using real 2D dermatological images. We make our code publicly available at
https://github.com/sfu-mial/DermSynth3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Medical Image Analysis (MedIA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generic 3D <span class="highlight-title">Diffusion</span> Adapter Using Controlled Multi-View Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain 3D object synthesis has been lagging behind image synthesis due
to limited data and higher computational complexity. To bridge this gap, recent
works have investigated multi-view <span class="highlight-title">diffusion</span> but often fall short in either 3D
consistency, visual quality, or efficiency. This paper proposes MVEdit, which
functions as a 3D counterpart of SDEdit, employing ancestral sampling to
jointly denoise multi-view images and output high-quality textured meshes.
Built on off-the-shelf 2D <span class="highlight-title">diffusion</span> models, MVEdit achieves 3D consistency
through a training-free 3D Adapter, which lifts the 2D views of the last
timestep into a coherent 3D representation, then conditions the 2D views of the
next timestep using rendered views, without uncompromising visual quality. With
an inference time of only 2-5 minutes, this framework achieves better trade-off
between quality and speed than score distillation. MVEdit is highly versatile
and extendable, with a wide range of applications including text/image-to-3D
generation, 3D-to-3D editing, and high-quality texture synthesis. In
particular, evaluations demonstrate state-of-the-art performance in both
image-to-3D and text-guided texture generation tasks. Additionally, we
introduce a method for fine-tuning 2D latent <span class="highlight-title">diffusion</span> models on small 3D
<span class="highlight-title">dataset</span>s with limited resources, enabling fast low-resolution text-to-3D
initialization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2 note: Fix missing acknowledgements. Project page:
  https://lakonik.github.io/mvedit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuSHRoom: Multi-Sensor Hybrid Room <span class="highlight-title">Dataset</span> for Joint 3D Reconstruction
  and Novel View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuqian Ren, Wenjia Wang, Dingding Cai, Tuuli Tuominen, Juho Kannala, Esa Rahtu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse technologies demand accurate, real-time, and immersive modeling on
consumer-grade hardware for both non-human perception (e.g.,
drone/robot/autonomous car navigation) and immersive technologies like AR/VR,
requiring both structural accuracy and photorealism. However, there exists a
knowledge gap in how to apply geometric reconstruction and photorealism
modeling (novel view synthesis) in a unified framework. To address this gap and
promote the development of robust and immersive modeling and <span class="highlight-title">rendering</span> with
consumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room
<span class="highlight-title">Dataset</span> (MuSHRoom). Our <span class="highlight-title">dataset</span> presents exciting challenges and requires
state-of-the-art methods to be cost-effective, robust to noisy data and
devices, and can jointly learn 3D reconstruction and novel view synthesis
instead of treating them as separate tasks, making them ideal for real-world
applications. We benchmark several famous pipelines on our <span class="highlight-title">dataset</span> for joint 3D
mesh reconstruction and novel view synthesis. Our <span class="highlight-title">dataset</span> and benchmark show
great potential in promoting the improvements for fusing 3D reconstruction and
high-quality <span class="highlight-title">rendering</span> in a robust and computationally efficient end-to-end
fashion. The <span class="highlight-title">dataset</span> and code are available at the project website:
https://xuqianren.github.io/publications/MuSHRoom/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Text-to-3D Generation via Surface-Aligned Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing <span class="highlight-title">pre-train</span>ed multi-view <span class="highlight-title">diffusion</span>
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base <span class="highlight-title">diffusion</span> model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D <span class="highlight-title">diffusion</span> guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid <span class="highlight-title">diffusion</span> guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lizhiqi49.github.io/MVControl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape
  Generative Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Yang, Xiangru Huang, Bo Sun, Chandrajit Bajaj, Qixing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GenCorres, a novel unsupervised joint shape matching
(JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized
deformable shape collection while constraining deformations between adjacent
synthetic shapes to preserve geometric structures such as local rigidity and
local conformality. GenCorres presents three appealing advantages over existing
JSM techniques. First, GenCorres performs JSM among a synthetic shape
collection whose size is much bigger than the input shapes and fully leverages
the datadriven power of JSM. Second, GenCorres unifies consistent shape
matching and pairwise matching (i.e., by enforcing deformation priors between
adjacent synthetic shapes). Third, the generator provides a concise encoding of
consistent shape correspondences. However, learning a mesh generator from an
unorganized shape collection is challenging, requiring a good initialization.
GenCorres addresses this issue by learning an implicit generator from the input
shapes, which provides intermediate shapes between two arbitrary shapes. We
introduce a novel approach for computing correspondences between adjacent
implicit surfaces, which we use to regularize the implicit generator. Synthetic
shapes of the implicit generator then guide initial fittings (i.e., via
template-based deformation) for learning the mesh generator. Experimental
results show that GenCorres considerably outperforms state-of-the-art JSM
techniques. The synthetic shapes of GenCorres also achieve salient performance
gains against state-of-the-art deformable shape generators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        NeRF <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth-guided <span class="highlight-title">NeRF</span> Training via Earth Mover's Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) are trained to minimize the <span class="highlight-title">rendering</span> loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during <span class="highlight-title">NeRF</span> training, leveraging dense predictions from <span class="highlight-title">pre-train</span>ed depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for <span class="highlight-title">NeRF</span> supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf <span class="highlight-title">pretrain</span>ed <span class="highlight-title">diffusion</span> models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided <span class="highlight-title">NeRF</span> outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decent<span class="highlight-title">NeRF</span>s: Decentralized Neural Radiance Fields from Crowdsourced
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaid Tasneem, Akshat Dave, Abhishek Singh, Kushagra Tiwary, Praneeth Vepakomma, Ashok Veeraraghavan, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (<span class="highlight-title">NeRF</span>s) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, Decent<span class="highlight-title">NeRF</span>, is the first attempt at
decentralized, crowd-sourced <span class="highlight-title">NeRF</span>s that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized <span class="highlight-title">NeRF</span>s between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global <span class="highlight-title">NeRF</span>s and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn <span class="highlight-title">NeRF</span>s with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism <span class="highlight-title">dataset</span>s. We further analyze how secure aggregation of
global <span class="highlight-title">NeRF</span>s in Decent<span class="highlight-title">NeRF</span> minimizes the undesired reconstruction of personal
content by the server.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global-guided Focal Neural Radiance Field for Large-scale Scene
  <span class="highlight-title">Rendering</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields~(<span class="highlight-title">NeRF</span>) have recently been applied to render
large-scale scenes. However, their limited model capacity typically results in
blurred <span class="highlight-title">rendering</span> results. Existing large-scale <span class="highlight-title">NeRF</span>s primarily address this
limitation by partitioning the scene into blocks, which are subsequently
handled by separate sub-<span class="highlight-title">NeRF</span>s. These sub-<span class="highlight-title">NeRF</span>s, trained from scratch and
processed independently, lead to inconsistencies in geometry and appearance
across the scene. Consequently, the <span class="highlight-title">rendering</span> quality fails to exhibit
significant improvement despite the expansion of model capacity. In this work,
we present global-guided focal neural radiance field (GF-<span class="highlight-title">NeRF</span>) that achieves
high-fidelity <span class="highlight-title">rendering</span> of large-scale scenes. Our proposed GF-<span class="highlight-title">NeRF</span> utilizes a
two-stage (Global and Focal) architecture and a global-guided training
strategy. The global stage obtains a continuous representation of the entire
scene while the focal stage decomposes the scene into multiple blocks and
further processes them with distinct sub-encoders. Leveraging this two-stage
architecture, sub-encoders only need fine-tuning based on the global encoder,
thus reducing training complexity in the focal stage while maintaining
scene-wide consistency. Spatial information and error information from the
global stage also benefit the sub-encoders to focus on crucial areas and
effectively capture more details of large-scale scenes. Notably, our approach
does not rely on any prior knowledge about the target scene, attributing
GF-<span class="highlight-title">NeRF</span> adaptable to various large-scale scene types, including street-view and
aerial-view scenes. We demonstrate that our method achieves high-fidelity,
natural <span class="highlight-title">rendering</span> results on various types of large-scale <span class="highlight-title">dataset</span>s. Our project
page: https://shaomq2187.github.io/GF-<span class="highlight-title">NeRF</span>/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Neural Volumetric Pose Features for Camera Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel neural volumetric pose feature, termed PoseMap, designed
to enhance camera localization by encapsulating the information between images
and the associated camera poses. Our framework leverages an Absolute Pose
Regression (APR) architecture, together with an augmented <span class="highlight-title">NeRF</span> module. This
integration not only facilitates the generation of novel views to enrich the
training <span class="highlight-title">dataset</span> but also enables the learning of effective pose features.
Additionally, we extend our architecture for <span class="highlight-title">self-supervised</span> online alignment,
allowing our method to be used and fine-tuned for unlabelled images within a
unified framework. Experiments demonstrate that our method achieves 14.28% and
20.51% performance gain on average in indoor and outdoor benchmark scenes,
outperforming existing APR methods with state-of-the-art accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IFF<span class="highlight-title">NeRF</span>: Initialisation Free and Fast 6DoF pose estimation from a single
  image and a <span class="highlight-title">NeRF</span> model <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce IFF<span class="highlight-title">NeRF</span> to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (<span class="highlight-title">NeRF</span>)
formulation. IFF<span class="highlight-title">NeRF</span> is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFF<span class="highlight-title">NeRF</span> utilizes the Metropolis-Hasting algorithm to sample surface
points from within the <span class="highlight-title">NeRF</span> model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to i<span class="highlight-title">NeRF</span> while performing
at 34fps on consumer hardware and not requiring the initial pose guess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instant Uncertainty Calibration of <span class="highlight-title">NeRF</span>s Using a Meta-calibrator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niki Amini-Naieni, Tomas Jakab, Andrea Vedaldi, Ronald Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) have markedly improved novel view
synthesis, accurate uncertainty quantification in their image predictions
remains an open problem. The prevailing methods for estimating uncertainty,
including the state-of-the-art Density-aware <span class="highlight-title">NeRF</span> Ensembles (DANE) [29],
quantify uncertainty without calibration. This frequently leads to over- or
under-confidence in image predictions, which can undermine their real-world
applications. In this paper, we propose a method which, for the first time,
achieves calibrated uncertainties for <span class="highlight-title">NeRF</span>s. To accomplish this, we overcome a
significant challenge in adapting existing calibration techniques to <span class="highlight-title">NeRF</span>s: a
need to hold out ground truth images from the target scene, reducing the number
of images left to train the <span class="highlight-title">NeRF</span>. This issue is particularly problematic in
sparse-view settings, where we can operate with as few as three images. To
address this, we introduce the concept of a meta-calibrator that performs
uncertainty calibration for <span class="highlight-title">NeRF</span>s with a single forward pass without the need
for holding out any images from the target scene. Our meta-calibrator is a
neural network that takes as input the <span class="highlight-title">NeRF</span> images and uncalibrated uncertainty
maps and outputs a scene-specific calibration curve that corrects the <span class="highlight-title">NeRF</span>'s
uncalibrated uncertainties. We show that the meta-calibrator can generalize on
unseen scenes and achieves well-calibrated and state-of-the-art uncertainty for
<span class="highlight-title">NeRF</span>s, significantly beating DANE and other approaches. This opens
opportunities to improve applications that rely on accurate <span class="highlight-title">NeRF</span> uncertainty
estimates such as next-best view planning and potentially more trustworthy
image reconstruction for medical diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Scene Creation and <span class="highlight-title">Rendering</span> via Rough Meshes: A Lighting Transfer
  Avenue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Cai, Yujie Li, Yuqin Liang, Rongfei Jia, Binqiang Zhao, Mingming Gong, Huan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies how to flexibly integrate reconstructed 3D models into
practical 3D modeling pipelines such as 3D scene creation and <span class="highlight-title">rendering</span>. Due to
the technical difficulty, one can only obtain rough 3D models (R3DMs) for most
real objects using existing 3D reconstruction techniques. As a result,
physically-based <span class="highlight-title">rendering</span> (PBR) would render low-quality images or videos for
scenes that are constructed by R3DMs. One promising solution would be
representing real-world objects as <span class="highlight-title">Neural Fields</span> such as <span class="highlight-title">NeRF</span>s, which are able
to generate photo-realistic <span class="highlight-title">rendering</span>s of an object under desired viewpoints.
However, a drawback is that the synthesized views through <span class="highlight-title">Neural Fields</span>
<span class="highlight-title">Rendering</span> (NFR) cannot reflect the simulated lighting details on R3DMs in PBR
pipelines, especially when object interactions in the 3D scene creation cause
local shadows. To solve this dilemma, we propose a lighting transfer network
(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.
LighTNet reasons about a simplified image composition model, remedies the
uneven surface issue caused by R3DMs, and is empowered by several
perceptual-motivated constraints and a new Lab angle loss which enhances the
contrast between lighting strength and colors. Comparisons demonstrate that
LighTNet is superior in synthesizing impressive lighting, and is promising in
pushing NFR further in practical 3D modeling workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI), project page:
  http://3d-front-future.github.io/LighTNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzhe Zhao, Peng Wang, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While neural <span class="highlight-title">rendering</span> has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (<span class="highlight-title">NeRF</span>) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of <span class="highlight-title">NeRF</span> struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time <span class="highlight-title">rendering</span>. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
<span class="highlight-title">rendering</span> by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior <span class="highlight-title">rendering</span> quality compared to previous state-of-the-art deblur neural
<span class="highlight-title">rendering</span> methods on both synthetic and real <span class="highlight-title">dataset</span>s but also enables
real-time <span class="highlight-title">rendering</span> capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiqiang Sun, Xingyi Li, Liao Shen, Xinyi Ye, Ke Xian, Zhiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic <span class="highlight-title">NeRF</span> methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time <span class="highlight-title">rendering</span> approach to ensure consistent temporal
coherence across the entire scene. We curate a <span class="highlight-title">dataset</span> comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our <span class="highlight-title">dataset</span> demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024. Project page:
  https://huiqiang-sun.github.io/dyblurf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask-Based Modeling for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganlin Yang, Guoqiang Wei, Zhizheng Zhang, Yan Lu, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most Neural Radiance Fields (<span class="highlight-title">NeRF</span>s) exhibit limited generalization
capabilities, which restrict their applicability in representing multiple
scenes using a single model. To address this problem, existing generalizable
<span class="highlight-title">NeRF</span> methods simply condition the model on image features. These methods still
struggle to learn precise global representations over diverse scenes since they
lack an effective mechanism for interacting among different points and views.
In this work, we unveil that 3D implicit representation learning can be
significantly improved by mask-based modeling. Specifically, we propose masked
ray and view modeling for generalizable <span class="highlight-title">NeRF</span> (MRVM-<span class="highlight-title">NeRF</span>), which is a
<span class="highlight-title">self-supervised</span> <span class="highlight-title">pretrain</span>ing target to predict complete scene representations
from partially masked features along each ray. With this <span class="highlight-title">pretrain</span>ing target,
MRVM-<span class="highlight-title">NeRF</span> enables better use of correlations across different points and views
as the geometry priors, which thereby strengthens the capability of capturing
intricate details within the scenes and boosts the generalization capability
across different scenes. Extensive experiments demonstrate the effectiveness of
our proposed MRVM-<span class="highlight-title">NeRF</span> on both synthetic and real-world <span class="highlight-title">dataset</span>s, qualitatively
and quantitatively. Besides, we also conduct experiments to show the
compatibility of our proposed method with various backbones and its superiority
under few-shot cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GGRt: Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Yuanyuan Gao, Chenming Wu, Dingwen Zhang, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time <span class="highlight-title">rendering</span> at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing <span class="highlight-title">NeRF</span>-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
<span class="highlight-title">dataset</span>s and enabling real-time <span class="highlight-title">rendering</span> for immersive experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  \href{https://3d-aigc.github.io/GGRt}{https://3d-aigc.github.io/GGRt}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aria-<span class="highlight-title">NeRF</span>: Multimodal Egocentric View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankai Sun, Jianing Qiu, Chuanyang Zheng, John Tucker, Javier Yu, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (<span class="highlight-title">NeRF</span>s). The construction of a <span class="highlight-title">NeRF</span>-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric <span class="highlight-title">NeRF</span>-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's <span class="highlight-title">NeRF</span>s by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video <span class="highlight-title">dataset</span>. This
<span class="highlight-title">dataset</span> offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU <span class="highlight-title">dataset</span>s (1kHz and 800Hz) paired with a magnetometer. The
<span class="highlight-title">dataset</span> was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this <span class="highlight-title">dataset</span>
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeblurDiNAT: A Lightweight and Effective <span class="highlight-title">Transformer</span> for Image
  Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhou Liu, Binghan Li, Chengkai Liu, Mi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blurry images may contain local and global non-uniform artifacts, which
complicate the deblurring process and make it more challenging to achieve
satisfactory results. Recently, <span class="highlight-title">Transformer</span>s generate improved deblurring
outcomes than existing CNN architectures. However, the large model size and
long inference time are still two bothersome issues which have not been fully
explored. To this end, we propose DeblurDiNAT, a compact encoder-decoder
<span class="highlight-title">Transformer</span> which efficiently restores clean images from real-world blurry
ones. We adopt an alternating dilation factor structure with the aim of
global-local feature learning. Also, we observe that simply using
self-attention layers in networks does not always produce good deblurred
results. To solve this problem, we propose a channel modulation self-attention
(CMSA) block, where a cross-channel learner (CCL) is utilized to capture
channel relationships. In addition, we present a divide and multiply
feed-forward network (DMFN) allowing fast feature propagation. Moreover, we
design a lightweight gated feature fusion (LGFF) module, which performs
controlled feature merging. Comprehensive experimental results show that the
proposed model, named DeblurDiNAT, provides a favorable performance boost
without introducing noticeable computational costs over the baseline, and
achieves state-of-the-art (SOTA) performance on several image deblurring
<span class="highlight-title">dataset</span>s. Compared to nearest competitors, our space-efficient and time-saving
method demonstrates a stronger generalization ability with 3%-68% fewer
parameters and produces deblurred images that are visually closer to the ground
truth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning for Image Super-Resolution and Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Scanvic, Mike Davies, Patrice Abry, Julián Tachella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Self-supervised</span> methods have recently proved to be nearly as effective as
supervised methods in various imaging inverse problems, paving the way for
learning-based methods in scientific and medical imaging applications where
ground truth data is hard or expensive to obtain. This is the case in magnetic
resonance imaging and computed tomography. These methods critically rely on
invariance to translations and/or rotations of the image distribution to learn
from incomplete measurement data alone. However, existing approaches fail to
obtain competitive performances in the problems of image super-resolution and
deblurring, which play a key role in most imaging systems. In this work, we
show that invariance to translations and rotations is insufficient to learn
from measurements that only contain low-frequency information. Instead, we
propose a new <span class="highlight-title">self-supervised</span> approach that leverages the fact that many image
distributions are approximately scale-invariant, and that enables recovering
high-frequency information lost in the measurement process. We demonstrate
throughout a series of experiments on real <span class="highlight-title">dataset</span>s that the proposed method
outperforms other <span class="highlight-title">self-supervised</span> approaches, and obtains performances on par
with fully supervised learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzhe Zhao, Peng Wang, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While neural <span class="highlight-title">rendering</span> has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (<span class="highlight-title">NeRF</span>) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of <span class="highlight-title">NeRF</span> struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time <span class="highlight-title">rendering</span>. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
<span class="highlight-title">rendering</span> by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior <span class="highlight-title">rendering</span> quality compared to previous state-of-the-art deblur neural
<span class="highlight-title">rendering</span> methods on both synthetic and real <span class="highlight-title">dataset</span>s but also enables
real-time <span class="highlight-title">rendering</span> capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Image Restoration via Priors from <span class="highlight-title">Pre-train</span>ed Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaogang Xu, Shu Kong, Tao Hu, Zhe Liu, Hujun Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  <span class="highlight-title">Pre-train</span>ed models with large-scale training data, such as <span class="highlight-title">CLIP</span> and Stable
<span class="highlight-title">Diffusion</span>, have demonstrated remarkable performance in various high-level
computer vision tasks such as image understanding and generation from language
descriptions. Yet, their potential for low-level tasks such as image
restoration remains relatively unexplored. In this paper, we explore such
models to enhance image restoration. As off-the-shelf features (OSF) from
<span class="highlight-title">pre-train</span>ed models do not directly serve image restoration, we propose to learn
an additional lightweight module called <span class="highlight-title">Pre-Train</span>-Guided Refinement Module
(PTG-RM) to refine restoration results of a target restoration network with
OSF. PTG-RM consists of two components, <span class="highlight-title">Pre-Train</span>-Guided Spatial-Varying
Enhancement (PTG-SVE), and <span class="highlight-title">Pre-Train</span>-Guided Channel-Spatial Attention
(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,
while PTG-CSA enhances spatial-channel attention for restoration-related
learning. Extensive experiments demonstrate that PTG-RM, with its compact size
($<$1M parameters), effectively enhances restoration performance of various
models across different tasks, including low-light enhancement, deraining,
deblurring, and denoising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-18T00:00:00Z">2024-03-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with
  Noisy Polarization Priors <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning surfaces from neural radiance field (<span class="highlight-title">NeRF</span>) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA <span class="highlight-title">dataset</span>. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TivNe-SLAM: Dynamic Mapping and Tracking via Time-Varying Neural
  Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18917v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18917v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyao Duan, Zhiliu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous attempts to integrate Neural Radiance Fields (<span class="highlight-title">NeRF</span>) into the
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or require the ground truth camera poses, which
impedes their application in real-world scenarios. In this paper, we propose a
time-varying representation to track and reconstruct the dynamic scenes.
Firstly, two processes, tracking process and mapping process, are
simultaneously maintained in our framework. For the tracking process, all input
images are uniformly sampled, then progressively trained in a <span class="highlight-title">self-supervised</span>
paradigm. For the mapping process, we leverage motion masks to distinguish
dynamic objects from static background, and sample more pixels from dynamic
areas. Secondly, the parameter optimization for both processes consists of two
stages: the first stage associates time with 3D positions to convert the
deformation field to the canonical field. And the second stage associates time
with the embeddings of canonical field to obtain colors and Signed Distance
Function (SDF). Lastly, we propose a novel keyframe selection strategy based on
the overlapping rate. We evaluate our approach on two synthetic <span class="highlight-title">dataset</span>s and
one real-world <span class="highlight-title">dataset</span>. And the experiments validate that our method achieves
competitive results in both tracking and mapping when compared to existing
state-of-the-art <span class="highlight-title">NeRF</span>-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Mesh <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and
  Detail 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjin Chen, Junhao Chen, Xiaojun Ye, Huan-ang Gao, Xiaoxue Chen, Zhaoxin Fan, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human body reconstruction has been a challenge in the field of computer
vision. Previous methods are often time-consuming and difficult to capture the
detailed appearance of the human body. In this paper, we propose a new method
called \emph{Ultraman} for fast reconstruction of textured 3D human models from
a single image. Compared to existing techniques, \emph{Ultraman} greatly
improves the reconstruction speed and accuracy while preserving high-quality
texture details. We present a set of new frameworks for human reconstruction
consisting of three parts, geometric reconstruction, texture generation and
texture mapping. Firstly, a mesh reconstruction framework is used, which
accurately extracts 3D human shapes from a single image. At the same time, we
propose a method to generate a multi-view consistent image of the human body
based on a single image. This is finally combined with a novel texture mapping
method to optimize texture details and ensure color consistency during
reconstruction. Through extensive experiments and evaluations, we demonstrate
the superior performance of \emph{Ultraman} on various standard <span class="highlight-title">dataset</span>s. In
addition, \emph{Ultraman} outperforms state-of-the-art methods in terms of
human <span class="highlight-title">rendering</span> quality and speed. Upon acceptance of the article, we will make
the code and data publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://air-discover.github.io/Ultraman/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GetMesh: A Controllable Model for High-quality Mesh Generation and
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Lyu, Ben Fei, Jinyi Wang, Xudong Xu, Ya Zhang, Weidong Yang, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mesh is a fundamental representation of 3D assets in various industrial
applications, and is widely supported by professional softwares. However, due
to its irregular structure, mesh creation and manipulation is often
time-consuming and labor-intensive. In this paper, we propose a highly
controllable generative model, GetMesh, for mesh generation and manipulation
across different categories. By taking a varying number of points as the latent
representation, and re-organizing them as triplane representation, GetMesh
generates meshes with rich and sharp details, outperforming both
single-category and multi-category counterparts. Moreover, it also enables
fine-grained control over the generation process that previous mesh generative
models cannot achieve, where changing global/local mesh topologies,
adding/removing mesh parts, and combining mesh parts across categories can be
intuitively, efficiently, and robustly accomplished by adjusting the number,
positions or features of latent points. Project page is
https://getmesh.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with
  Noisy Polarization Priors <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning surfaces from neural radiance field (<span class="highlight-title">NeRF</span>) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA <span class="highlight-title">dataset</span>. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical
  Shape Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Pepe, Richard Schussnig, Jianning Li, Christina Gsaxner, Dieter Schmalstieg, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shape reconstruction from imaging volumes is a recurring need in medical
image analysis. Common workflows start with a segmentation step, followed by
careful post-processing and,finally, ad hoc meshing algorithms. As this
sequence can be timeconsuming, neural networks are trained to reconstruct
shapes through template deformation. These networks deliver state-ofthe-art
results without manual intervention, but, so far, they have primarily been
evaluated on anatomical shapes with little topological variety between
individuals. In contrast, other works favor learning implicit shape models,
which have multiple benefits for meshing and visualization. Our work follows
this direction by introducing deep medial voxels, a semi-implicit
representation that faithfully approximates the topological skeleton from
imaging volumes and eventually leads to shape reconstruction via convolution
surfaces. Our reconstruction technique shows potential for both visualization
and computer simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh
  and Implicit Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhua Wu, Qi Wang, Guangming Wang, Junping Wang, Tiankun Zhao, Yang Liu, Dongchao Gao, Zhe Liu, Hesheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road surface reconstruction plays a vital role in autonomous driving systems,
enabling road lane perception and high-precision mapping. Recently, neural
implicit encoding has achieved remarkable results in scene representation,
particularly in the realistic <span class="highlight-title">rendering</span> of scene textures. However, it faces
challenges in directly representing geometric information for large-scale
scenes. To address this, we propose EMIE-MAP, a novel method for large-scale
road surface reconstruction based on explicit mesh and implicit encoding. The
road geometry is represented using explicit mesh, where each vertex stores
implicit encoding representing the color and semantic information. To overcome
the difficulty in optimizing road elevation, we introduce a trajectory-based
elevation initialization and an elevation residual learning method based on
Multi-Layer Perceptron (<span class="highlight-title">MLP</span>). Additionally, by employing implicit encoding and
multi-camera color <span class="highlight-title">MLP</span>s decoding, we achieve separate modeling of scene
physical properties and camera characteristics, allowing surround-view
reconstruction compatible with different camera models. Our method achieves
remarkable road surface reconstruction performance in a variety of real-world
challenging scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized 3D Human Pose and Shape Refinement <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Wehrbein, Bodo Rosenhahn, Iain Matthews, Carsten Stoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, regression-based methods have dominated the field of 3D human pose
and shape estimation. Despite their promising results, a common issue is the
misalignment between predictions and image observations, often caused by minor
joint rotation errors that accumulate along the kinematic chain. To address
this issue, we propose to construct dense correspondences between initial human
model estimates and the corresponding images that can be used to refine the
initial predictions. To this end, we utilize <span class="highlight-title">rendering</span>s of the 3D models to
predict per-pixel 2D displacements between the synthetic <span class="highlight-title">rendering</span>s and the RGB
images. This allows us to effectively integrate and exploit appearance
information of the persons. Our per-pixel displacements can be efficiently
transformed to per-visible-vertex displacements and then used for 3D model
refinement by minimizing a reprojection loss. To demonstrate the effectiveness
of our approach, we refine the initial 3D human mesh predictions of multiple
models using different refinement procedures on 3DPW and RICH. We show that our
approach not only consistently leads to better image-model alignment, but also
to improved 3D accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2023 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures
  for Human Avatar Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing photo-realistic drivable human avatars from multi-view image
sequences has been a popular and challenging topic in the field of computer
vision and graphics. While existing <span class="highlight-title">NeRF</span>-based methods can achieve high-quality
novel view <span class="highlight-title">rendering</span> of human models, both training and inference processes are
time-consuming. Recent approaches have utilized 3D Gaussians to represent the
human body, enabling faster training and <span class="highlight-title">rendering</span>. However, they undermine the
importance of the mesh guidance and directly predict Gaussians in 3D space with
coarse mesh guidance. This hinders the learning procedure of the Gaussians and
tends to produce blurry textures. Therefore, we propose UV Gaussians, which
models the 3D human body by jointly learning mesh deformations and 2D UV-space
Gaussian textures. We utilize the embedding of UV map to learn Gaussian
textures in 2D space, leveraging the capabilities of powerful 2D networks to
extract features. Additionally, through an independent Mesh network, we
optimize pose-dependent geometric deformations, thereby guiding Gaussian
<span class="highlight-title">rendering</span> and significantly enhancing <span class="highlight-title">rendering</span> quality. We collect and process
a new <span class="highlight-title">dataset</span> of human motion, which includes multi-view images, scanned
models, parametric model registration, and corresponding texture maps.
Experimental results demonstrate that our method achieves state-of-the-art
synthesis of novel view and novel pose. The code and data will be made
available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the
paper is accepted.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic
  Vision Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Mandula, Jonas Kühne, Luca Pascarella, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) are gaining popularity in civil and military
applications. However, uncontrolled access to restricted areas threatens
privacy and security. Thus, prevention and detection of UAVs are pivotal to
guarantee confidentiality and safety. Although active scanning, mainly based on
radars, is one of the most accurate technologies, it can be expensive and less
versatile than passive inspections, e.g., object recognition. Dynamic vision
sensors (DVS) are bio-inspired event-based vision models that leverage
timestamped pixel-level brightness changes in fast-moving scenes that adapt
well to low-latency object detection. This paper presents F-UAV-D (Fast
Unmanned Aerial Vehicle Detector), an embedded system that enables fast-moving
drone detection. In particular, we propose a setup to exploit DVS as an
alternative to RGB cameras in a real-time and low-power configuration. Our
approach leverages the high-dynamic range (HDR) and background suppression of
DVS and, when trained with various fast-moving drones, outperforms RGB input in
suboptimal ambient conditions such as low illumination and fast-moving scenes.
Our results show that F-UAV-D can (i) detect drones by using less than <15 W on
average and (ii) perform real-time inference (i.e., <50 ms) by leveraging the
CPU and GPU nodes of our edge computer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Instrumentation and Measurement
  Technology Conference (I2MTC)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defense Against Adversarial Attacks on No-Reference Image Quality Models
  with Gradient Norm Regularization <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the
quality score of an input image without additional information. NR-IQA models
play a crucial role in the media industry, aiding in performance evaluation and
optimization guidance. However, these models are found to be vulnerable to
adversarial attacks, which introduce imperceptible perturbations to input
images, resulting in significant changes in predicted scores. In this paper, we
propose a defense method to improve the stability in predicted scores when
attacked by small perturbations, thus enhancing the adversarial robustness of
NR-IQA models. To be specific, we present theoretical evidence showing that the
magnitude of score changes is related to the $\ell_1$ norm of the model's
gradient with respect to the input image. Building upon this theoretical
foundation, we propose a norm regularization training strategy aimed at
reducing the $\ell_1$ norm of the gradient, thereby boosting the robustness of
NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate
the effectiveness of our strategy in reducing score changes in the presence of
adversarial attacks. To the best of our knowledge, this work marks the first
attempt to defend against adversarial attacks on NR-IQA models. Our study
offers valuable insights into the adversarial robustness of NR-IQA models and
provides a foundation for future research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposure Bracketing is All You Need for Unifying Image Restoration and
  Enhancement Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is highly desired but challenging to acquire high-quality photos with
clear content in low-light environments. Although multi-image processing
methods (using burst, dual-exposure, or multi-exposure images) have made
significant progress in addressing this issue, they typically focus on specific
restoration or enhancement problems, being insufficient in exploiting
multi-image. Motivated by that multi-exposure images are complementary in
denoising, deblurring, high dynamic range imaging, and super-resolution, we
propose to utilize exposure bracketing photography to unify restoration and
enhancement tasks in this work. Due to the difficulty in collecting real-world
pairs, we suggest a solution that first <span class="highlight-title">pre-train</span>s the model with synthetic
paired data and then adapts it to real-world unlabeled images. In particular, a
temporally modulated recurrent network (TMRNet) and <span class="highlight-title">self-supervised</span> adaptation
method are proposed. Moreover, we construct a data simulation pipeline to
synthesize pairs and collect real-world images from 200 nighttime scenarios.
Experiments on both <span class="highlight-title">dataset</span>s show that our method performs favorably against
the state-of-the-art multi-image processing ones. The <span class="highlight-title">dataset</span>, code, and
<span class="highlight-title">pre-train</span>ed models are available at https://github.com/cszhilu1998/BracketIRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-17T00:00:00Z">2024-03-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality-Aware Image-Text Alignment for Real-World Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods
to measure image quality in alignment with human perception when a high-quality
reference image is unavailable. The reliance on annotated Mean Opinion Scores
(MOS) in the majority of state-of-the-art NR-IQA approaches limits their
scalability and broader applicability to real-world scenarios. To overcome this
limitation, we propose Quali<span class="highlight-title">CLIP</span> (Quality-aware <span class="highlight-title">CLIP</span>), a <span class="highlight-title">CLIP</span>-based
<span class="highlight-title">self-supervised</span> opinion-unaware method that does not require labeled MOS. In
particular, we introduce a quality-aware image-text alignment strategy to make
<span class="highlight-title">CLIP</span> generate representations that correlate with the inherent quality of the
images. Starting from pristine images, we synthetically degrade them with
increasing levels of intensity. Then, we train <span class="highlight-title">CLIP</span> to rank these degraded
images based on their similarity to quality-related antonym text <span class="highlight-title">prompt</span>s, while
guaranteeing consistent representations for images with comparable quality. Our
method achieves state-of-the-art performance on several <span class="highlight-title">dataset</span>s with authentic
distortions. Moreover, despite not requiring MOS, Quali<span class="highlight-title">CLIP</span> outperforms
supervised methods when their training <span class="highlight-title">dataset</span> differs from the testing one,
thus proving to be more suitable for real-world scenarios. Furthermore, our
approach demonstrates greater robustness and improved explainability than
competing methods. The code and the model are publicly available at
https://github.com/miccunifi/Quali<span class="highlight-title">CLIP</span>.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-16T00:00:00Z">2024-03-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Multimodal Large Language Models for Image
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Wu, Kede Ma, Jie Liang, Yujiu Yang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Multimodal Large Language Models (MLLMs) have experienced significant
advancement on visual understanding and reasoning, their potentials to serve as
powerful, flexible, interpretable, and text-driven models for Image Quality
Assessment (IQA) remains largely unexplored. In this paper, we conduct a
comprehensive and systematic study of <span class="highlight-title">prompt</span>ing MLLMs for IQA. Specifically, we
first investigate nine <span class="highlight-title">prompt</span>ing systems for MLLMs as the combinations of three
standardized testing procedures in psychophysics (i.e., the single-stimulus,
double-stimulus, and multiple-stimulus methods) and three popular <span class="highlight-title">prompt</span>ing
strategies in natural language processing (i.e., the standard, in-context, and
chain-of-thought <span class="highlight-title">prompt</span>ing). We then present a difficult sample selection
procedure, taking into account sample diversity and uncertainty, to further
challenge MLLMs equipped with the respective optimal <span class="highlight-title">prompt</span>ing systems. We
assess three open-source and one close-source MLLMs on several visual
attributes of image quality (e.g., structural and textural distortions, color
differences, and geometric transformations) in both full-reference and
no-reference scenarios. Experimental results show that only the close-source
<span class="highlight-title">GPT</span>-4V provides a reasonable account for human perception of image quality, but
is weak at discriminating fine-grained quality variations (e.g., color
differences) and at comparing visual quality of multiple images, tasks humans
can perform effortlessly.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-15T00:00:00Z">2024-03-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively
  Aggregated Spatio-Temporal Aligment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoning Liu, Ao Li, Zongwei Wu, Yapeng Du, Le Zhang, Yulun Zhang, Radu Timofte, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging <span class="highlight-title">Transformer</span> attention has led to great advancements in HDR
deghosting. However, the intricate nature of self-attention introduces
practical challenges, as existing state-of-the-art methods often demand
high-end GPUs or exhibit slow inference speeds, especially for high-resolution
images like 2K. Striking an optimal balance between performance and latency
remains a critical concern. In response, this work presents PASTA, a novel
Progressively Aggregated Spatio-Temporal Alignment framework for HDR
deghosting. Our approach achieves effectiveness and efficiency by harnessing
hierarchical representation during feature distanglement. Through the
utilization of diverse granularities within the hierarchical structure, our
method substantially boosts computational speed and optimizes the HDR imaging
workflow. In addition, we explore within-scale feature modeling with local and
global attention, gradually merging and refining them in a coarse-to-fine
fashion. Experimental results showcase PASTA's superiority over current SOTA
methods in both visual quality and performance metrics, accompanied by a
substantial 3-fold (x3) increase in inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-14T00:00:00Z">2024-03-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RING-<span class="highlight-title">NeRF</span> : Rethinking Inductive Biases for Versatile and Efficient
  <span class="highlight-title">Neural Fields</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doriand Petit, Steve Bourgeois, Dumitru Pavel, Vincent Gay-Bellile, Florian Chabot, Loic Barthe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in <span class="highlight-title">Neural Fields</span> mostly rely on developing task-specific
supervision which often complicates the models. Rather than developing
hard-to-combine and specific modules, another approach generally overlooked is
to directly inject generic priors on the scene representation (also called
inductive biases) into the <span class="highlight-title">NeRF</span> architecture. Based on this idea, we propose
the RING-<span class="highlight-title">NeRF</span> architecture which includes two inductive biases : a continuous
multi-scale representation of the scene and an invariance of the decoder's
latent space over spatial and scale domains. We also design a single
reconstruction process that takes advantage of those inductive biases and
experimentally demonstrates on-par performances in terms of quality with
dedicated architecture on multiple tasks (anti-aliasing, few view
reconstruction, SDF reconstruction without scene-specific initialization) while
being more efficient. Moreover, RING-<span class="highlight-title">NeRF</span> has the distinctive ability to
dynamically increase the resolution of the model, opening the way to adaptive
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-based Asynchronous HDR Imaging by Temporal Incident Light
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Wu, Ganchao Tan, Jinze Chen, Wei Zhai, Yang Cao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current
frame-based cameras struggle to achieve high dynamic range imaging due to the
conflict between globally uniform exposure and spatially variant scene
illumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR
imaging system, based on key insights into the challenges in HDR imaging and
the unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our
proposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD
panels modulate the irradiance incident upon the DVS by altering their
transparency, thereby triggering the pixel-independent event streams. The HDR
image is subsequently decoded from the event streams through our
temporal-weighted algorithm. Experiments under standard test platform and
several challenging scenes have verified the feasibility of the system in HDR
imaging task.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with
  Spike Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Chen, Shiyan Chen, Jiyuan Zhang, Baoyue Zhang, Yajing Zheng, Tiejun Huang, Zhaofei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a sequence of sharp images from the blurry input is crucial
for enhancing our insights into the captured scene and poses a significant
challenge due to the limited temporal features embedded in the image. Spike
cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing
motion features and beneficial for solving this ill-posed problem. Nonetheless,
existing methods fall into the supervised learning paradigm, which suffers from
notable performance degradation when applied to real-world scenarios that
diverge from the synthetic training data domain. Moreover, the quality of
reconstructed images is capped by the generated images based on motion analysis
interpolation, which inherently differs from the actual scene, affecting the
generalization ability of these methods in real high-speed scenarios. To
address these challenges, we propose the first <span class="highlight-title">self-supervised</span> framework for
the task of spike-guided motion deblurring. Our approach begins with the
formulation of a spike-guided deblurring model that explores the theoretical
relationships among spike streams, blurry images, and their corresponding sharp
sequences. We subsequently develop a <span class="highlight-title">self-supervised</span> cascaded framework to
alleviate the issues of spike noise and spatial-resolution mismatching
encountered in the deblurring model. With knowledge distillation and
re-blurring loss, we further design a lightweight deblur network to generate
high-quality sequences with brightness and texture consistency with the
original input. Quantitative and qualitative experiments conducted on our
real-world and synthetic <span class="highlight-title">dataset</span>s with spikes validate the superior
generalization of the proposed framework. Our code, data and trained models
will be available at \url{https://github.com/chenkang455/S-SDM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-13T00:00:00Z">2024-03-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MatFuse: Controllable Material Generation with <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Vecchio, Renato Sortino, Simone Palazzo, Concetto Spampinato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality materials in computer graphics is a challenging and
time-consuming task, which requires great expertise. To simplify this process,
we introduce MatFuse, a unified approach that harnesses the generative power of
<span class="highlight-title">diffusion</span> models for creation and editing of 3D materials. Our method
integrates multiple sources of conditioning, including color palettes,
sketches, text, and pictures, enhancing creative possibilities and granting
fine-grained control over material synthesis. Additionally, MatFuse enables
map-level material editing capabilities through latent manipulation by means of
a multi-encoder compression model which learns a disentangled latent
representation for each map. We demonstrate the effectiveness of MatFuse under
multiple conditioning settings and explore the potential of material editing.
Finally, we assess the quality of the generated materials both quantitatively
in terms of <span class="highlight-title">CLIP</span>-IQA and FID scores and qualitatively by conducting a user
study. Source code for training MatFuse and supplemental materials are publicly
available at https://gvecchio.com/matfuse.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Multi-scale Network with Learnable Discrete Wavelet Transform
  for Blind Motion Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gao, Tianheng Qiu, Xinyu Zhang, Hanlin Bai, Kang Liu, Xuan Huang, Hu Wei, Guoying Zhang, Huaping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coarse-to-fine schemes are widely used in traditional single-image motion
deblur; however, in the context of deep learning, existing multi-scale
algorithms not only require the use of complex modules for feature fusion of
low-scale RGB images and deep semantics, but also manually generate
low-resolution pairs of images that do not have sufficient confidence. In this
work, we propose a multi-scale network based on single-input and
multiple-outputs(SIMO) for motion deblurring. This simplifies the complexity of
algorithms based on a coarse-to-fine scheme. To alleviate restoration defects
impacting detail information brought about by using a multi-scale architecture,
we combine the characteristics of real-world blurring trajectories with a
learnable wavelet transform module to focus on the directional continuity and
frequency features of the step-by-step transitions between blurred images to
sharp images. In conclusion, we propose a multi-scale network with a learnable
discrete wavelet transform (MLWNet), which exhibits state-of-the-art
performance on multiple real-world deblurred <span class="highlight-title">dataset</span>s, in terms of both
subjective and objective quality as well as computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-12T00:00:00Z">2024-03-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Text: Frozen Large Language Models in Visual Signal Comprehension <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhu, Fangyun Wei, Yanye Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the potential of a large language model (LLM) to
directly comprehend visual signals without the necessity of fine-tuning on
multi-modal <span class="highlight-title">dataset</span>s. The foundational concept of our method views an image as
a linguistic entity, and translates it to a set of discrete words derived from
the LLM's vocabulary. To achieve this, we present the Vision-to-Language
Tokenizer, abbreviated as V2T Tokenizer, which transforms an image into a
``foreign language'' with the combined aid of an encoder-decoder, the LLM
vocabulary, and a <span class="highlight-title">CLIP</span> model. With this innovative image encoding, the LLM
gains the ability not only for visual comprehension but also for image
denoising and restoration in an auto-regressive fashion-crucially, without any
fine-tuning. We undertake rigorous experiments to validate our method,
encompassing understanding tasks like image recognition, image captioning, and
visual question answering, as well as image denoising tasks like inpainting,
outpainting, deblurring, and shift restoration. Code and models are available
at https://github.com/zh460045050/V2L-Tokenizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-11T00:00:00Z">2024-03-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDRTransDC: High Dynamic Range Image Reconstruction with <span class="highlight-title">Transformer</span>
  Deformation Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaikang Shang, Xuejing Kang, Anlong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image
with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.
Caused by large motion and severe under-/over-exposure among input LDR images,
HDR imaging suffers from ghosting artifacts and fusion distortions. To address
these critical issues, we propose an HDR <span class="highlight-title">Transformer</span> Deformation Convolution
(HDRTransDC) network to generate high-quality HDR images, which consists of the
<span class="highlight-title">Transformer</span> Deformable Convolution Alignment Module (TDCAM) and the Dynamic
Weight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM
extracts long-distance content similar to the reference feature in the entire
non-reference features, which can accurately remove misalignment and fill the
content occluded by moving objects. For the purpose of eliminating fusion
distortions, we propose DWFB to spatially adaptively select useful information
across frames to effectively fuse multi-exposed features. Extensive experiments
show that our method quantitatively and qualitatively achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of No-Reference Image Quality Models via MAP Estimation in
  <span class="highlight-title">Diffusion</span> Latents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixia Zhang, Dingquan Li, Guangtao Zhai, Xiaokang Yang, Kede Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary no-reference image quality assessment (NR-IQA) models can
effectively quantify the perceived image quality, with high correlations
between model predictions and human perceptual scores on fixed test sets.
However, little progress has been made in comparing NR-IQA models from a
perceptual optimization perspective. Here, for the first time, we demonstrate
that NR-IQA models can be plugged into the maximum a posteriori (MAP)
estimation framework for image enhancement. This is achieved by taking the
gradients in differentiable and bijective <span class="highlight-title">diffusion</span> latents rather than in the
raw pixel domain. Different NR-IQA models are likely to induce different
enhanced images, which are ultimately subject to psychophysical testing. This
leads to a new computational method for comparing NR-IQA models within the
analysis-by-synthesis framework. Compared to conventional correlation-based
metrics, our method provides complementary insights into the relative strengths
and weaknesses of the competing NR-IQA models in the context of perceptual
optimization.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-10T00:00:00Z">2024-03-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depicting Beyond Scores: Advancing Image Quality Assessment through
  Multi-modal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Depicted image Quality Assessment method (DepictQA),
overcoming the constraints of traditional score-based methods. DepictQA allows
for detailed, language-based, human-like evaluation of image quality by
leveraging Multi-modal Large Language Models (MLLMs). Unlike conventional Image
Quality Assessment (IQA) methods relying on scores, DepictQA interprets image
content and distortions descriptively and comparatively, aligning closely with
humans' reasoning process. To build the DepictQA model, we establish a
hierarchical task framework, and collect a multi-modal IQA training <span class="highlight-title">dataset</span>. To
tackle the challenges of limited training data and multi-image processing, we
propose to use multi-source training data and specialized image tags. These
designs result in a better performance of DepictQA than score-based approaches
on multiple benchmarks. Moreover, compared with general MLLMs, DepictQA can
generate more accurate reasoning descriptive languages. Our work demonstrates
the utility of our full-reference <span class="highlight-title">dataset</span> in non-reference applications, and
indicates that language-based IQA methods have the potential to be customized
for individual preferences.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-08T00:00:00Z">2024-03-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIFu for the Real World: A <span class="highlight-title">Self-supervised</span> Framework to Reconstruct
  Dressed Human from Single-view Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangyang Xiong, Dong Du, Yushuang Wu, Jingqi Dong, Di Kang, Linchao Bao, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is very challenging to accurately reconstruct sophisticated human geometry
caused by various poses and garments from a single image. Recently, works based
on pixel-aligned implicit function (PIFu) have made a big step and achieved
state-of-the-art fidelity on image-based 3D human digitization. However, the
training of PIFu relies heavily on expensive and limited 3D ground truth data
(i.e. synthetic data), thus hindering its generalization to more diverse real
world images. In this work, we propose an end-to-end <span class="highlight-title">self-supervised</span> network
named SelfPIFu to utilize abundant and diverse in-the-wild images, resulting in
largely improved reconstructions when tested on unconstrained in-the-wild
images. At the core of SelfPIFu is the depth-guided volume-/surface-aware
signed distance fields (SDF) learning, which enables <span class="highlight-title">self-supervised</span> learning
of a PIFu without access to GT mesh. The whole framework consists of a normal
estimator, a depth estimator, and a SDF-based PIFu and better utilizes extra
depth GT during training. Extensive experiments demonstrate the effectiveness
of our <span class="highlight-title">self-supervised</span> framework and the superiority of using depth as input.
On synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18%
higher compared with PIFuHD. For in-the-wild images, we conduct user studies on
the reconstructed results, the selection rate of our results is over 68%
compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object
  Surface Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced techniques using Neural Radiance Fields (<span class="highlight-title">NeRF</span>), Signed Distance
Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D
indoor scene reconstruction. We introduce a novel two-phase learning approach,
H2O-SDF, that discriminates between object and non-object regions within indoor
environments. This method achieves a nuanced balance, carefully preserving the
geometric integrity of room layouts while also capturing intricate surface
details of specific objects. A cornerstone of our two-phase learning framework
is the introduction of the Object Surface Field (OSF), a novel concept designed
to mitigate the persistent vanishing gradient problem that has previously
hindered the capture of high-frequency details in other methods. Our proposed
approach is validated through several experiments that include ablation
studies.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Degradations with Recurrent Network for Video Restoration in
  Under-Display Camera <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxu Liu, Xuan Wang, Yuanting Fan, Shuai Li, Xueming Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under-display camera (UDC) systems are the foundation of full-screen display
devices in which the lens mounts under the display. The pixel array of
light-emitting diodes used for display diffracts and attenuates incident light,
causing various degradations as the light intensity changes. Unlike general
video restoration which recovers video by treating different degradation
factors equally, video restoration for UDC systems is more challenging that
concerns removing diverse degradation over time while preserving temporal
consistency. In this paper, we introduce a novel video restoration network,
called D$^2$RNet, specifically designed for UDC systems. It employs a set of
Decoupling Attention Modules (DAM) that effectively separate the various video
degradation factors. More specifically, a soft mask generation function is
proposed to formulate each frame into flare and haze based on the diffraction
arising from incident light of different intensities, followed by the proposed
flare and haze removal components that leverage long- and short-term feature
learning to handle the respective degradations. Such a design offers an
targeted and effective solution to eliminating various types of degradation in
UDC systems. We further extend our design into multi-scale to overcome the
scale-changing of degradation that often occur in long-range videos. To
demonstrate the superiority of D$^2$RNet, we propose a large-scale UDC video
benchmark by gathering HDR videos and generating realistically degraded videos
using the point spread function measured by a commercial UDC system. Extensive
quantitative and qualitative evaluations demonstrate the superiority of
D$^2$RNet compared to other state-of-the-art video restoration and UDC image
restoration methods. Code is available at
https://github.com/ChengxuLiu/DDRNet.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>IQA: Boosting the Performance and Generalization for No-Reference
  Image Quality Assessment via <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewen Chen, Haina Qin, Juan Wang, Chunfeng Yuan, Bing Li, Weiming Hu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the diversity of assessment requirements in various application
scenarios for the IQA task, existing IQA methods struggle to directly adapt to
these varied requirements after training. Thus, when facing new requirements, a
typical approach is fine-tuning these models on <span class="highlight-title">dataset</span>s specifically created
for those requirements. However, it is time-consuming to establish IQA
<span class="highlight-title">dataset</span>s. In this work, we propose a <span class="highlight-title">Prompt</span>-based IQA (<span class="highlight-title">Prompt</span>IQA) that can
directly adapt to new requirements without fine-tuning after training. On one
hand, it utilizes a short sequence of Image-Score Pairs (ISP) as <span class="highlight-title">prompt</span>s for
targeted predictions, which significantly reduces the dependency on the data
requirements. On the other hand, <span class="highlight-title">Prompt</span>IQA is trained on a mixed <span class="highlight-title">dataset</span> with
two proposed data augmentation strategies to learn diverse requirements, thus
enabling it to effectively adapt to new requirements. Experiments indicate that
the <span class="highlight-title">Prompt</span>IQA outperforms SOTA methods with higher performance and better
generalization. The code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-07T00:00:00Z">2024-03-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Image HDR Reconstruction Assisted Ghost Suppression and Detail
  Preservation Network for Multi-Exposure HDR Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huafeng Li, Zhenmei Yang, Yafei Zhang, Dapeng Tao, Zhengtao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of high dynamic range (HDR) images from multi-exposure low
dynamic range (LDR) images in dynamic scenes presents significant challenges,
especially in preserving and restoring information in oversaturated regions and
avoiding ghosting artifacts. While current methods often struggle to address
these challenges, our work aims to bridge this gap by developing a
multi-exposure HDR image reconstruction network for dynamic scenes,
complemented by single-frame HDR image reconstruction. This network, comprising
single-frame HDR reconstruction with enhanced stop image (SHDR-ESI) and
SHDR-ESI-assisted multi-exposure HDR reconstruction (SHDRA-MHDR), effectively
leverages the ghost-free characteristic of single-frame HDR reconstruction and
the detail-enhancing capability of ESI in oversaturated areas. Specifically,
SHDR-ESI innovatively integrates single-frame HDR reconstruction with the
utilization of ESI. This integration not only optimizes the single image HDR
reconstruction process but also effectively guides the synthesis of
multi-exposure HDR images in SHDR-AMHDR. In this method, the single-frame HDR
reconstruction is specifically applied to reduce potential ghosting effects in
multiexposure HDR synthesis, while the use of ESI images assists in enhancing
the detail information in the HDR synthesis process. Technically, SHDR-ESI
incorporates a detail enhancement mechanism, which includes a
self-representation module and a mutual-representation module, designed to
aggregate crucial information from both reference image and ESI. To fully
leverage the complementary information from non-reference images, a feature
interaction fusion module is integrated within SHDRA-MHDR. Additionally, a
ghost suppression module, guided by the ghost-free results of SHDR-ESI, is
employed to suppress the ghosting artifacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Computational Imaging</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRConStyle: Image Restoration Framework Using Contrastive Learning and
  Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15784v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15784v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Fan, Xin Zhao, Liang Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the contrastive learning paradigm has achieved remarkable success
in high-level tasks such as classification, detection, and segmentation.
However, contrastive learning applied in low-level tasks, like image
restoration, is limited, and its effectiveness is uncertain. This raises a
question: Why does the contrastive learning paradigm not yield satisfactory
results in image restoration? In this paper, we conduct in-depth analyses and
propose three guidelines to address the above question. In addition, inspired
by style transfer and based on contrastive learning, we propose a novel module
for image restoration called \textbf{ConStyle}, which can be efficiently
integrated into any U-Net structure network. By leveraging the flexibility of
ConStyle, we develop a \textbf{general restoration network} for image
restoration. ConStyle and the general restoration network together form an
image restoration framework, namely \textbf{IRConStyle}. To demonstrate the
capability and compatibility of ConStyle, we replace the general restoration
network with <span class="highlight-title">transformer</span>-based, CNN-based, and <span class="highlight-title">MLP</span>-based networks,
respectively. We perform extensive experiments on various image restoration
tasks, including denoising, deblurring, deraining, and dehazing. The results on
19 benchmarks demonstrate that ConStyle can be integrated with any U-Net-based
network and significantly enhance performance. For instance, ConStyle NAFNet
significantly outperforms the original NAFNet on SOTS outdoor (dehazing) and
Rain100H (deraining) <span class="highlight-title">dataset</span>s, with PSNR improvements of 4.16 dB and 3.58 dB
with 85% fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-06T00:00:00Z">2024-03-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDRFlow: Real-Time HDR Video Reconstruction with Large Motions <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangwei Xu, Yujin Wang, Jinwei Gu, Tianfan Xue, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing High Dynamic Range (HDR) video from image sequences captured
with alternating exposures is challenging, especially in the presence of large
camera or object motion. Existing methods typically align low dynamic range
sequences using optical flow or attention mechanism for deghosting. However,
they often struggle to handle large complex motions and are computationally
expensive. To address these challenges, we propose a robust and efficient flow
estimator tailored for real-time HDR video reconstruction, named HDRFlow.
HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an
efficient flow network with a multi-size large kernel (MLK), and a new HDR flow
training scheme. The HALoss supervises our flow network to learn an
HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK
can effectively model large motions at a negligible cost. In addition, we
incorporate synthetic data, Sintel, into our training <span class="highlight-title">dataset</span>, utilizing both
its provided forward flow and backward flow generated by us to supervise our
flow network, enhancing our performance in large motion regions. Extensive
experiments demonstrate that our HDRFlow outperforms previous methods on
standard benchmarks. To the best of our knowledge, HDRFlow is the first
real-time HDR video reconstruction method for video sequences captured with
alternating exposures, capable of processing 720p resolution inputs at 25ms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project website: https://openimaginglab.github.io/HDRFlow/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-05T00:00:00Z">2024-03-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Human Mesh Reconstruction with Textures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhan, Jianxin Yang, Yuanqi Li, Jie Guo, Yanwen Guo, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of 3D detailed human mesh reconstruction has made significant
progress in recent years. However, current methods still face challenges when
used in industrial applications due to unstable results, low-quality meshes,
and a lack of UV unwrapping and skinning weights. In this paper, we present
SHERT, a novel pipeline that can reconstruct semantic human meshes with
textures and high-precision details. SHERT applies semantic- and normal-based
sampling between the detailed surface (eg mesh and SDF) and the corresponding
SMPL-X model to obtain a partially sampled semantic mesh and then generates the
complete semantic mesh by our specifically designed <span class="highlight-title">self-supervised</span> completion
and refinement networks. Using the complete semantic mesh as a basis, we employ
a texture <span class="highlight-title">diffusion</span> model to create human textures that are driven by both
images and texts. Our reconstructed meshes have stable UV unwrapping,
high-quality triangle meshes, and consistent semantic information. The given
SMPL-X model provides semantic information and shape priors, allowing SHERT to
perform well even with incorrect and incomplete inputs. The semantic
information also makes it easy to substitute and animate different body parts
such as the face, body, and hands. Quantitative and qualitative experiments
demonstrate that SHERT is capable of producing high-fidelity and robust
semantic meshes that outperform state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid
  <span class="highlight-title">Transformer</span> and Contrastive Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defocus blur is a persistent problem in microscope imaging that poses harm to
pathology interpretation and medical intervention in cell microscopy and
microscope surgery. To address this problem, a unified framework including
multi-pyramid <span class="highlight-title">transformer</span> (MPT) and extended frequency contrastive
regularization (EFCR) is proposed to tackle two outstanding challenges in
microscopy deblur: longer attention span and feature deficiency. The MPT
employs an explicit pyramid structure at each network stage that integrates the
cross-scale window attention (CSWA), the intra-scale channel attention (ISCA),
and the feature-enhancing feed-forward network (FEFN) to capture long-range
cross-scale spatial interaction and global channel context. The EFCR addresses
the feature deficiency problem by exploring latent deblur signals from
different frequency bands. It also enables deblur knowledge transfer to learn
cross-domain information from extra data, improving deblur performance for
labeled and unlabeled data. Extensive experiments and downstream task
validation show the framework achieves state-of-the-art performance across
multiple <span class="highlight-title">dataset</span>s. Project page: https://github.com/PieceZhang/MPT-CataBlur.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Variational Approach for Joint Image Recovery and Feature Extraction
  Based on Spatially-Varying Generalised Gaussian Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01375v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01375v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilie Chouzenoux, Marie-Caroline Corbineau, Jean-Christophe Pesquet, Gabriele Scrivanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The joint problem of reconstruction / feature extraction is a challenging
task in image processing. It consists in performing, in a joint manner, the
restoration of an image and the extraction of its features. In this work, we
firstly propose a novel nonsmooth and non-convex variational formulation of the
problem. For this purpose, we introduce a versatile generalised Gaussian prior
whose parameters, including its exponent, are space-variant. Secondly, we
design an alternating proximal-based optimisation algorithm that efficiently
exploits the structure of the proposed non-convex objective function. We also
analyse the convergence of this algorithm. As shown in numerical experiments
conducted on joint deblurring/segmentation tasks, the proposed method provides
high-quality results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-04T00:00:00Z">2024-03-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehoon Jang, Inha Lee, Minje Kim, Kyungdon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor scenes we are living in are visually homogenous or textureless, while
they inherently have structural forms and provide enough structural priors for
3D scene reconstruction. Motivated by this fact, we propose a structure-aware
online signed distance fields (SDF) reconstruction framework in indoor scenes,
especially under the Atlanta world (AW) assumption. Thus, we dub this
incremental SDF reconstruction for AW as AiSDF. Within the online framework, we
infer the underlying Atlanta structure of a given scene and then estimate
planar surfel regions supporting the Atlanta structure. This Atlanta-aware
surfel representation provides an explicit planar map for a given scene. In
addition, based on these Atlanta planar surfel regions, we adaptively sample
and constrain the structural regularity in the SDF reconstruction, which
enables us to improve the reconstruction quality by maintaining a high-level
structure while enhancing the details of a given scene. We evaluate the
proposed AiSDF on the ScanNet and ReplicaCAD <span class="highlight-title">dataset</span>s, where we demonstrate
that the proposed framework is capable of reconstructing fine details of
objects implicitly, as well as structures explicitly in room-scale scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, Accepted to IEEE RA-L (First two authors
  contributed equally)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Afifi, Zhenhua Hu, Liang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic range (HDR) imaging involves capturing a series of frames of the
same scene, each with different exposure settings, to broaden the dynamic range
of light. This can be achieved through burst capturing or using staggered HDR
sensors that capture long and short exposures simultaneously in the camera
image signal processor (ISP). Within camera ISP pipeline, illuminant estimation
is a crucial step aiming to estimate the color of the global illuminant in the
scene. This estimation is used in camera ISP white-balance module to remove
undesirable color cast in the final image. Despite the multiple frames captured
in the HDR pipeline, conventional illuminant estimation methods often rely only
on a single frame of the scene. In this paper, we explore leveraging
information from frames captured with different exposure times. Specifically,
we introduce a simple feature extracted from dual-exposure images to guide
illuminant estimators, referred to as the dual-exposure feature (DEF). To
validate the efficiency of DEF, we employed two illuminant estimators using the
proposed DEF: 1) a multilayer perceptron network (<span class="highlight-title">MLP</span>), referred to as
exposure-based <span class="highlight-title">MLP</span> (E<span class="highlight-title">MLP</span>), and 2) a modified version of the convolutional color
constancy (CCC) to integrate our DEF, that we call ECCC. Both E<span class="highlight-title">MLP</span> and ECCC
achieve promising results, in some cases surpassing prior methods that require
hundreds of thousands or millions of parameters, with only a few hundred
parameters for E<span class="highlight-title">MLP</span> and a few thousand parameters for ECCC.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Open-ended Visual Quality Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative settings (e.g. pairwise choice, listwise ranking) have been
adopted by a wide range of subjective studies for image quality assessment
(IQA), as it inherently standardizes the evaluation criteria across different
observers and offer more clear-cut responses. In this work, we extend the edge
of emerging large multi-modality models (LMMs) to further advance visual
quality comparison into open-ended settings, that 1) can respond to open-range
questions on quality comparison; 2) can provide detailed reasonings beyond
direct answers. To this end, we propose the Co-Instruct. To train this
first-of-its-kind open-source open-ended visual quality comparer, we collect
the Co-Instruct-562K <span class="highlight-title">dataset</span>, from two sources: (a) LLM-merged single image
quality description, (b) <span class="highlight-title">GPT</span>-4V "teacher" responses on unlabeled data.
Furthermore, to better evaluate this setting, we propose the MICBench, the
first benchmark on multi-image comparison for LMMs. We demonstrate that
Co-Instruct not only achieves in average 30% higher accuracy than
state-of-the-art open-source LMMs, but also outperforms <span class="highlight-title">GPT</span>-4V (its teacher),
on both existing related benchmarks and the proposed MICBench. Our model is
published at https://huggingface.co/q-future/co-instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix typos</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-03T00:00:00Z">2024-03-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit
  Representation for Diverse 3D Shapes <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, Lin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit representation of geometric shapes has witnessed considerable
advancements in recent years. However, common distance field based implicit
representations, specifically signed distance field (SDF) for watertight shapes
or unsigned distance field (UDF) for arbitrary shapes, routinely suffer from
degradation of reconstruction accuracy when converting to explicit surface
points and meshes. In this paper, we introduce a novel neural implicit
representation based on unsigned orthogonal distance fields (UODFs). In UODFs,
the minimal unsigned distance from any spatial point to the shape surface is
defined solely in one orthogonal direction, contrasting with the
multi-directional determination made by SDF and UDF. Consequently, every point
in the 3D UODFs can directly access its closest surface points along three
orthogonal directions. This distinctive feature leverages the accurate
reconstruction of surface points without interpolation errors. We verify the
effectiveness of UODFs through a range of reconstruction examples, extending
from simple watertight or non-watertight shapes to complex shapes that include
hollows, internal or assembling structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-28T00:00:00Z">2024-02-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        HDR <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> High Dynamic Range Imaging with Multi-Exposure Images in
  Dynamic Scenes <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilu Zhang, Haoyu Wang, Shuai Liu, Xiaotao Wang, Lei Lei, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Merging multi-exposure images is a common approach for obtaining high dynamic
range (HDR) images, with the primary challenge being the avoidance of ghosting
artifacts in dynamic scenes. Recent methods have proposed using deep neural
networks for deghosting. However, the methods typically rely on sufficient data
with HDR ground-truths, which are difficult and costly to collect. In this
work, to eliminate the need for labeled data, we propose SelfHDR, a
<span class="highlight-title">self-supervised</span> HDR reconstruction method that only requires dynamic
multi-exposure images during training. Specifically, SelfHDR learns a
reconstruction network under the supervision of two complementary components,
which can be constructed from multi-exposure images and focus on HDR color as
well as structure, respectively. The color component is estimated from aligned
multi-exposure images, while the structure one is generated through a
structure-focused network that is supervised by the color component and an
input reference (\eg, medium-exposure) image. During testing, the learned
reconstruction network is directly deployed to predict an HDR image.
Experiments on real-world images demonstrate our SelfHDR achieves superior
results against the state-of-the-art <span class="highlight-title">self-supervised</span> methods, and comparable
performance to supervised ones. Codes are available at
https://github.com/cszhilu1998/SelfHDR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        IQA <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Black-box Adversarial Attacks Against Image Quality Assessment Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the
perceptual quality of an image in line with its subjective evaluation. To put
the NR-IQA models into practice, it is essential to study their potential
loopholes for model refinement. This paper makes the first attempt to explore
the black-box adversarial attacks on NR-IQA models. Specifically, we first
formulate the attack problem as maximizing the deviation between the estimated
quality scores of original and perturbed images, while restricting the
perturbed image distortions for visual quality preservation. Under such
formulation, we then design a Bi-directional loss function to mislead the
estimated quality scores of adversarial examples towards an opposite direction
with maximum deviation. On this basis, we finally develop an efficient and
effective black-box attack method against NR-IQA models. Extensive experiments
reveal that all the evaluated NR-IQA models are vulnerable to the proposed
attack method. And the generated perturbations are not transferable, enabling
them to serve the investigation of specialities of disparate IQA models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Deblur <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Deblur Polarized Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chu Zhou, Minggui Teng, Xinyu Zhou, Chao Xu, Boxin Sh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A polarization camera can capture four polarized images with different
polarizer angles in a single shot, which is useful in polarization-based vision
applications since the degree of polarization (DoP) and the angle of
polarization (AoP) can be directly computed from the captured polarized images.
However, since the on-chip micro-polarizers block part of the light so that the
sensor often requires a longer exposure time, the captured polarized images are
prone to motion blur caused by camera shakes, leading to noticeable degradation
in the computed DoP and AoP. Deblurring methods for conventional images often
show degenerated performance when handling the polarized images since they only
focus on deblurring without considering the polarization constrains. In this
paper, we propose a polarized image deblurring pipeline to solve the problem in
a polarization-aware manner by adopting a divide-and-conquer strategy to
explicitly decompose the problem into two less ill-posed sub-problems, and
design a two-stage neural network to handle the two sub-problems respectively.
Experimental results show that our method achieves state-of-the-art performance
on both synthetic and real-world images, and can improve the performance of
polarization-based vision applications such as image dehazing and reflection
removal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhargav Ghanekar, Salman Siddique Khan, Vivek Boominathan, Pranav Sharma, Shreyas Singh, Kaushik Mitra, Ashok Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive, compact, single-shot 3D sensing is useful in many application areas
such as microscopy, medical imaging, surgical navigation, and autonomous
driving where form factor, time, and power constraints can exist. Obtaining
RGB-D scene information over a short imaging distance, in an ultra-compact form
factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP)
sensors are a potential solution to achieve the same. DP sensors collect light
rays from two different halves of the lens in two interleaved pixel arrays,
thus capturing two slightly different views of the scene, like a stereo camera
system. However, imaging with a DP sensor implies that the defocus blur size is
directly proportional to the disparity seen between the views. This creates a
trade-off between disparity estimation vs. deblurring accuracy. To improve this
trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which
we use a coded aperture in the imaging lens along with a DP sensor. In our
approach, we jointly learn an optimal coded pattern and the reconstruction
algorithm in an end-to-end optimization setting. Our resulting CADS imaging
system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF)
estimates and 5-6% in depth estimation quality over naive DP sensing for a wide
range of aperture settings. Furthermore, we build the proposed CADS prototypes
for DSLR photography settings and in an endoscope and a dermoscope form factor.
Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D
reconstruction results in simulations and real-world experiments in a passive,
snapshot, and compact manner.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-02-26T00:00:00Z">2024-02-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        SDF <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed
  Distance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human hands are highly articulated and versatile at handling objects. Jointly
estimating the 3D poses of a hand and the object it manipulates from a
monocular camera is challenging due to frequent occlusions. Thus, existing
methods often rely on intermediate 3D shape representations to increase
performance. These representations are typically explicit, such as 3D point
clouds or meshes, and thus provide information in the direct surroundings of
the intermediate hand pose estimate. To address this, we introduce HOISDF, a
Signed Distance Field (SDF) guided hand-object pose estimation network, which
jointly exploits hand and object SDFs to provide a global, implicit
representation over the complete reconstruction volume. Specifically, the role
of the SDFs is threefold: equip the visual encoder with implicit shape
information, help to encode hand-object interactions, and guide the hand and
object pose regression via SDF-based sampling and by augmenting the feature
representations. We show that HOISDF achieves state-of-the-art results on
hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available
at https://github.com/amathislab/HOISDF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024. 9 figures, many tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T00:49:33.879329011Z">
            2024-03-28 00:49:33 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
